{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# 🔬 DiffLlama vs Llama: Google Colab Experiment\n",
        "\n",
        "This Notebook is designed to run a comparative experiment on Google Colab environment to evaluate the noise robustness of DiffLlama and Llama on mathematical reasoning tasks.\n",
        "\n",
        "## 📋 Experiment Overview\n",
        "- **Objective**: Compare DiffLlama-375M and Llama-375M performance on noisy math problems\n",
        "- **Dataset**: GSM8K math reasoning dataset and its noisy variants\n",
        "- **Evaluation**: Zero-shot performance + attention mechanism analysis\n",
        "- **Environment**: Google Colab (GPU recommended)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_section"
      },
      "source": [
        "## 🚀 Step 1: Environment Setup\n",
        "\n",
        "First, check the runtime environment and configure necessary settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "check_environment",
        "outputId": "cc63a34a-196f-457a-b6c0-a7b66ebe8464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🖥️  CUDA available: True\n",
            "🔧 GPU: Tesla T4\n",
            "💾 GPU Memory: 15.8 GB\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"🖥️  CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"🔧 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"💾 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"⚠️  No GPU detected. Experiment will be slow on CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clone_repo",
        "outputId": "a5d345c0-a7c2-4984-c5a8-baac31382087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 Cloning repository...\n",
            "Cloning into 'DiffLlama-Math-Robustness'...\n",
            "remote: Enumerating objects: 215, done.\u001b[K\n",
            "remote: Counting objects: 100% (215/215), done.\u001b[K\n",
            "remote: Compressing objects: 100% (149/149), done.\u001b[K\n",
            "remote: Total 215 (delta 120), reused 153 (delta 61), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (215/215), 133.91 KiB | 6.70 MiB/s, done.\n",
            "Resolving deltas: 100% (120/120), done.\n",
            "📥 Copying files...\n",
            "📥 Removing repository...\n",
            "📥 Done\n"
          ]
        }
      ],
      "source": [
        "# Clone from Git repository if project files are not in current directory\n",
        "# Replace with your actual repository URL\n",
        "import os\n",
        "if not os.path.exists('colab/experiment.py'):\n",
        "    print(\"📥 Cloning repository...\")\n",
        "    !git clone https://github.com/github-bowen/DiffLlama-Math-Robustness.git\n",
        "    print(\"📥 Copying files...\")\n",
        "    !cp -r DiffLlama-Math-Robustness/* .\n",
        "    print(\"📥 Removing repository...\")\n",
        "    !rm -rf DiffLlama-Math-Robustness\n",
        "    print(\"📥 Done\")\n",
        "else:\n",
        "    print(\"✅ Project files found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upload_section"
      },
      "source": [
        "## 📁 Step 2: Upload Project Files\n",
        "\n",
        "If you didn't clone using Git, manually upload the following files to Colab:\n",
        "\n",
        "**Required Files**:\n",
        "- `colab_experiment.py` (main Colab script)\n",
        "- `pre_download_models.py` (model download script)\n",
        "- All Python files in the `src/` directory\n",
        "- `requirements.txt`\n",
        "\n",
        "Use Colab's file upload feature or copy files from Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructions_section"
      },
      "source": [
        "## 📖 Step 3: View Usage Instructions\n",
        "\n",
        "Run the command below to view detailed usage instructions and options."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "show_instructions",
        "outputId": "801ca771-75dc-4018-ae51-b5bd1f2f3181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🎯 GOOGLE COLAB USAGE INSTRUCTIONS\n",
            "\n",
            "1. 📱 Basic Setup (Run once):\n",
            "   !python -m colab.experiment --setup\n",
            "\n",
            "2. 🚀 Quick Test (Recommended first run):\n",
            "   !python -m colab.experiment --mode quick\n",
            "\n",
            "3. 📊 Medium Experiment:\n",
            "   !python -m colab.experiment --mode medium\n",
            "\n",
            "4. 🔬 Full Experiment:\n",
            "   !python -m colab.experiment --mode full --max-samples 500\n",
            "\n",
            "5. 🎯 Experiment with Fine-tuning:\n",
            "   !python -m colab.experiment --mode medium --enable-sft --sft-samples 200\n",
            "\n",
            "6. 🔄 Skip Zero-shot (only SFT and attention):\n",
            "   !python -m colab.experiment --mode medium --skip-zero-shot --enable-sft\n",
            "\n",
            "7. 📈 Only Fine-tuning workflow:\n",
            "   !python -m colab.experiment --mode medium --skip-zero-shot --enable-sft --skip-attention\n",
            "\n",
            "🔧 Options:\n",
            "   --mode: quick/medium/full (experiment scope)\n",
            "   --max-samples: Limit number of evaluation samples\n",
            "   --enable-sft: Enable supervised fine-tuning (disabled by default)\n",
            "   --sft-samples: Number of samples for fine-tuning (default: varies by mode)\n",
            "   --sft-epochs: Number of epochs for fine-tuning (default: varies by mode)\n",
            "   --skip-zero-shot: Skip zero-shot evaluation to save time\n",
            "   --skip-attention: Skip attention analysis to save time\n",
            "   --help: Show all options\n",
            "\n",
            "💡 Tips:\n",
            "   - Use to persist models across sessions\n",
            "   - Start with quick mode to verify everything works\n",
            "   - Fine-tuning requires significant GPU memory and time\n",
            "   - Monitor GPU memory usage in Colab\n",
            "   - Use medium mode with --enable-sft for balanced experiments\n",
            "   - Skip zero-shot if you only need SFT results\n",
            "\n",
            "📁 Results will be saved to:\n",
            "   - Local: /content/results/\n",
            "   - Drive: /content/drive/MyDrive/DiffLlama_Experiment/results/\n",
            "\n",
            "⚠️  Resource Usage:\n",
            "   - Quick mode: ~15-30 minutes, minimal GPU memory\n",
            "   - Medium mode: ~1-2 hours, moderate GPU memory\n",
            "   - Full mode: ~3-6 hours, high GPU memory\n",
            "   - SFT adds: +30-60 minutes, requires >12GB GPU memory\n",
            "   - Skipping zero-shot saves: ~30-50% of total time\n",
            "\n",
            "🎯 Common Workflows:\n",
            "   - Data validation: --mode quick\n",
            "   - Zero-shot comparison: --mode medium\n",
            "   - SFT-only experiment: --mode medium --skip-zero-shot --enable-sft\n",
            "   - Complete research: --mode full --enable-sft\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display usage instructions\n",
        "!python -m colab.experiment --instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_run_section"
      },
      "source": [
        "## 🔧 Step 4: Initial Setup\n",
        "\n",
        "Run initial setup to install dependencies and configure the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq8duQXXpnBa",
        "outputId": "49fac045-a2da-4f9e-e381-ecbfc47d2236"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "initial_setup",
        "outputId": "f94e3516-081d-4cd0-cff4-3e3aba5201a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Installing dependencies...\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers>=4.20.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (4.52.2)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (2.14.4)\n",
            "Requirement already satisfied: accelerate>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (1.7.0)\n",
            "Collecting evaluate>=0.4.0 (from -r requirements.txt (line 5))\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (3.10.0)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (0.13.2)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (2.0.2)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (4.67.1)\n",
            "Requirement already satisfied: huggingface_hub>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (0.31.4)\n",
            "Collecting hf_xet (from -r requirements.txt (line 13))\n",
            "  Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
            "Requirement already satisfied: Pillow>=8.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->-r requirements.txt (line 1)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->-r requirements.txt (line 1)) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->-r requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->-r requirements.txt (line 1)) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->-r requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->-r requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 2)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 2)) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->-r requirements.txt (line 3)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->-r requirements.txt (line 3)) (0.3.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->-r requirements.txt (line 3)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->-r requirements.txt (line 3)) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->-r requirements.txt (line 3)) (3.11.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.20.0->-r requirements.txt (line 4)) (5.9.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->-r requirements.txt (line 6)) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->-r requirements.txt (line 6)) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->-r requirements.txt (line 6)) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->-r requirements.txt (line 7)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->-r requirements.txt (line 7)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->-r requirements.txt (line 7)) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->-r requirements.txt (line 7)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->-r requirements.txt (line 7)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->-r requirements.txt (line 7)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->-r requirements.txt (line 9)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->-r requirements.txt (line 9)) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 3)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 3)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 3)) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 3)) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 3)) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 3)) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.5.0->-r requirements.txt (line 7)) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.20.0->-r requirements.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.20.0->-r requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.20.0->-r requirements.txt (line 2)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.20.0->-r requirements.txt (line 2)) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->-r requirements.txt (line 1)) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m127.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m129.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, hf_xet, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, evaluate\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed evaluate-0.4.3 hf_xet-1.1.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "✅ Dependencies installed\n",
            "📁 Using Google Drive storage: /content/drive/MyDrive/DiffLlama_Experiment\n",
            "  ✓ cache -> /content/drive/MyDrive/DiffLlama_Experiment/models\n",
            "  ✓ data -> /content/drive/MyDrive/DiffLlama_Experiment/data\n",
            "  ✓ results -> /content/drive/MyDrive/DiffLlama_Experiment/results\n",
            "  ✓ models_finetuned -> /content/drive/MyDrive/DiffLlama_Experiment/models_finetuned\n",
            "✅ Setup completed\n"
          ]
        }
      ],
      "source": [
        "# Run initial setup (includes Google Drive mounting)\n",
        "!python -m colab.experiment --setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gziSZrcrxTug",
        "outputId": "4f8fa680-c2ca-4e49-9a0d-c42054f9bd60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 80\n",
            "drwxr-xr-x 1 root root  4096 Jun  2 23:08 .\n",
            "drwxr-xr-x 1 root root  4096 Jun  2 23:05 ..\n",
            "lrwxrwxrwx 1 root root    50 Jun  2 23:08 cache -> /content/drive/MyDrive/DiffLlama_Experiment/models\n",
            "drwxr-xr-x 3 root root  4096 Jun  2 23:06 colab\n",
            "drwxr-xr-x 4 root root  4096 May 29 14:01 .config\n",
            "lrwxrwxrwx 1 root root    48 Jun  2 23:08 data -> /content/drive/MyDrive/DiffLlama_Experiment/data\n",
            "drwx------ 6 root root  4096 Jun  2 23:06 drive\n",
            "-rw-r--r-- 1 root root  1074 Jun  2 23:06 LICENSE\n",
            "-rw-r--r-- 1 root root 18135 Jun  2 23:06 main.py\n",
            "lrwxrwxrwx 1 root root    60 Jun  2 23:08 models_finetuned -> /content/drive/MyDrive/DiffLlama_Experiment/models_finetuned\n",
            "-rw-r--r-- 1 root root  6409 Jun  2 23:06 README.md\n",
            "-rw-r--r-- 1 root root   226 Jun  2 23:06 requirements.txt\n",
            "lrwxrwxrwx 1 root root    51 Jun  2 23:08 results -> /content/drive/MyDrive/DiffLlama_Experiment/results\n",
            "drwxr-xr-x 1 root root  4096 May 29 14:01 sample_data\n",
            "drwxr-xr-x 2 root root  4096 Jun  2 23:06 scripts\n",
            "drwxr-xr-x 2 root root  4096 Jun  2 23:06 src\n",
            "-rw-r--r-- 1 root root  7346 Jun  2 23:06 USAGE_GUIDE.md\n"
          ]
        }
      ],
      "source": [
        "!ls -al"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m scripts.download_models"
      ],
      "metadata": {
        "id": "PT-uWhz_693m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "experiment_section"
      },
      "source": [
        "## 🚀 Step 5: Run Experiments\n",
        "\n",
        "Choose an appropriate experiment mode based on your needs:\n",
        "\n",
        "```bash\n",
        "options:\n",
        "  -h, --help            show this help message and exit\n",
        "  --mode {quick,medium,full}\n",
        "                        Experiment mode (default: quick)\n",
        "  --max-samples MAX_SAMPLES\n",
        "                        Maximum samples for evaluation\n",
        "  --enable-sft          Enable supervised fine-tuning (disabled by default)\n",
        "  --sft-samples SFT_SAMPLES\n",
        "                        Number of samples for fine-tuning\n",
        "  --sft-epochs SFT_EPOCHS\n",
        "                        Number of epochs for fine-tuning\n",
        "  --skip-attention      Skip attention analysis\n",
        "  --skip-zero-shot      Skip zero-shot evaluation\n",
        "  --setup               Only run setup (dependencies and environment)\n",
        "  --instructions        Display usage instructions\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quick_test_section"
      },
      "source": [
        "### 🏃 Quick Test (Recommended for First Run)\n",
        "Validate the experiment workflow using a small number of samples, takes about 30-60 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "quick_test"
      },
      "outputs": [],
      "source": [
        "# Quick test mode\n",
        "!python -m colab.experiment --mode quick"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "medium_test_section"
      },
      "source": [
        "### 📊 Medium-Scale Experiment\n",
        "Use a moderate number of samples, balancing time and result quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "medium_test"
      },
      "outputs": [],
      "source": [
        "# Medium-scale experiment (make sure quick test runs successfully first)\n",
        "!python -m colab.experiment --mode medium"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "full_test_section"
      },
      "source": [
        "### 🔬 Full Experiment\n",
        "Use the complete dataset for the experiment, may take several hours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "full_test"
      },
      "outputs": [],
      "source": [
        "# Full experiment (run only when you have enough time)\n",
        "## Evaluation only\n",
        "!python -m colab.experiment --mode full --skip-attention\n",
        "\n",
        "## SFT only\n",
        "!python -m colab.experiment --mode full --skip-zero-shot --enable-sft --skip-attention\n",
        "\n",
        "## Evaluation + SFT\n",
        "!python -m colab.experiment --mode full --enable-sft --skip-attention\n",
        "\n",
        "## All steps: Evaluation + SFT + Attention Analysis\n",
        "!python -m colab.experiment --mode full --enable-sft"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%reset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyghnEPL73Bm",
        "outputId": "fca607f5-831d-46ba-a958-835cec75a145"
      },
      "execution_count": 24,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SFT on all samples:\n",
        "## SFT + Attention Analysis\n",
        "!python -m main --max-samples 50 --sft-samples 7473 --sft-epochs 1 --skip-zero-shot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1lAjOhN5XOq",
        "outputId": "9a7bbdfb-86d7-496c-caff-b04b67b5f65a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-02 23:20:24.555666: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1748906424.589585    4380 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1748906424.610868    4380 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-02 23:20:24.668618: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "🔬 Running FULL EXPERIMENT\n",
            "================================================================================\n",
            "DIFFLAMA VS LLAMA: NOISE ROBUSTNESS EXPERIMENT\n",
            "================================================================================\n",
            "Start time: 2025-06-02 23:20:30\n",
            "✓ All required dependencies found\n",
            "✓ Directory ready: data\n",
            "✓ Directory ready: results\n",
            "✓ Directory ready: results/attention_maps\n",
            "✓ Directory ready: models_finetuned\n",
            "\n",
            "================================================================================\n",
            "STEP 1: DATA PREPARATION\n",
            "================================================================================\n",
            "✓ GSM8K dataset already exists\n",
            "✓ Noisy datasets already exist\n",
            "✓ Data preparation completed\n",
            "\n",
            "⏭️  Skipping zero-shot evaluation\n",
            "\n",
            "================================================================================\n",
            "STEP 3: SUPERVISED FINE-TUNING\n",
            "================================================================================\n",
            "================================================================================\n",
            "STARTING SUPERVISED FINE-TUNING PIPELINE\n",
            "================================================================================\n",
            "Creating training subset with 7473 samples...\n",
            "Training subset saved to data/gsm8k_train_sft.jsonl with 7473 samples\n",
            "\n",
            "==================== FINE-TUNING LLAMA ====================\n",
            "Fine-tuning llama model...\n",
            "Device: cuda\n",
            "Training samples: 7473\n",
            "Epochs: 1\n",
            "Batch size: 4\n",
            "Learning rate: 5e-05\n",
            "Loading Llama-375M from: ./cache/models--reyllama--Llama_375M/snapshots/416b70824d560b02245268c208ffd5388b4aa056/checkpoint-64434\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting: '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting: '<|finetune_right_pad_id|>', ID: 128004\n",
            "llama loaded on cuda.\n",
            "Loading training data from data/gsm8k_train_sft.jsonl...\n",
            "Prepared 7473 training examples\n",
            "Tokenizing training data...\n",
            "Map: 100% 7473/7473 [00:08<00:00, 853.93 examples/s]\n",
            "Starting fine-tuning for llama...\n",
            "{'loss': 2.4686, 'grad_norm': 7.710882186889648, 'learning_rate': 4.975922953451044e-05, 'epoch': 0.01}\n",
            "{'loss': 2.1106, 'grad_norm': 7.39383602142334, 'learning_rate': 4.949170679507759e-05, 'epoch': 0.01}\n",
            "{'loss': 2.0389, 'grad_norm': 6.492391586303711, 'learning_rate': 4.922418405564474e-05, 'epoch': 0.02}\n",
            "{'loss': 2.0991, 'grad_norm': 7.515586853027344, 'learning_rate': 4.8956661316211885e-05, 'epoch': 0.02}\n",
            "{'loss': 2.0769, 'grad_norm': 6.71142578125, 'learning_rate': 4.8689138576779034e-05, 'epoch': 0.03}\n",
            "{'loss': 2.0111, 'grad_norm': 6.884924411773682, 'learning_rate': 4.842161583734618e-05, 'epoch': 0.03}\n",
            "{'loss': 1.8983, 'grad_norm': 6.658547878265381, 'learning_rate': 4.815409309791333e-05, 'epoch': 0.04}\n",
            "{'loss': 1.9105, 'grad_norm': 6.464270114898682, 'learning_rate': 4.788657035848048e-05, 'epoch': 0.04}\n",
            "{'loss': 1.9207, 'grad_norm': 5.362054347991943, 'learning_rate': 4.761904761904762e-05, 'epoch': 0.05}\n",
            "{'loss': 1.8602, 'grad_norm': 6.584726810455322, 'learning_rate': 4.735152487961477e-05, 'epoch': 0.05}\n",
            "{'loss': 1.8825, 'grad_norm': 5.7943854331970215, 'learning_rate': 4.708400214018192e-05, 'epoch': 0.06}\n",
            "{'loss': 1.981, 'grad_norm': 6.447309494018555, 'learning_rate': 4.6816479400749066e-05, 'epoch': 0.06}\n",
            "{'loss': 1.9782, 'grad_norm': 6.512517929077148, 'learning_rate': 4.6548956661316214e-05, 'epoch': 0.07}\n",
            "{'loss': 1.8696, 'grad_norm': 5.3769755363464355, 'learning_rate': 4.628143392188336e-05, 'epoch': 0.07}\n",
            "  8% 148/1869 [07:25<1:26:39,  3.02s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "custom_section"
      },
      "source": [
        "### 🛠 Custom Experiment\n",
        "Adjust experiment parameters as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "custom_experiment"
      },
      "outputs": [],
      "source": [
        "# Custom experiment example\n",
        "# Only run evaluation, skip attention analysis to save time\n",
        "!python -m colab.experiment --mode medium --skip-attention --max-samples 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "results_section"
      },
      "source": [
        "## 📊 Step 6: View Experiment Results\n",
        "\n",
        "After completing the experiment, review the generated result files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "list_results",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12b4ae11-b5bf-40d4-ba9d-af08edba4712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "drwx------ 5 root root 4096 Jun  2 23:17 attention_maps\n"
          ]
        }
      ],
      "source": [
        "# List generated result files\n",
        "!ls -la results/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "show_summary",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eae2bf2d-cd5d-4a30-9048-db9afab865c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No experiment summary found. Please run an experiment first.\n"
          ]
        }
      ],
      "source": [
        "# View the latest experiment summary\n",
        "import json\n",
        "import glob\n",
        "\n",
        "# Find the latest summary file\n",
        "summary_files = glob.glob('results/colab_summary_*.json')\n",
        "if summary_files:\n",
        "    latest_summary = max(summary_files)\n",
        "    print(f\"📋 Latest experiment summary: {latest_summary}\")\n",
        "\n",
        "    with open(latest_summary, 'r') as f:\n",
        "        summary = json.load(f)\n",
        "\n",
        "    print(\"\\n📊 Experiment Summary:\")\n",
        "    for key, value in summary.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "else:\n",
        "    print(\"No experiment summary found. Please run an experiment first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "show_results",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d616627-706e-4bbb-f9ee-130ce4fcf1f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No results found. Please run an experiment first.\n"
          ]
        }
      ],
      "source": [
        "# Display main results\n",
        "import pandas as pd\n",
        "\n",
        "# Find the latest results file\n",
        "result_files = glob.glob('results/colab_results_*.csv')\n",
        "if result_files:\n",
        "    latest_results = max(result_files)\n",
        "    print(f\"📈 Latest results: {latest_results}\")\n",
        "\n",
        "    df = pd.read_csv(latest_results)\n",
        "    print(\"\\n📊 Performance Comparison:\")\n",
        "    print(df.pivot(index='model', columns='dataset', values='accuracy'))\n",
        "\n",
        "    # Calculate performance differences\n",
        "    pivot_df = df.pivot(index='model', columns='dataset', values='accuracy')\n",
        "    if 'llama' in pivot_df.index and 'diffllama' in pivot_df.index:\n",
        "        print(\"\\n🔍 Performance Difference (DiffLlama - Llama):\")\n",
        "        diff = pivot_df.loc['diffllama'] - pivot_df.loc['llama']\n",
        "        print(diff)\n",
        "else:\n",
        "    print(\"No results found. Please run an experiment first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualization_section"
      },
      "source": [
        "## 📈 Step 7: Results Visualization\n",
        "\n",
        "If your experiment included attention analysis, you can view the generated attention heatmaps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "show_attention_maps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f251cbb9-e8aa-406a-c4dd-8ec4ea8623ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 Attention Visualization Files:\n",
            "  📊 results/attention_maps/clean_q1/llama_attn_layer-1_head0_sample.png\n",
            "  📊 results/attention_maps/clean_q1/diffllama_attn_layer-1_head0_sample.png\n",
            "  📊 results/attention_maps/noisy_q1/llama_attn_layer-1_head0_sample.png\n",
            "  📊 results/attention_maps/noisy_q1/diffllama_attn_layer-1_head0_sample.png\n",
            "  📊 results/attention_maps/clean_q2/llama_attn_layer-1_head0_sample.png\n"
          ]
        }
      ],
      "source": [
        "# Display attention heatmaps\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "attention_dir = 'results/attention_maps'\n",
        "if os.path.exists(attention_dir):\n",
        "    print(\"🧠 Attention Visualization Files:\")\n",
        "\n",
        "    # List all attention map files\n",
        "    for root, dirs, files in os.walk(attention_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.png'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                print(f\"  📊 {file_path}\")\n",
        "\n",
        "                # Display images (optional, uncomment to show)\n",
        "                # display(Image(file_path))\n",
        "else:\n",
        "    print(\"No attention maps found. Run experiment with attention analysis enabled.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "show_attention_analysis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a57e6e06-1d75-4696-9b80-f4705ea3a16d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No attention analysis found. Run experiment with attention analysis enabled.\n"
          ]
        }
      ],
      "source": [
        "# Display attention analysis results\n",
        "attention_files = glob.glob('results/colab_attention_*.json')\n",
        "if attention_files:\n",
        "    latest_attention = max(attention_files)\n",
        "    print(f\"🧠 Latest attention analysis: {latest_attention}\")\n",
        "\n",
        "    with open(latest_attention, 'r') as f:\n",
        "        attention_data = json.load(f)\n",
        "\n",
        "    print(\"\\n📊 Attention Allocation Analysis:\")\n",
        "    for model, data in attention_data.items():\n",
        "        print(f\"\\n{model.upper()} Model:\")\n",
        "        for condition, stats in data.items():\n",
        "            print(f\"  {condition.capitalize()}:\")\n",
        "            print(f\"    KMI (Key Math Info): {stats['kmi_mean']:.3f} ± {stats['kmi_std']:.3f}\")\n",
        "            print(f\"    NI (Noise Info): {stats['ni_mean']:.3f} ± {stats['ni_std']:.3f}\")\n",
        "            print(f\"    OC (Other Context): {stats['oc_mean']:.3f} ± {stats['oc_std']:.3f}\")\n",
        "else:\n",
        "    print(\"No attention analysis found. Run experiment with attention analysis enabled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download_section"
      },
      "source": [
        "## 💾 Step 8: Download Results\n",
        "\n",
        "Download experiment results locally or ensure they are saved in Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "download_results",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b55fe21-ca83-48d8-c8f9-4d11b6c7118a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 Results packaged in: experiment_results_20250602_231718.zip\n",
            "You can download this file from Colab's Files panel.\n",
            "\n",
            "💾 Results are also saved in Google Drive:\n",
            "  /content/drive/MyDrive/DiffLlama_Experiment/\n"
          ]
        }
      ],
      "source": [
        "# Compress result files for download\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "zip_filename = f'experiment_results_{timestamp}.zip'\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "    # Add all files from results directory\n",
        "    for root, dirs, files in os.walk('results'):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            zipf.write(file_path)\n",
        "\n",
        "print(f\"📦 Results packaged in: {zip_filename}\")\n",
        "print(\"You can download this file from Colab's Files panel.\")\n",
        "\n",
        "# Reminder if Google Drive was used\n",
        "if os.path.exists('/content/drive/MyDrive/DiffLlama_Experiment'):\n",
        "    print(\"\\n💾 Results are also saved in Google Drive:\")\n",
        "    print(\"  /content/drive/MyDrive/DiffLlama_Experiment/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "troubleshooting_section"
      },
      "source": [
        "## 🛠 Troubleshooting\n",
        "\n",
        "If you encounter issues, try the following solutions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clear_cache"
      },
      "outputs": [],
      "source": [
        "# Clear GPU memory cache\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"✅ GPU cache cleared\")\n",
        "\n",
        "# Check available memory\n",
        "import psutil\n",
        "memory = psutil.virtual_memory()\n",
        "print(f\"💾 RAM: {memory.available / 1e9:.1f}GB available / {memory.total / 1e9:.1f}GB total\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "restart_runtime"
      },
      "outputs": [],
      "source": [
        "# If memory is insufficient, you can restart the runtime (use with caution)\n",
        "# import os\n",
        "# os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion_section"
      },
      "source": [
        "## 🎯 Experiment Conclusions\n",
        "\n",
        "Based on the experiment results, you can analyze the following key questions:\n",
        "\n",
        "1. **Noise Robustness**: Does DiffLlama perform better on noisy data?\n",
        "2. **Attention Mechanism**: Is differential attention more effective at focusing on key information?\n",
        "3. **Performance Degradation**: How do both models' performances change across different noise types?\n",
        "\n",
        "---\n",
        "\n",
        "**Thank you for using this experiment framework!** 🎉\n",
        "\n",
        "If you have issues, please check:\n",
        "- If GPU memory is sufficient\n",
        "- If all required files are uploaded\n",
        "- If network connection is stable\n",
        "\n",
        "**Tip**: It's recommended to run the quick test mode first to validate the environment before running the full experiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARSSQGg95F6N"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}