{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üî¨ DiffLlama vs Llama: Google Colab Experiment\n",
        "\n",
        "This Notebook is designed to run a comparative experiment on Google Colab environment to evaluate the noise robustness of DiffLlama and Llama on mathematical reasoning tasks.\n",
        "\n",
        "## üìã Experiment Overview\n",
        "- **Objective**: Compare DiffLlama-375M and Llama-375M performance on noisy math problems\n",
        "- **Dataset**: GSM8K math reasoning dataset and its noisy variants\n",
        "- **Evaluation**: Zero-shot performance + attention mechanism analysis\n",
        "- **Environment**: Google Colab (GPU recommended)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_section"
      },
      "source": [
        "## üöÄ Step 1: Environment Setup\n",
        "\n",
        "First, check the runtime environment and configure necessary settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "check_environment",
        "outputId": "cc63a34a-196f-457a-b6c0-a7b66ebe8464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üñ•Ô∏è  CUDA available: True\n",
            "üîß GPU: Tesla T4\n",
            "üíæ GPU Memory: 15.8 GB\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"üñ•Ô∏è  CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üîß GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No GPU detected. Experiment will be slow on CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clone_repo",
        "outputId": "a5d345c0-a7c2-4984-c5a8-baac31382087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì• Cloning repository...\n",
            "Cloning into 'DiffLlama-Math-Robustness'...\n",
            "remote: Enumerating objects: 215, done.\u001b[K\n",
            "remote: Counting objects: 100% (215/215), done.\u001b[K\n",
            "remote: Compressing objects: 100% (149/149), done.\u001b[K\n",
            "remote: Total 215 (delta 120), reused 153 (delta 61), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (215/215), 133.91 KiB | 6.70 MiB/s, done.\n",
            "Resolving deltas: 100% (120/120), done.\n",
            "üì• Copying files...\n",
            "üì• Removing repository...\n",
            "üì• Done\n"
          ]
        }
      ],
      "source": [
        "# Clone from Git repository if project files are not in current directory\n",
        "# Replace with your actual repository URL\n",
        "import os\n",
        "if not os.path.exists('colab/experiment.py'):\n",
        "    print(\"üì• Cloning repository...\")\n",
        "    !git clone https://github.com/github-bowen/DiffLlama-Math-Robustness.git\n",
        "    print(\"üì• Copying files...\")\n",
        "    !cp -r DiffLlama-Math-Robustness/* .\n",
        "    print(\"üì• Removing repository...\")\n",
        "    !rm -rf DiffLlama-Math-Robustness\n",
        "    print(\"üì• Done\")\n",
        "else:\n",
        "    print(\"‚úÖ Project files found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upload_section"
      },
      "source": [
        "## üìÅ Step 2: Upload Project Files\n",
        "\n",
        "If you didn't clone using Git, manually upload the following files to Colab:\n",
        "\n",
        "**Required Files**:\n",
        "- `colab_experiment.py` (main Colab script)\n",
        "- `pre_download_models.py` (model download script)\n",
        "- All Python files in the `src/` directory\n",
        "- `requirements.txt`\n",
        "\n",
        "Use Colab's file upload feature or copy files from Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructions_section"
      },
      "source": [
        "## üìñ Step 3: View Usage Instructions\n",
        "\n",
        "Run the command below to view detailed usage instructions and options."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "show_instructions",
        "outputId": "801ca771-75dc-4018-ae51-b5bd1f2f3181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéØ GOOGLE COLAB USAGE INSTRUCTIONS\n",
            "\n",
            "1. üì± Basic Setup (Run once):\n",
            "   !python -m colab.experiment --setup\n",
            "\n",
            "2. üöÄ Quick Test (Recommended first run):\n",
            "   !python -m colab.experiment --mode quick\n",
            "\n",
            "3. üìä Medium Experiment:\n",
            "   !python -m colab.experiment --mode medium\n",
            "\n",
            "4. üî¨ Full Experiment:\n",
            "   !python -m colab.experiment --mode full --max-samples 500\n",
            "\n",
            "5. üéØ Experiment with Fine-tuning:\n",
            "   !python -m colab.experiment --mode medium --enable-sft --sft-samples 200\n",
            "\n",
            "6. üîÑ Skip Zero-shot (only SFT and attention):\n",
            "   !python -m colab.experiment --mode medium --skip-zero-shot --enable-sft\n",
            "\n",
            "7. üìà Only Fine-tuning workflow:\n",
            "   !python -m colab.experiment --mode medium --skip-zero-shot --enable-sft --skip-attention\n",
            "\n",
            "üîß Options:\n",
            "   --mode: quick/medium/full (experiment scope)\n",
            "   --max-samples: Limit number of evaluation samples\n",
            "   --enable-sft: Enable supervised fine-tuning (disabled by default)\n",
            "   --sft-samples: Number of samples for fine-tuning (default: varies by mode)\n",
            "   --sft-epochs: Number of epochs for fine-tuning (default: varies by mode)\n",
            "   --skip-zero-shot: Skip zero-shot evaluation to save time\n",
            "   --skip-attention: Skip attention analysis to save time\n",
            "   --help: Show all options\n",
            "\n",
            "üí° Tips:\n",
            "   - Use to persist models across sessions\n",
            "   - Start with quick mode to verify everything works\n",
            "   - Fine-tuning requires significant GPU memory and time\n",
            "   - Monitor GPU memory usage in Colab\n",
            "   - Use medium mode with --enable-sft for balanced experiments\n",
            "   - Skip zero-shot if you only need SFT results\n",
            "\n",
            "üìÅ Results will be saved to:\n",
            "   - Local: /content/results/\n",
            "   - Drive: /content/drive/MyDrive/DiffLlama_Experiment/results/\n",
            "\n",
            "‚ö†Ô∏è  Resource Usage:\n",
            "   - Quick mode: ~15-30 minutes, minimal GPU memory\n",
            "   - Medium mode: ~1-2 hours, moderate GPU memory\n",
            "   - Full mode: ~3-6 hours, high GPU memory\n",
            "   - SFT adds: +30-60 minutes, requires >12GB GPU memory\n",
            "   - Skipping zero-shot saves: ~30-50% of total time\n",
            "\n",
            "üéØ Common Workflows:\n",
            "   - Data validation: --mode quick\n",
            "   - Zero-shot comparison: --mode medium\n",
            "   - SFT-only experiment: --mode medium --skip-zero-shot --enable-sft\n",
            "   - Complete research: --mode full --enable-sft\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display usage instructions\n",
        "!python -m colab.experiment --instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_run_section"
      },
      "source": [
        "## üîß Step 4: Initial Setup\n",
        "\n",
        "Run initial setup to install dependencies and configure the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq8duQXXpnBa",
        "outputId": "49fac045-a2da-4f9e-e381-ecbfc47d2236"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "initial_setup",
        "outputId": "f94e3516-081d-4cd0-cff4-3e3aba5201a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Installing dependencies...\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers>=4.20.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (4.52.2)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (2.14.4)\n",
            "Requirement already satisfied: accelerate>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (1.7.0)\n",
            "Collecting evaluate>=0.4.0 (from -r requirements.txt (line 5))\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (1.6.1)\n",
            "Requirement already satisfied: matplotlib>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (3.10.0)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (0.13.2)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (2.0.2)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (4.67.1)\n",
            "Requirement already satisfied: huggingface_hub>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (0.31.4)\n",
            "Collecting hf_xet (from -r requirements.txt (line 13))\n",
            "  Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
            "Requirement already satisfied: Pillow>=8.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->-r requirements.txt (line 1)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->-r requirements.txt (line 1)) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->-r requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->-r requirements.txt (line 1)) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->-r requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->-r requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->-r requirements.txt (line 1)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 2)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.20.0->-r requirements.txt (line 2)) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->-r requirements.txt (line 3)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->-r requirements.txt (line 3)) (0.3.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->-r requirements.txt (line 3)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->-r requirements.txt (line 3)) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->-r requirements.txt (line 3)) (3.11.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.20.0->-r requirements.txt (line 4)) (5.9.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->-r requirements.txt (line 6)) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->-r requirements.txt (line 6)) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->-r requirements.txt (line 6)) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->-r requirements.txt (line 7)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->-r requirements.txt (line 7)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->-r requirements.txt (line 7)) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->-r requirements.txt (line 7)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->-r requirements.txt (line 7)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.5.0->-r requirements.txt (line 7)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->-r requirements.txt (line 9)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->-r requirements.txt (line 9)) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 3)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 3)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 3)) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 3)) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 3)) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->-r requirements.txt (line 3)) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.5.0->-r requirements.txt (line 7)) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.20.0->-r requirements.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.20.0->-r requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.20.0->-r requirements.txt (line 2)) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.20.0->-r requirements.txt (line 2)) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->-r requirements.txt (line 1)) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m127.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m129.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, hf_xet, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, evaluate\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed evaluate-0.4.3 hf_xet-1.1.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "‚úÖ Dependencies installed\n",
            "üìÅ Using Google Drive storage: /content/drive/MyDrive/DiffLlama_Experiment\n",
            "  ‚úì cache -> /content/drive/MyDrive/DiffLlama_Experiment/models\n",
            "  ‚úì data -> /content/drive/MyDrive/DiffLlama_Experiment/data\n",
            "  ‚úì results -> /content/drive/MyDrive/DiffLlama_Experiment/results\n",
            "  ‚úì models_finetuned -> /content/drive/MyDrive/DiffLlama_Experiment/models_finetuned\n",
            "‚úÖ Setup completed\n"
          ]
        }
      ],
      "source": [
        "# Run initial setup (includes Google Drive mounting)\n",
        "!python -m colab.experiment --setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gziSZrcrxTug",
        "outputId": "4f8fa680-c2ca-4e49-9a0d-c42054f9bd60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 80\n",
            "drwxr-xr-x 1 root root  4096 Jun  2 23:08 .\n",
            "drwxr-xr-x 1 root root  4096 Jun  2 23:05 ..\n",
            "lrwxrwxrwx 1 root root    50 Jun  2 23:08 cache -> /content/drive/MyDrive/DiffLlama_Experiment/models\n",
            "drwxr-xr-x 3 root root  4096 Jun  2 23:06 colab\n",
            "drwxr-xr-x 4 root root  4096 May 29 14:01 .config\n",
            "lrwxrwxrwx 1 root root    48 Jun  2 23:08 data -> /content/drive/MyDrive/DiffLlama_Experiment/data\n",
            "drwx------ 6 root root  4096 Jun  2 23:06 drive\n",
            "-rw-r--r-- 1 root root  1074 Jun  2 23:06 LICENSE\n",
            "-rw-r--r-- 1 root root 18135 Jun  2 23:06 main.py\n",
            "lrwxrwxrwx 1 root root    60 Jun  2 23:08 models_finetuned -> /content/drive/MyDrive/DiffLlama_Experiment/models_finetuned\n",
            "-rw-r--r-- 1 root root  6409 Jun  2 23:06 README.md\n",
            "-rw-r--r-- 1 root root   226 Jun  2 23:06 requirements.txt\n",
            "lrwxrwxrwx 1 root root    51 Jun  2 23:08 results -> /content/drive/MyDrive/DiffLlama_Experiment/results\n",
            "drwxr-xr-x 1 root root  4096 May 29 14:01 sample_data\n",
            "drwxr-xr-x 2 root root  4096 Jun  2 23:06 scripts\n",
            "drwxr-xr-x 2 root root  4096 Jun  2 23:06 src\n",
            "-rw-r--r-- 1 root root  7346 Jun  2 23:06 USAGE_GUIDE.md\n"
          ]
        }
      ],
      "source": [
        "!ls -al"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m scripts.download_models"
      ],
      "metadata": {
        "id": "PT-uWhz_693m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "experiment_section"
      },
      "source": [
        "## üöÄ Step 5: Run Experiments\n",
        "\n",
        "Choose an appropriate experiment mode based on your needs:\n",
        "\n",
        "```bash\n",
        "options:\n",
        "  -h, --help            show this help message and exit\n",
        "  --mode {quick,medium,full}\n",
        "                        Experiment mode (default: quick)\n",
        "  --max-samples MAX_SAMPLES\n",
        "                        Maximum samples for evaluation\n",
        "  --enable-sft          Enable supervised fine-tuning (disabled by default)\n",
        "  --sft-samples SFT_SAMPLES\n",
        "                        Number of samples for fine-tuning\n",
        "  --sft-epochs SFT_EPOCHS\n",
        "                        Number of epochs for fine-tuning\n",
        "  --skip-attention      Skip attention analysis\n",
        "  --skip-zero-shot      Skip zero-shot evaluation\n",
        "  --setup               Only run setup (dependencies and environment)\n",
        "  --instructions        Display usage instructions\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quick_test_section"
      },
      "source": [
        "### üèÉ Quick Test (Recommended for First Run)\n",
        "Validate the experiment workflow using a small number of samples, takes about 30-60 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "quick_test"
      },
      "outputs": [],
      "source": [
        "# Quick test mode\n",
        "!python -m colab.experiment --mode quick"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "medium_test_section"
      },
      "source": [
        "### üìä Medium-Scale Experiment\n",
        "Use a moderate number of samples, balancing time and result quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "medium_test"
      },
      "outputs": [],
      "source": [
        "# Medium-scale experiment (make sure quick test runs successfully first)\n",
        "!python -m colab.experiment --mode medium"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "full_test_section"
      },
      "source": [
        "### üî¨ Full Experiment\n",
        "Use the complete dataset for the experiment, may take several hours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "full_test"
      },
      "outputs": [],
      "source": [
        "# Full experiment (run only when you have enough time)\n",
        "## Evaluation only\n",
        "!python -m colab.experiment --mode full --skip-attention\n",
        "\n",
        "## SFT only\n",
        "!python -m colab.experiment --mode full --skip-zero-shot --enable-sft --skip-attention\n",
        "\n",
        "## Evaluation + SFT\n",
        "!python -m colab.experiment --mode full --enable-sft --skip-attention\n",
        "\n",
        "## All steps: Evaluation + SFT + Attention Analysis\n",
        "!python -m colab.experiment --mode full --enable-sft"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%reset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyghnEPL73Bm",
        "outputId": "fca607f5-831d-46ba-a958-835cec75a145"
      },
      "execution_count": 24,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SFT on all samples:\n",
        "## SFT + Attention Analysis\n",
        "!python -m main --max-samples 50 --sft-samples 7473 --sft-epochs 1 --skip-zero-shot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1lAjOhN5XOq",
        "outputId": "9a7bbdfb-86d7-496c-caff-b04b67b5f65a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-02 23:20:24.555666: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1748906424.589585    4380 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1748906424.610868    4380 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-02 23:20:24.668618: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "üî¨ Running FULL EXPERIMENT\n",
            "================================================================================\n",
            "DIFFLAMA VS LLAMA: NOISE ROBUSTNESS EXPERIMENT\n",
            "================================================================================\n",
            "Start time: 2025-06-02 23:20:30\n",
            "‚úì All required dependencies found\n",
            "‚úì Directory ready: data\n",
            "‚úì Directory ready: results\n",
            "‚úì Directory ready: results/attention_maps\n",
            "‚úì Directory ready: models_finetuned\n",
            "\n",
            "================================================================================\n",
            "STEP 1: DATA PREPARATION\n",
            "================================================================================\n",
            "‚úì GSM8K dataset already exists\n",
            "‚úì Noisy datasets already exist\n",
            "‚úì Data preparation completed\n",
            "\n",
            "‚è≠Ô∏è  Skipping zero-shot evaluation\n",
            "\n",
            "================================================================================\n",
            "STEP 3: SUPERVISED FINE-TUNING\n",
            "================================================================================\n",
            "================================================================================\n",
            "STARTING SUPERVISED FINE-TUNING PIPELINE\n",
            "================================================================================\n",
            "Creating training subset with 7473 samples...\n",
            "Training subset saved to data/gsm8k_train_sft.jsonl with 7473 samples\n",
            "\n",
            "==================== FINE-TUNING LLAMA ====================\n",
            "Fine-tuning llama model...\n",
            "Device: cuda\n",
            "Training samples: 7473\n",
            "Epochs: 1\n",
            "Batch size: 4\n",
            "Learning rate: 5e-05\n",
            "Loading Llama-375M from: ./cache/models--reyllama--Llama_375M/snapshots/416b70824d560b02245268c208ffd5388b4aa056/checkpoint-64434\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting: '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting: '<|finetune_right_pad_id|>', ID: 128004\n",
            "llama loaded on cuda.\n",
            "Loading training data from data/gsm8k_train_sft.jsonl...\n",
            "Prepared 7473 training examples\n",
            "Tokenizing training data...\n",
            "Map: 100% 7473/7473 [00:08<00:00, 853.93 examples/s]\n",
            "Starting fine-tuning for llama...\n",
            "{'loss': 2.4686, 'grad_norm': 7.710882186889648, 'learning_rate': 4.975922953451044e-05, 'epoch': 0.01}\n",
            "{'loss': 2.1106, 'grad_norm': 7.39383602142334, 'learning_rate': 4.949170679507759e-05, 'epoch': 0.01}\n",
            "{'loss': 2.0389, 'grad_norm': 6.492391586303711, 'learning_rate': 4.922418405564474e-05, 'epoch': 0.02}\n",
            "{'loss': 2.0991, 'grad_norm': 7.515586853027344, 'learning_rate': 4.8956661316211885e-05, 'epoch': 0.02}\n",
            "{'loss': 2.0769, 'grad_norm': 6.71142578125, 'learning_rate': 4.8689138576779034e-05, 'epoch': 0.03}\n",
            "{'loss': 2.0111, 'grad_norm': 6.884924411773682, 'learning_rate': 4.842161583734618e-05, 'epoch': 0.03}\n",
            "{'loss': 1.8983, 'grad_norm': 6.658547878265381, 'learning_rate': 4.815409309791333e-05, 'epoch': 0.04}\n",
            "{'loss': 1.9105, 'grad_norm': 6.464270114898682, 'learning_rate': 4.788657035848048e-05, 'epoch': 0.04}\n",
            "{'loss': 1.9207, 'grad_norm': 5.362054347991943, 'learning_rate': 4.761904761904762e-05, 'epoch': 0.05}\n",
            "{'loss': 1.8602, 'grad_norm': 6.584726810455322, 'learning_rate': 4.735152487961477e-05, 'epoch': 0.05}\n",
            "{'loss': 1.8825, 'grad_norm': 5.7943854331970215, 'learning_rate': 4.708400214018192e-05, 'epoch': 0.06}\n",
            "{'loss': 1.981, 'grad_norm': 6.447309494018555, 'learning_rate': 4.6816479400749066e-05, 'epoch': 0.06}\n",
            "{'loss': 1.9782, 'grad_norm': 6.512517929077148, 'learning_rate': 4.6548956661316214e-05, 'epoch': 0.07}\n",
            "{'loss': 1.8696, 'grad_norm': 5.3769755363464355, 'learning_rate': 4.628143392188336e-05, 'epoch': 0.07}\n",
            "{'loss': 1.7738, 'grad_norm': 6.3563551902771, 'learning_rate': 4.601391118245051e-05, 'epoch': 0.08}\n",
            "{'loss': 1.8643, 'grad_norm': 6.241040229797363, 'learning_rate': 4.574638844301766e-05, 'epoch': 0.09}\n",
            "{'loss': 1.9484, 'grad_norm': 5.640344142913818, 'learning_rate': 4.547886570358481e-05, 'epoch': 0.09}\n",
            "{'loss': 1.9131, 'grad_norm': 5.272248268127441, 'learning_rate': 4.5211342964151956e-05, 'epoch': 0.1}\n",
            "{'loss': 1.8214, 'grad_norm': 5.147177219390869, 'learning_rate': 4.4943820224719104e-05, 'epoch': 0.1}\n",
            "{'loss': 1.9202, 'grad_norm': 6.250762462615967, 'learning_rate': 4.467629748528625e-05, 'epoch': 0.11}\n",
            "{'loss': 1.8356, 'grad_norm': 6.4745635986328125, 'learning_rate': 4.44087747458534e-05, 'epoch': 0.11}\n",
            "{'loss': 1.8506, 'grad_norm': 4.2790727615356445, 'learning_rate': 4.414125200642055e-05, 'epoch': 0.12}\n",
            "{'loss': 1.8219, 'grad_norm': 6.249500751495361, 'learning_rate': 4.38737292669877e-05, 'epoch': 0.12}\n",
            "{'loss': 1.8921, 'grad_norm': 5.596974849700928, 'learning_rate': 4.3606206527554846e-05, 'epoch': 0.13}\n",
            "{'loss': 1.7453, 'grad_norm': 5.610856056213379, 'learning_rate': 4.3338683788121995e-05, 'epoch': 0.13}\n",
            "{'loss': 1.6952, 'grad_norm': 5.698556900024414, 'learning_rate': 4.307116104868914e-05, 'epoch': 0.14}\n",
            "{'loss': 1.8043, 'grad_norm': 5.492650032043457, 'learning_rate': 4.280363830925629e-05, 'epoch': 0.14}\n",
            "{'loss': 1.837, 'grad_norm': 5.767235279083252, 'learning_rate': 4.253611556982344e-05, 'epoch': 0.15}\n",
            "{'loss': 1.8139, 'grad_norm': 5.143636226654053, 'learning_rate': 4.226859283039059e-05, 'epoch': 0.16}\n",
            "{'loss': 1.8439, 'grad_norm': 5.283181667327881, 'learning_rate': 4.2001070090957737e-05, 'epoch': 0.16}\n",
            "{'loss': 1.7586, 'grad_norm': 5.503965854644775, 'learning_rate': 4.1733547351524885e-05, 'epoch': 0.17}\n",
            "{'loss': 1.7857, 'grad_norm': 5.2120795249938965, 'learning_rate': 4.146602461209203e-05, 'epoch': 0.17}\n",
            "{'loss': 1.8318, 'grad_norm': 4.667128562927246, 'learning_rate': 4.119850187265918e-05, 'epoch': 0.18}\n",
            "{'loss': 1.8209, 'grad_norm': 6.122904300689697, 'learning_rate': 4.093097913322633e-05, 'epoch': 0.18}\n",
            "{'loss': 1.7944, 'grad_norm': 5.134989261627197, 'learning_rate': 4.066345639379348e-05, 'epoch': 0.19}\n",
            "{'loss': 1.7218, 'grad_norm': 5.837385177612305, 'learning_rate': 4.039593365436063e-05, 'epoch': 0.19}\n",
            "{'loss': 1.8344, 'grad_norm': 4.790799617767334, 'learning_rate': 4.0128410914927775e-05, 'epoch': 0.2}\n",
            "{'loss': 1.7119, 'grad_norm': 5.68754243850708, 'learning_rate': 3.9860888175494924e-05, 'epoch': 0.2}\n",
            "{'loss': 1.6425, 'grad_norm': 4.8636250495910645, 'learning_rate': 3.959336543606207e-05, 'epoch': 0.21}\n",
            "{'loss': 1.7519, 'grad_norm': 5.58227014541626, 'learning_rate': 3.9325842696629214e-05, 'epoch': 0.21}\n",
            "{'loss': 1.7599, 'grad_norm': 4.851140975952148, 'learning_rate': 3.905831995719636e-05, 'epoch': 0.22}\n",
            "{'loss': 1.8647, 'grad_norm': 4.623716831207275, 'learning_rate': 3.879079721776351e-05, 'epoch': 0.22}\n",
            "{'loss': 1.7692, 'grad_norm': 6.006775379180908, 'learning_rate': 3.852327447833066e-05, 'epoch': 0.23}\n",
            "{'loss': 1.7493, 'grad_norm': 5.2148261070251465, 'learning_rate': 3.825575173889781e-05, 'epoch': 0.24}\n",
            "{'loss': 1.6976, 'grad_norm': 6.080453395843506, 'learning_rate': 3.7988228999464956e-05, 'epoch': 0.24}\n",
            "{'loss': 1.7948, 'grad_norm': 5.546862602233887, 'learning_rate': 3.7720706260032104e-05, 'epoch': 0.25}\n",
            "{'loss': 1.7401, 'grad_norm': 5.070540428161621, 'learning_rate': 3.745318352059925e-05, 'epoch': 0.25}\n",
            "{'loss': 1.7158, 'grad_norm': 5.1061787605285645, 'learning_rate': 3.71856607811664e-05, 'epoch': 0.26}\n",
            "{'loss': 1.729, 'grad_norm': 4.751321792602539, 'learning_rate': 3.691813804173355e-05, 'epoch': 0.26}\n",
            "{'loss': 1.7444, 'grad_norm': 5.27414083480835, 'learning_rate': 3.66506153023007e-05, 'epoch': 0.27}\n",
            "{'loss': 1.7826, 'grad_norm': 5.531082630157471, 'learning_rate': 3.6383092562867846e-05, 'epoch': 0.27}\n",
            "{'loss': 1.7031, 'grad_norm': 4.81363582611084, 'learning_rate': 3.6115569823434994e-05, 'epoch': 0.28}\n",
            "{'loss': 1.7664, 'grad_norm': 4.720686435699463, 'learning_rate': 3.584804708400214e-05, 'epoch': 0.28}\n",
            "{'loss': 1.7553, 'grad_norm': 6.527250289916992, 'learning_rate': 3.558052434456929e-05, 'epoch': 0.29}\n",
            "{'loss': 1.7639, 'grad_norm': 5.242690086364746, 'learning_rate': 3.531300160513644e-05, 'epoch': 0.29}\n",
            "{'loss': 1.6018, 'grad_norm': 5.440169334411621, 'learning_rate': 3.504547886570359e-05, 'epoch': 0.3}\n",
            "{'loss': 1.7182, 'grad_norm': 4.32728385925293, 'learning_rate': 3.4777956126270736e-05, 'epoch': 0.31}\n",
            "{'loss': 1.7159, 'grad_norm': 5.131317615509033, 'learning_rate': 3.4510433386837885e-05, 'epoch': 0.31}\n",
            "{'loss': 1.6503, 'grad_norm': 5.350683212280273, 'learning_rate': 3.424291064740503e-05, 'epoch': 0.32}\n",
            "{'loss': 1.6747, 'grad_norm': 4.677755355834961, 'learning_rate': 3.397538790797218e-05, 'epoch': 0.32}\n",
            "{'loss': 1.6412, 'grad_norm': 5.09818696975708, 'learning_rate': 3.370786516853933e-05, 'epoch': 0.33}\n",
            "{'loss': 1.5661, 'grad_norm': 5.987263202667236, 'learning_rate': 3.344034242910647e-05, 'epoch': 0.33}\n",
            "{'loss': 1.669, 'grad_norm': 4.84131383895874, 'learning_rate': 3.317281968967362e-05, 'epoch': 0.34}\n",
            "{'loss': 1.6125, 'grad_norm': 5.293638706207275, 'learning_rate': 3.290529695024077e-05, 'epoch': 0.34}\n",
            "{'loss': 1.6716, 'grad_norm': 4.937939167022705, 'learning_rate': 3.263777421080792e-05, 'epoch': 0.35}\n",
            "{'loss': 1.6625, 'grad_norm': 4.228416442871094, 'learning_rate': 3.2370251471375065e-05, 'epoch': 0.35}\n",
            "{'loss': 1.6548, 'grad_norm': 4.831337928771973, 'learning_rate': 3.2102728731942213e-05, 'epoch': 0.36}\n",
            "{'loss': 1.5768, 'grad_norm': 4.866718292236328, 'learning_rate': 3.183520599250936e-05, 'epoch': 0.36}\n",
            "{'loss': 1.7142, 'grad_norm': 6.353050708770752, 'learning_rate': 3.156768325307651e-05, 'epoch': 0.37}\n",
            "{'loss': 1.5093, 'grad_norm': 5.308817386627197, 'learning_rate': 3.130016051364366e-05, 'epoch': 0.37}\n",
            "{'loss': 1.7392, 'grad_norm': 6.0578999519348145, 'learning_rate': 3.103263777421081e-05, 'epoch': 0.38}\n",
            "{'loss': 1.5436, 'grad_norm': 5.103486061096191, 'learning_rate': 3.0765115034777955e-05, 'epoch': 0.39}\n",
            "{'loss': 1.5556, 'grad_norm': 5.374453067779541, 'learning_rate': 3.0497592295345107e-05, 'epoch': 0.39}\n",
            "{'loss': 1.5985, 'grad_norm': 5.099724769592285, 'learning_rate': 3.0230069555912256e-05, 'epoch': 0.4}\n",
            "{'loss': 1.6227, 'grad_norm': 4.658552646636963, 'learning_rate': 2.9962546816479404e-05, 'epoch': 0.4}\n",
            "{'loss': 1.6463, 'grad_norm': 4.516212463378906, 'learning_rate': 2.9695024077046552e-05, 'epoch': 0.41}\n",
            "{'loss': 1.6176, 'grad_norm': 5.072113513946533, 'learning_rate': 2.94275013376137e-05, 'epoch': 0.41}\n",
            "{'loss': 1.6926, 'grad_norm': 4.88979959487915, 'learning_rate': 2.915997859818085e-05, 'epoch': 0.42}\n",
            "{'loss': 1.6368, 'grad_norm': 4.794417381286621, 'learning_rate': 2.8892455858747998e-05, 'epoch': 0.42}\n",
            "{'loss': 1.6578, 'grad_norm': 5.712533473968506, 'learning_rate': 2.8624933119315146e-05, 'epoch': 0.43}\n",
            "{'loss': 1.6233, 'grad_norm': 4.552298545837402, 'learning_rate': 2.8357410379882294e-05, 'epoch': 0.43}\n",
            "{'loss': 1.5794, 'grad_norm': 4.650752544403076, 'learning_rate': 2.8089887640449443e-05, 'epoch': 0.44}\n",
            "{'loss': 1.6534, 'grad_norm': 5.003698348999023, 'learning_rate': 2.782236490101659e-05, 'epoch': 0.44}\n",
            "{'loss': 1.5917, 'grad_norm': 4.380573272705078, 'learning_rate': 2.755484216158374e-05, 'epoch': 0.45}\n",
            "{'loss': 1.6168, 'grad_norm': 6.23445463180542, 'learning_rate': 2.7287319422150888e-05, 'epoch': 0.45}\n",
            "{'loss': 1.7251, 'grad_norm': 5.574749946594238, 'learning_rate': 2.7019796682718036e-05, 'epoch': 0.46}\n",
            "{'loss': 1.5234, 'grad_norm': 4.529772758483887, 'learning_rate': 2.6752273943285185e-05, 'epoch': 0.47}\n",
            "{'loss': 1.6026, 'grad_norm': 4.853870868682861, 'learning_rate': 2.6484751203852326e-05, 'epoch': 0.47}\n",
            "{'loss': 1.6572, 'grad_norm': 4.496481418609619, 'learning_rate': 2.6217228464419475e-05, 'epoch': 0.48}\n",
            "{'loss': 1.5858, 'grad_norm': 4.764199256896973, 'learning_rate': 2.5949705724986623e-05, 'epoch': 0.48}\n",
            "{'loss': 1.6115, 'grad_norm': 4.88102912902832, 'learning_rate': 2.568218298555377e-05, 'epoch': 0.49}\n",
            "{'loss': 1.601, 'grad_norm': 4.08353853225708, 'learning_rate': 2.541466024612092e-05, 'epoch': 0.49}\n",
            "{'loss': 1.5545, 'grad_norm': 4.429121971130371, 'learning_rate': 2.5147137506688068e-05, 'epoch': 0.5}\n",
            "{'loss': 1.5536, 'grad_norm': 4.865725517272949, 'learning_rate': 2.487961476725522e-05, 'epoch': 0.5}\n",
            "{'loss': 1.5361, 'grad_norm': 4.356485843658447, 'learning_rate': 2.461209202782237e-05, 'epoch': 0.51}\n",
            "{'loss': 1.5923, 'grad_norm': 4.564931392669678, 'learning_rate': 2.4344569288389517e-05, 'epoch': 0.51}\n",
            "{'loss': 1.6311, 'grad_norm': 5.082540512084961, 'learning_rate': 2.4077046548956665e-05, 'epoch': 0.52}\n",
            "{'loss': 1.6254, 'grad_norm': 4.55072546005249, 'learning_rate': 2.380952380952381e-05, 'epoch': 0.52}\n",
            "{'loss': 1.6137, 'grad_norm': 4.3665852546691895, 'learning_rate': 2.354200107009096e-05, 'epoch': 0.53}\n",
            "{'loss': 1.4849, 'grad_norm': 5.056692600250244, 'learning_rate': 2.3274478330658107e-05, 'epoch': 0.54}\n",
            "{'loss': 1.6172, 'grad_norm': 4.324281215667725, 'learning_rate': 2.3006955591225255e-05, 'epoch': 0.54}\n",
            "{'loss': 1.4936, 'grad_norm': 4.681723117828369, 'learning_rate': 2.2739432851792404e-05, 'epoch': 0.55}\n",
            "{'loss': 1.5591, 'grad_norm': 4.644924640655518, 'learning_rate': 2.2471910112359552e-05, 'epoch': 0.55}\n",
            "{'loss': 1.5956, 'grad_norm': 4.151395320892334, 'learning_rate': 2.22043873729267e-05, 'epoch': 0.56}\n",
            "{'loss': 1.6751, 'grad_norm': 5.030176162719727, 'learning_rate': 2.193686463349385e-05, 'epoch': 0.56}\n",
            "{'loss': 1.5513, 'grad_norm': 4.630366325378418, 'learning_rate': 2.1669341894060997e-05, 'epoch': 0.57}\n",
            "{'loss': 1.5122, 'grad_norm': 6.194973945617676, 'learning_rate': 2.1401819154628146e-05, 'epoch': 0.57}\n",
            "{'loss': 1.5137, 'grad_norm': 4.7107133865356445, 'learning_rate': 2.1134296415195294e-05, 'epoch': 0.58}\n",
            "{'loss': 1.5002, 'grad_norm': 5.106865882873535, 'learning_rate': 2.0866773675762442e-05, 'epoch': 0.58}\n",
            "{'loss': 1.521, 'grad_norm': 4.064514636993408, 'learning_rate': 2.059925093632959e-05, 'epoch': 0.59}\n",
            "{'loss': 1.616, 'grad_norm': 5.153470039367676, 'learning_rate': 2.033172819689674e-05, 'epoch': 0.59}\n",
            "{'loss': 1.5586, 'grad_norm': 3.95341420173645, 'learning_rate': 2.0064205457463888e-05, 'epoch': 0.6}\n",
            "{'loss': 1.662, 'grad_norm': 4.918670177459717, 'learning_rate': 1.9796682718031036e-05, 'epoch': 0.6}\n",
            "{'loss': 1.5516, 'grad_norm': 3.932668447494507, 'learning_rate': 1.952915997859818e-05, 'epoch': 0.61}\n",
            "{'loss': 1.5008, 'grad_norm': 4.026895046234131, 'learning_rate': 1.926163723916533e-05, 'epoch': 0.62}\n",
            "{'loss': 1.4295, 'grad_norm': 4.59721565246582, 'learning_rate': 1.8994114499732478e-05, 'epoch': 0.62}\n",
            "{'loss': 1.5845, 'grad_norm': 4.1332011222839355, 'learning_rate': 1.8726591760299626e-05, 'epoch': 0.63}\n",
            "{'loss': 1.5531, 'grad_norm': 4.097070693969727, 'learning_rate': 1.8459069020866775e-05, 'epoch': 0.63}\n",
            "{'loss': 1.4601, 'grad_norm': 4.051064491271973, 'learning_rate': 1.8191546281433923e-05, 'epoch': 0.64}\n",
            "{'loss': 1.479, 'grad_norm': 4.462271690368652, 'learning_rate': 1.792402354200107e-05, 'epoch': 0.64}\n",
            "{'loss': 1.5112, 'grad_norm': 5.5701751708984375, 'learning_rate': 1.765650080256822e-05, 'epoch': 0.65}\n",
            "{'loss': 1.5168, 'grad_norm': 4.704504489898682, 'learning_rate': 1.7388978063135368e-05, 'epoch': 0.65}\n",
            "{'loss': 1.6471, 'grad_norm': 4.503874778747559, 'learning_rate': 1.7121455323702517e-05, 'epoch': 0.66}\n",
            "{'loss': 1.5088, 'grad_norm': 4.474127292633057, 'learning_rate': 1.6853932584269665e-05, 'epoch': 0.66}\n",
            "{'loss': 1.486, 'grad_norm': 3.729233741760254, 'learning_rate': 1.658640984483681e-05, 'epoch': 0.67}\n",
            "{'loss': 1.4971, 'grad_norm': 4.612123489379883, 'learning_rate': 1.631888710540396e-05, 'epoch': 0.67}\n",
            "{'loss': 1.5163, 'grad_norm': 4.21294641494751, 'learning_rate': 1.6051364365971107e-05, 'epoch': 0.68}\n",
            "{'loss': 1.6649, 'grad_norm': 5.8183746337890625, 'learning_rate': 1.5783841626538255e-05, 'epoch': 0.69}\n",
            "{'loss': 1.4364, 'grad_norm': 5.304134845733643, 'learning_rate': 1.5516318887105404e-05, 'epoch': 0.69}\n",
            "{'loss': 1.423, 'grad_norm': 5.354799747467041, 'learning_rate': 1.5248796147672554e-05, 'epoch': 0.7}\n",
            "{'loss': 1.5205, 'grad_norm': 4.186451435089111, 'learning_rate': 1.4981273408239702e-05, 'epoch': 0.7}\n",
            "{'loss': 1.5192, 'grad_norm': 4.855930328369141, 'learning_rate': 1.471375066880685e-05, 'epoch': 0.71}\n",
            "{'loss': 1.4663, 'grad_norm': 4.458408355712891, 'learning_rate': 1.4446227929373999e-05, 'epoch': 0.71}\n",
            "{'loss': 1.5385, 'grad_norm': 5.1317291259765625, 'learning_rate': 1.4178705189941147e-05, 'epoch': 0.72}\n",
            "{'loss': 1.5178, 'grad_norm': 4.006033897399902, 'learning_rate': 1.3911182450508296e-05, 'epoch': 0.72}\n",
            "{'loss': 1.5253, 'grad_norm': 4.992156505584717, 'learning_rate': 1.3643659711075444e-05, 'epoch': 0.73}\n",
            "{'loss': 1.4749, 'grad_norm': 4.041798114776611, 'learning_rate': 1.3376136971642592e-05, 'epoch': 0.73}\n",
            "{'loss': 1.4716, 'grad_norm': 4.656774520874023, 'learning_rate': 1.3108614232209737e-05, 'epoch': 0.74}\n",
            "{'loss': 1.4564, 'grad_norm': 4.869903087615967, 'learning_rate': 1.2841091492776886e-05, 'epoch': 0.74}\n",
            "{'loss': 1.4039, 'grad_norm': 3.3640968799591064, 'learning_rate': 1.2573568753344034e-05, 'epoch': 0.75}\n",
            "{'loss': 1.5279, 'grad_norm': 4.925705909729004, 'learning_rate': 1.2306046013911184e-05, 'epoch': 0.75}\n",
            "{'loss': 1.4243, 'grad_norm': 4.935454368591309, 'learning_rate': 1.2038523274478333e-05, 'epoch': 0.76}\n",
            "{'loss': 1.4797, 'grad_norm': 4.354747772216797, 'learning_rate': 1.177100053504548e-05, 'epoch': 0.77}\n",
            "{'loss': 1.4469, 'grad_norm': 5.5105791091918945, 'learning_rate': 1.1503477795612628e-05, 'epoch': 0.77}\n",
            "{'loss': 1.4502, 'grad_norm': 5.138343811035156, 'learning_rate': 1.1235955056179776e-05, 'epoch': 0.78}\n",
            "{'loss': 1.4859, 'grad_norm': 4.562192916870117, 'learning_rate': 1.0968432316746924e-05, 'epoch': 0.78}\n",
            "{'loss': 1.5216, 'grad_norm': 4.102634906768799, 'learning_rate': 1.0700909577314073e-05, 'epoch': 0.79}\n",
            "{'loss': 1.5087, 'grad_norm': 4.172418117523193, 'learning_rate': 1.0433386837881221e-05, 'epoch': 0.79}\n",
            "{'loss': 1.5519, 'grad_norm': 4.046884536743164, 'learning_rate': 1.016586409844837e-05, 'epoch': 0.8}\n",
            "{'loss': 1.4614, 'grad_norm': 3.6585533618927, 'learning_rate': 9.898341359015518e-06, 'epoch': 0.8}\n",
            "{'loss': 1.4815, 'grad_norm': 4.635512828826904, 'learning_rate': 9.630818619582665e-06, 'epoch': 0.81}\n",
            "{'loss': 1.5014, 'grad_norm': 4.593347549438477, 'learning_rate': 9.363295880149813e-06, 'epoch': 0.81}\n",
            "{'loss': 1.5058, 'grad_norm': 4.792293548583984, 'learning_rate': 9.095773140716961e-06, 'epoch': 0.82}\n",
            "{'loss': 1.5016, 'grad_norm': 5.127095699310303, 'learning_rate': 8.82825040128411e-06, 'epoch': 0.82}\n",
            "{'loss': 1.5048, 'grad_norm': 4.5618672370910645, 'learning_rate': 8.560727661851258e-06, 'epoch': 0.83}\n",
            "{'loss': 1.4643, 'grad_norm': 4.641732215881348, 'learning_rate': 8.293204922418405e-06, 'epoch': 0.83}\n",
            "{'loss': 1.4911, 'grad_norm': 5.088928699493408, 'learning_rate': 8.025682182985553e-06, 'epoch': 0.84}\n",
            "{'loss': 1.5511, 'grad_norm': 3.895843267440796, 'learning_rate': 7.758159443552702e-06, 'epoch': 0.85}\n",
            "{'loss': 1.5145, 'grad_norm': 4.140117168426514, 'learning_rate': 7.490636704119851e-06, 'epoch': 0.85}\n",
            "{'loss': 1.4806, 'grad_norm': 4.677988052368164, 'learning_rate': 7.223113964686999e-06, 'epoch': 0.86}\n",
            "{'loss': 1.4813, 'grad_norm': 4.707076072692871, 'learning_rate': 6.955591225254148e-06, 'epoch': 0.86}\n",
            "{'loss': 1.4291, 'grad_norm': 4.941504001617432, 'learning_rate': 6.688068485821296e-06, 'epoch': 0.87}\n",
            "{'loss': 1.5198, 'grad_norm': 5.1621832847595215, 'learning_rate': 6.420545746388443e-06, 'epoch': 0.87}\n",
            "{'loss': 1.4733, 'grad_norm': 4.333670616149902, 'learning_rate': 6.153023006955592e-06, 'epoch': 0.88}\n",
            "{'loss': 1.5411, 'grad_norm': 4.984453201293945, 'learning_rate': 5.88550026752274e-06, 'epoch': 0.88}\n",
            "{'loss': 1.4726, 'grad_norm': 5.188155651092529, 'learning_rate': 5.617977528089888e-06, 'epoch': 0.89}\n",
            "{'loss': 1.5299, 'grad_norm': 3.9904685020446777, 'learning_rate': 5.3504547886570364e-06, 'epoch': 0.89}\n",
            "{'loss': 1.4286, 'grad_norm': 4.734035015106201, 'learning_rate': 5.082932049224185e-06, 'epoch': 0.9}\n",
            "{'loss': 1.4101, 'grad_norm': 4.2527618408203125, 'learning_rate': 4.815409309791332e-06, 'epoch': 0.9}\n",
            "{'loss': 1.4692, 'grad_norm': 4.93960428237915, 'learning_rate': 4.547886570358481e-06, 'epoch': 0.91}\n",
            "{'loss': 1.3679, 'grad_norm': 5.0264692306518555, 'learning_rate': 4.280363830925629e-06, 'epoch': 0.92}\n",
            "{'loss': 1.4625, 'grad_norm': 4.968451499938965, 'learning_rate': 4.012841091492777e-06, 'epoch': 0.92}\n",
            "{'loss': 1.5061, 'grad_norm': 3.8930623531341553, 'learning_rate': 3.7453183520599255e-06, 'epoch': 0.93}\n",
            "{'loss': 1.4524, 'grad_norm': 5.682309150695801, 'learning_rate': 3.477795612627074e-06, 'epoch': 0.93}\n",
            "{'loss': 1.4052, 'grad_norm': 4.558748245239258, 'learning_rate': 3.2102728731942214e-06, 'epoch': 0.94}\n",
            "{'loss': 1.4156, 'grad_norm': 3.424144983291626, 'learning_rate': 2.94275013376137e-06, 'epoch': 0.94}\n",
            "{'loss': 1.4208, 'grad_norm': 4.571402549743652, 'learning_rate': 2.6752273943285182e-06, 'epoch': 0.95}\n",
            "{'loss': 1.5079, 'grad_norm': 3.724116086959839, 'learning_rate': 2.407704654895666e-06, 'epoch': 0.95}\n",
            "{'loss': 1.4178, 'grad_norm': 4.1228179931640625, 'learning_rate': 2.1401819154628146e-06, 'epoch': 0.96}\n",
            "{'loss': 1.3372, 'grad_norm': 4.9062910079956055, 'learning_rate': 1.8726591760299627e-06, 'epoch': 0.96}\n",
            "{'loss': 1.4413, 'grad_norm': 5.056373596191406, 'learning_rate': 1.6051364365971107e-06, 'epoch': 0.97}\n",
            "{'loss': 1.3728, 'grad_norm': 4.060412406921387, 'learning_rate': 1.3376136971642591e-06, 'epoch': 0.97}\n",
            "{'loss': 1.4342, 'grad_norm': 4.50340461730957, 'learning_rate': 1.0700909577314073e-06, 'epoch': 0.98}\n",
            "{'loss': 1.5489, 'grad_norm': 5.351808071136475, 'learning_rate': 8.025682182985554e-07, 'epoch': 0.98}\n",
            "{'loss': 1.4171, 'grad_norm': 5.443467617034912, 'learning_rate': 5.350454788657036e-07, 'epoch': 0.99}\n",
            "{'loss': 1.4632, 'grad_norm': 4.00404167175293, 'learning_rate': 2.675227394328518e-07, 'epoch': 1.0}\n",
            "{'train_runtime': 5681.1844, 'train_samples_per_second': 1.315, 'train_steps_per_second': 0.329, 'train_loss': 1.628662116518628, 'epoch': 1.0}\n",
            "100% 1869/1869 [1:34:41<00:00,  3.04s/it]\n",
            "Fine-tuning completed for llama\n",
            "Fine-tuned model saved to ./models_finetuned/llama_sft\n",
            "‚úì llama fine-tuning completed: ./models_finetuned/llama_sft\n",
            "\n",
            "==================== FINE-TUNING DIFFLLAMA ====================\n",
            "Fine-tuning diffllama model...\n",
            "Device: cuda\n",
            "Training samples: 7473\n",
            "Epochs: 1\n",
            "Batch size: 4\n",
            "Learning rate: 5e-05\n",
            "Loading DiffLlama-375M from: ./cache/models--reyllama--DiffLlama-375M/snapshots/8960f22033190f1560537f4932fe649828ef53e2/checkpoint-64434\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting: '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting: '<|finetune_right_pad_id|>', ID: 128004\n",
            "diffllama loaded on cuda.\n",
            "Loading training data from data/gsm8k_train_sft.jsonl...\n",
            "Prepared 7473 training examples\n",
            "Tokenizing training data...\n",
            "Map: 100% 7473/7473 [00:10<00:00, 729.28 examples/s]\n",
            "Starting fine-tuning for diffllama...\n",
            "{'loss': 2.4615, 'grad_norm': 11.878734588623047, 'learning_rate': 4.975922953451044e-05, 'epoch': 0.01}\n",
            "{'loss': 2.1591, 'grad_norm': 12.533675193786621, 'learning_rate': 4.949170679507759e-05, 'epoch': 0.01}\n",
            "{'loss': 2.0505, 'grad_norm': 9.34647274017334, 'learning_rate': 4.922418405564474e-05, 'epoch': 0.02}\n",
            "{'loss': 2.0924, 'grad_norm': 10.621565818786621, 'learning_rate': 4.8956661316211885e-05, 'epoch': 0.02}\n",
            "{'loss': 2.1035, 'grad_norm': 10.591066360473633, 'learning_rate': 4.8689138576779034e-05, 'epoch': 0.03}\n",
            "{'loss': 2.0302, 'grad_norm': 10.557415962219238, 'learning_rate': 4.842161583734618e-05, 'epoch': 0.03}\n",
            "{'loss': 1.9405, 'grad_norm': 11.440848350524902, 'learning_rate': 4.815409309791333e-05, 'epoch': 0.04}\n",
            "{'loss': 1.9251, 'grad_norm': 8.771401405334473, 'learning_rate': 4.788657035848048e-05, 'epoch': 0.04}\n",
            "{'loss': 1.9466, 'grad_norm': 7.756270408630371, 'learning_rate': 4.761904761904762e-05, 'epoch': 0.05}\n",
            "{'loss': 1.9126, 'grad_norm': 10.940784454345703, 'learning_rate': 4.735152487961477e-05, 'epoch': 0.05}\n",
            "{'loss': 1.887, 'grad_norm': 9.984405517578125, 'learning_rate': 4.708400214018192e-05, 'epoch': 0.06}\n",
            "{'loss': 2.0055, 'grad_norm': 9.912908554077148, 'learning_rate': 4.6816479400749066e-05, 'epoch': 0.06}\n",
            "{'loss': 2.0018, 'grad_norm': 8.290452003479004, 'learning_rate': 4.6548956661316214e-05, 'epoch': 0.07}\n",
            "{'loss': 1.8804, 'grad_norm': 7.723081588745117, 'learning_rate': 4.628143392188336e-05, 'epoch': 0.07}\n",
            "{'loss': 1.8352, 'grad_norm': 8.246539115905762, 'learning_rate': 4.601391118245051e-05, 'epoch': 0.08}\n",
            "{'loss': 1.885, 'grad_norm': 8.26729679107666, 'learning_rate': 4.574638844301766e-05, 'epoch': 0.09}\n",
            "{'loss': 1.966, 'grad_norm': 9.309667587280273, 'learning_rate': 4.547886570358481e-05, 'epoch': 0.09}\n",
            "{'loss': 1.9528, 'grad_norm': 7.054538726806641, 'learning_rate': 4.5211342964151956e-05, 'epoch': 0.1}\n",
            "{'loss': 1.8637, 'grad_norm': 9.003312110900879, 'learning_rate': 4.4943820224719104e-05, 'epoch': 0.1}\n",
            "{'loss': 1.9445, 'grad_norm': 9.088077545166016, 'learning_rate': 4.467629748528625e-05, 'epoch': 0.11}\n",
            "{'loss': 1.8859, 'grad_norm': 8.172725677490234, 'learning_rate': 4.44087747458534e-05, 'epoch': 0.11}\n",
            "{'loss': 1.9037, 'grad_norm': 6.420482635498047, 'learning_rate': 4.414125200642055e-05, 'epoch': 0.12}\n",
            "{'loss': 1.8449, 'grad_norm': 10.15073013305664, 'learning_rate': 4.38737292669877e-05, 'epoch': 0.12}\n",
            "{'loss': 1.9326, 'grad_norm': 7.373175144195557, 'learning_rate': 4.3606206527554846e-05, 'epoch': 0.13}\n",
            "{'loss': 1.753, 'grad_norm': 8.274986267089844, 'learning_rate': 4.3338683788121995e-05, 'epoch': 0.13}\n",
            "{'loss': 1.7436, 'grad_norm': 7.506980895996094, 'learning_rate': 4.307116104868914e-05, 'epoch': 0.14}\n",
            "{'loss': 1.8398, 'grad_norm': 7.5033488273620605, 'learning_rate': 4.280363830925629e-05, 'epoch': 0.14}\n",
            "{'loss': 1.8572, 'grad_norm': 8.285242080688477, 'learning_rate': 4.253611556982344e-05, 'epoch': 0.15}\n",
            "{'loss': 1.8447, 'grad_norm': 7.573273658752441, 'learning_rate': 4.226859283039059e-05, 'epoch': 0.16}\n",
            "{'loss': 1.8885, 'grad_norm': 7.017474174499512, 'learning_rate': 4.2001070090957737e-05, 'epoch': 0.16}\n",
            "{'loss': 1.7974, 'grad_norm': 7.808419704437256, 'learning_rate': 4.1733547351524885e-05, 'epoch': 0.17}\n",
            "{'loss': 1.8467, 'grad_norm': 9.708889961242676, 'learning_rate': 4.146602461209203e-05, 'epoch': 0.17}\n",
            "{'loss': 1.8688, 'grad_norm': 6.830509662628174, 'learning_rate': 4.119850187265918e-05, 'epoch': 0.18}\n",
            "{'loss': 1.8688, 'grad_norm': 8.299408912658691, 'learning_rate': 4.093097913322633e-05, 'epoch': 0.18}\n",
            "{'loss': 1.8051, 'grad_norm': 6.84838342666626, 'learning_rate': 4.066345639379348e-05, 'epoch': 0.19}\n",
            "{'loss': 1.7531, 'grad_norm': 9.112634658813477, 'learning_rate': 4.039593365436063e-05, 'epoch': 0.19}\n",
            "{'loss': 1.8685, 'grad_norm': 9.392624855041504, 'learning_rate': 4.0128410914927775e-05, 'epoch': 0.2}\n",
            "{'loss': 1.7458, 'grad_norm': 8.274811744689941, 'learning_rate': 3.9860888175494924e-05, 'epoch': 0.2}\n",
            "{'loss': 1.6694, 'grad_norm': 7.881504058837891, 'learning_rate': 3.959336543606207e-05, 'epoch': 0.21}\n",
            "{'loss': 1.7782, 'grad_norm': 7.501461982727051, 'learning_rate': 3.9325842696629214e-05, 'epoch': 0.21}\n",
            "{'loss': 1.798, 'grad_norm': 7.160229206085205, 'learning_rate': 3.905831995719636e-05, 'epoch': 0.22}\n",
            "{'loss': 1.8981, 'grad_norm': 6.598957061767578, 'learning_rate': 3.879079721776351e-05, 'epoch': 0.22}\n",
            "{'loss': 1.7925, 'grad_norm': 9.088151931762695, 'learning_rate': 3.852327447833066e-05, 'epoch': 0.23}\n",
            "{'loss': 1.78, 'grad_norm': 7.849057674407959, 'learning_rate': 3.825575173889781e-05, 'epoch': 0.24}\n",
            "{'loss': 1.7417, 'grad_norm': 8.28905200958252, 'learning_rate': 3.7988228999464956e-05, 'epoch': 0.24}\n",
            "{'loss': 1.8247, 'grad_norm': 7.334263324737549, 'learning_rate': 3.7720706260032104e-05, 'epoch': 0.25}\n",
            "{'loss': 1.7665, 'grad_norm': 8.856938362121582, 'learning_rate': 3.745318352059925e-05, 'epoch': 0.25}\n",
            "{'loss': 1.7209, 'grad_norm': 7.026973247528076, 'learning_rate': 3.71856607811664e-05, 'epoch': 0.26}\n",
            "{'loss': 1.7367, 'grad_norm': 6.299609184265137, 'learning_rate': 3.691813804173355e-05, 'epoch': 0.26}\n",
            "{'loss': 1.787, 'grad_norm': 7.475745677947998, 'learning_rate': 3.66506153023007e-05, 'epoch': 0.27}\n",
            "{'loss': 1.7985, 'grad_norm': 9.633315086364746, 'learning_rate': 3.6383092562867846e-05, 'epoch': 0.27}\n",
            "{'loss': 1.7218, 'grad_norm': 6.881845474243164, 'learning_rate': 3.6115569823434994e-05, 'epoch': 0.28}\n",
            "{'loss': 1.7711, 'grad_norm': 6.064846515655518, 'learning_rate': 3.584804708400214e-05, 'epoch': 0.28}\n",
            "{'loss': 1.7354, 'grad_norm': 11.112347602844238, 'learning_rate': 3.558052434456929e-05, 'epoch': 0.29}\n",
            "{'loss': 1.7859, 'grad_norm': 7.008575916290283, 'learning_rate': 3.531300160513644e-05, 'epoch': 0.29}\n",
            "{'loss': 1.653, 'grad_norm': 7.583080291748047, 'learning_rate': 3.504547886570359e-05, 'epoch': 0.3}\n",
            "{'loss': 1.7373, 'grad_norm': 6.145267009735107, 'learning_rate': 3.4777956126270736e-05, 'epoch': 0.31}\n",
            "{'loss': 1.7469, 'grad_norm': 8.172782897949219, 'learning_rate': 3.4510433386837885e-05, 'epoch': 0.31}\n",
            "{'loss': 1.6792, 'grad_norm': 6.546689510345459, 'learning_rate': 3.424291064740503e-05, 'epoch': 0.32}\n",
            "{'loss': 1.7032, 'grad_norm': 7.230948448181152, 'learning_rate': 3.397538790797218e-05, 'epoch': 0.32}\n",
            "{'loss': 1.6556, 'grad_norm': 6.364543914794922, 'learning_rate': 3.370786516853933e-05, 'epoch': 0.33}\n",
            "{'loss': 1.5617, 'grad_norm': 7.5417914390563965, 'learning_rate': 3.344034242910647e-05, 'epoch': 0.33}\n",
            "{'loss': 1.6875, 'grad_norm': 6.789012908935547, 'learning_rate': 3.317281968967362e-05, 'epoch': 0.34}\n",
            "{'loss': 1.6162, 'grad_norm': 6.520273208618164, 'learning_rate': 3.290529695024077e-05, 'epoch': 0.34}\n",
            "{'loss': 1.7032, 'grad_norm': 23.245939254760742, 'learning_rate': 3.263777421080792e-05, 'epoch': 0.35}\n",
            "{'loss': 1.6979, 'grad_norm': 6.463021755218506, 'learning_rate': 3.2370251471375065e-05, 'epoch': 0.35}\n",
            "{'loss': 1.6538, 'grad_norm': 6.258920192718506, 'learning_rate': 3.2102728731942213e-05, 'epoch': 0.36}\n",
            "{'loss': 1.5883, 'grad_norm': 6.6510009765625, 'learning_rate': 3.183520599250936e-05, 'epoch': 0.36}\n",
            "{'loss': 1.7406, 'grad_norm': 9.019067764282227, 'learning_rate': 3.156768325307651e-05, 'epoch': 0.37}\n",
            "{'loss': 1.541, 'grad_norm': 6.835079669952393, 'learning_rate': 3.130016051364366e-05, 'epoch': 0.37}\n",
            "{'loss': 1.7503, 'grad_norm': 7.000855445861816, 'learning_rate': 3.103263777421081e-05, 'epoch': 0.38}\n",
            "{'loss': 1.5519, 'grad_norm': 6.702695369720459, 'learning_rate': 3.0765115034777955e-05, 'epoch': 0.39}\n",
            "{'loss': 1.5937, 'grad_norm': 7.217665672302246, 'learning_rate': 3.0497592295345107e-05, 'epoch': 0.39}\n",
            "{'loss': 1.6253, 'grad_norm': 6.782568454742432, 'learning_rate': 3.0230069555912256e-05, 'epoch': 0.4}\n",
            "{'loss': 1.6423, 'grad_norm': 6.102225303649902, 'learning_rate': 2.9962546816479404e-05, 'epoch': 0.4}\n",
            "{'loss': 1.6769, 'grad_norm': 5.975606918334961, 'learning_rate': 2.9695024077046552e-05, 'epoch': 0.41}\n",
            "{'loss': 1.6311, 'grad_norm': 6.817192077636719, 'learning_rate': 2.94275013376137e-05, 'epoch': 0.41}\n",
            "{'loss': 1.7132, 'grad_norm': 8.804901123046875, 'learning_rate': 2.915997859818085e-05, 'epoch': 0.42}\n",
            "{'loss': 1.6473, 'grad_norm': 6.779774188995361, 'learning_rate': 2.8892455858747998e-05, 'epoch': 0.42}\n",
            "{'loss': 1.6724, 'grad_norm': 6.993314743041992, 'learning_rate': 2.8624933119315146e-05, 'epoch': 0.43}\n",
            "{'loss': 1.6354, 'grad_norm': 6.553788661956787, 'learning_rate': 2.8357410379882294e-05, 'epoch': 0.43}\n",
            "{'loss': 1.6095, 'grad_norm': 5.795150279998779, 'learning_rate': 2.8089887640449443e-05, 'epoch': 0.44}\n",
            "{'loss': 1.6725, 'grad_norm': 6.7957940101623535, 'learning_rate': 2.782236490101659e-05, 'epoch': 0.44}\n",
            "{'loss': 1.6316, 'grad_norm': 6.058192729949951, 'learning_rate': 2.755484216158374e-05, 'epoch': 0.45}\n",
            "{'loss': 1.6311, 'grad_norm': 7.600073337554932, 'learning_rate': 2.7287319422150888e-05, 'epoch': 0.45}\n",
            "{'loss': 1.7257, 'grad_norm': 6.6059699058532715, 'learning_rate': 2.7019796682718036e-05, 'epoch': 0.46}\n",
            "{'loss': 1.5352, 'grad_norm': 6.486733913421631, 'learning_rate': 2.6752273943285185e-05, 'epoch': 0.47}\n",
            "{'loss': 1.6087, 'grad_norm': 6.350088596343994, 'learning_rate': 2.6484751203852326e-05, 'epoch': 0.47}\n",
            "{'loss': 1.6421, 'grad_norm': 6.125678062438965, 'learning_rate': 2.6217228464419475e-05, 'epoch': 0.48}\n",
            "{'loss': 1.5958, 'grad_norm': 6.089606761932373, 'learning_rate': 2.5949705724986623e-05, 'epoch': 0.48}\n",
            "{'loss': 1.6349, 'grad_norm': 7.436362266540527, 'learning_rate': 2.568218298555377e-05, 'epoch': 0.49}\n",
            "{'loss': 1.592, 'grad_norm': 6.742980003356934, 'learning_rate': 2.541466024612092e-05, 'epoch': 0.49}\n",
            "{'loss': 1.5778, 'grad_norm': 5.762633323669434, 'learning_rate': 2.5147137506688068e-05, 'epoch': 0.5}\n",
            "{'loss': 1.5579, 'grad_norm': 8.159317016601562, 'learning_rate': 2.487961476725522e-05, 'epoch': 0.5}\n",
            "{'loss': 1.5597, 'grad_norm': 6.740106582641602, 'learning_rate': 2.461209202782237e-05, 'epoch': 0.51}\n",
            "{'loss': 1.6093, 'grad_norm': 5.5093770027160645, 'learning_rate': 2.4344569288389517e-05, 'epoch': 0.51}\n",
            "{'loss': 1.6352, 'grad_norm': 7.223130702972412, 'learning_rate': 2.4077046548956665e-05, 'epoch': 0.52}\n",
            "{'loss': 1.6269, 'grad_norm': 6.117430686950684, 'learning_rate': 2.380952380952381e-05, 'epoch': 0.52}\n",
            "{'loss': 1.6207, 'grad_norm': 5.990460395812988, 'learning_rate': 2.354200107009096e-05, 'epoch': 0.53}\n",
            "{'loss': 1.4887, 'grad_norm': 6.8622660636901855, 'learning_rate': 2.3274478330658107e-05, 'epoch': 0.54}\n",
            "{'loss': 1.6284, 'grad_norm': 5.531461238861084, 'learning_rate': 2.3006955591225255e-05, 'epoch': 0.54}\n",
            "{'loss': 1.4904, 'grad_norm': 6.390186786651611, 'learning_rate': 2.2739432851792404e-05, 'epoch': 0.55}\n",
            "{'loss': 1.5634, 'grad_norm': 6.981412410736084, 'learning_rate': 2.2471910112359552e-05, 'epoch': 0.55}\n",
            "{'loss': 1.6066, 'grad_norm': 5.631515979766846, 'learning_rate': 2.22043873729267e-05, 'epoch': 0.56}\n",
            "{'loss': 1.6741, 'grad_norm': 6.394903659820557, 'learning_rate': 2.193686463349385e-05, 'epoch': 0.56}\n",
            "{'loss': 1.5592, 'grad_norm': 6.316895008087158, 'learning_rate': 2.1669341894060997e-05, 'epoch': 0.57}\n",
            "{'loss': 1.5117, 'grad_norm': 8.391201972961426, 'learning_rate': 2.1401819154628146e-05, 'epoch': 0.57}\n",
            "{'loss': 1.5233, 'grad_norm': 6.570881366729736, 'learning_rate': 2.1134296415195294e-05, 'epoch': 0.58}\n",
            "{'loss': 1.5079, 'grad_norm': 6.566844940185547, 'learning_rate': 2.0866773675762442e-05, 'epoch': 0.58}\n",
            "{'loss': 1.538, 'grad_norm': 5.438694953918457, 'learning_rate': 2.059925093632959e-05, 'epoch': 0.59}\n",
            "{'loss': 1.6398, 'grad_norm': 6.324207305908203, 'learning_rate': 2.033172819689674e-05, 'epoch': 0.59}\n",
            "{'loss': 1.5802, 'grad_norm': 4.853237152099609, 'learning_rate': 2.0064205457463888e-05, 'epoch': 0.6}\n",
            "{'loss': 1.6852, 'grad_norm': 6.502613067626953, 'learning_rate': 1.9796682718031036e-05, 'epoch': 0.6}\n",
            "{'loss': 1.5577, 'grad_norm': 5.100254058837891, 'learning_rate': 1.952915997859818e-05, 'epoch': 0.61}\n",
            "{'loss': 1.5004, 'grad_norm': 5.230032444000244, 'learning_rate': 1.926163723916533e-05, 'epoch': 0.62}\n",
            "{'loss': 1.4319, 'grad_norm': 5.685513019561768, 'learning_rate': 1.8994114499732478e-05, 'epoch': 0.62}\n",
            "{'loss': 1.5846, 'grad_norm': 5.594658851623535, 'learning_rate': 1.8726591760299626e-05, 'epoch': 0.63}\n",
            "{'loss': 1.5464, 'grad_norm': 5.374022960662842, 'learning_rate': 1.8459069020866775e-05, 'epoch': 0.63}\n",
            "{'loss': 1.4822, 'grad_norm': 5.2957282066345215, 'learning_rate': 1.8191546281433923e-05, 'epoch': 0.64}\n",
            "{'loss': 1.4756, 'grad_norm': 5.809424877166748, 'learning_rate': 1.792402354200107e-05, 'epoch': 0.64}\n",
            "{'loss': 1.5151, 'grad_norm': 6.987916469573975, 'learning_rate': 1.765650080256822e-05, 'epoch': 0.65}\n",
            "{'loss': 1.5227, 'grad_norm': 6.384822368621826, 'learning_rate': 1.7388978063135368e-05, 'epoch': 0.65}\n",
            "{'loss': 1.6549, 'grad_norm': 7.288594722747803, 'learning_rate': 1.7121455323702517e-05, 'epoch': 0.66}\n",
            "{'loss': 1.5127, 'grad_norm': 5.99009895324707, 'learning_rate': 1.6853932584269665e-05, 'epoch': 0.66}\n",
            "{'loss': 1.4882, 'grad_norm': 4.882195472717285, 'learning_rate': 1.658640984483681e-05, 'epoch': 0.67}\n",
            "{'loss': 1.4931, 'grad_norm': 5.800650596618652, 'learning_rate': 1.631888710540396e-05, 'epoch': 0.67}\n",
            "{'loss': 1.5356, 'grad_norm': 6.248959064483643, 'learning_rate': 1.6051364365971107e-05, 'epoch': 0.68}\n",
            "{'loss': 1.6701, 'grad_norm': 6.723344326019287, 'learning_rate': 1.5783841626538255e-05, 'epoch': 0.69}\n",
            "{'loss': 1.4553, 'grad_norm': 9.355461120605469, 'learning_rate': 1.5516318887105404e-05, 'epoch': 0.69}\n",
            "{'loss': 1.4219, 'grad_norm': 5.980839729309082, 'learning_rate': 1.5248796147672554e-05, 'epoch': 0.7}\n",
            "{'loss': 1.515, 'grad_norm': 5.495022773742676, 'learning_rate': 1.4981273408239702e-05, 'epoch': 0.7}\n",
            "{'loss': 1.5188, 'grad_norm': 7.058588981628418, 'learning_rate': 1.471375066880685e-05, 'epoch': 0.71}\n",
            "{'loss': 1.445, 'grad_norm': 5.2624430656433105, 'learning_rate': 1.4446227929373999e-05, 'epoch': 0.71}\n",
            "{'loss': 1.5418, 'grad_norm': 6.77055549621582, 'learning_rate': 1.4178705189941147e-05, 'epoch': 0.72}\n",
            "{'loss': 1.5133, 'grad_norm': 5.510284423828125, 'learning_rate': 1.3911182450508296e-05, 'epoch': 0.72}\n",
            "{'loss': 1.5245, 'grad_norm': 6.688456058502197, 'learning_rate': 1.3643659711075444e-05, 'epoch': 0.73}\n",
            "{'loss': 1.4734, 'grad_norm': 4.729337215423584, 'learning_rate': 1.3376136971642592e-05, 'epoch': 0.73}\n",
            "{'loss': 1.4651, 'grad_norm': 5.741664409637451, 'learning_rate': 1.3108614232209737e-05, 'epoch': 0.74}\n",
            "{'loss': 1.443, 'grad_norm': 6.0188164710998535, 'learning_rate': 1.2841091492776886e-05, 'epoch': 0.74}\n",
            "{'loss': 1.3996, 'grad_norm': 4.556349754333496, 'learning_rate': 1.2573568753344034e-05, 'epoch': 0.75}\n",
            "{'loss': 1.545, 'grad_norm': 6.87191104888916, 'learning_rate': 1.2306046013911184e-05, 'epoch': 0.75}\n",
            "{'loss': 1.4179, 'grad_norm': 7.4514312744140625, 'learning_rate': 1.2038523274478333e-05, 'epoch': 0.76}\n",
            "{'loss': 1.4799, 'grad_norm': 5.336031913757324, 'learning_rate': 1.177100053504548e-05, 'epoch': 0.77}\n",
            "{'loss': 1.4651, 'grad_norm': 7.030922889709473, 'learning_rate': 1.1503477795612628e-05, 'epoch': 0.77}\n",
            "{'loss': 1.4536, 'grad_norm': 6.767943859100342, 'learning_rate': 1.1235955056179776e-05, 'epoch': 0.78}\n",
            "{'loss': 1.4663, 'grad_norm': 5.248081684112549, 'learning_rate': 1.0968432316746924e-05, 'epoch': 0.78}\n",
            "{'loss': 1.5097, 'grad_norm': 5.096709728240967, 'learning_rate': 1.0700909577314073e-05, 'epoch': 0.79}\n",
            "{'loss': 1.5142, 'grad_norm': 5.951312065124512, 'learning_rate': 1.0433386837881221e-05, 'epoch': 0.79}\n",
            "{'loss': 1.5499, 'grad_norm': 4.754688739776611, 'learning_rate': 1.016586409844837e-05, 'epoch': 0.8}\n",
            "{'loss': 1.4696, 'grad_norm': 4.685690879821777, 'learning_rate': 9.898341359015518e-06, 'epoch': 0.8}\n",
            "{'loss': 1.4688, 'grad_norm': 5.9161272048950195, 'learning_rate': 9.630818619582665e-06, 'epoch': 0.81}\n",
            "{'loss': 1.5076, 'grad_norm': 5.21413516998291, 'learning_rate': 9.363295880149813e-06, 'epoch': 0.81}\n",
            "{'loss': 1.5205, 'grad_norm': 6.200561046600342, 'learning_rate': 9.095773140716961e-06, 'epoch': 0.82}\n",
            "{'loss': 1.5068, 'grad_norm': 6.750777721405029, 'learning_rate': 8.82825040128411e-06, 'epoch': 0.82}\n",
            "{'loss': 1.5118, 'grad_norm': 6.541315078735352, 'learning_rate': 8.560727661851258e-06, 'epoch': 0.83}\n",
            "{'loss': 1.4663, 'grad_norm': 6.003997802734375, 'learning_rate': 8.293204922418405e-06, 'epoch': 0.83}\n",
            "{'loss': 1.4906, 'grad_norm': 6.497124671936035, 'learning_rate': 8.025682182985553e-06, 'epoch': 0.84}\n",
            "{'loss': 1.5679, 'grad_norm': 4.931722164154053, 'learning_rate': 7.758159443552702e-06, 'epoch': 0.85}\n",
            "{'loss': 1.5147, 'grad_norm': 5.247038841247559, 'learning_rate': 7.490636704119851e-06, 'epoch': 0.85}\n",
            "{'loss': 1.4855, 'grad_norm': 5.770443439483643, 'learning_rate': 7.223113964686999e-06, 'epoch': 0.86}\n",
            "{'loss': 1.4884, 'grad_norm': 6.275717735290527, 'learning_rate': 6.955591225254148e-06, 'epoch': 0.86}\n",
            "{'loss': 1.4295, 'grad_norm': 5.9280829429626465, 'learning_rate': 6.688068485821296e-06, 'epoch': 0.87}\n",
            "{'loss': 1.5027, 'grad_norm': 8.027390480041504, 'learning_rate': 6.420545746388443e-06, 'epoch': 0.87}\n",
            "{'loss': 1.4772, 'grad_norm': 5.713385581970215, 'learning_rate': 6.153023006955592e-06, 'epoch': 0.88}\n",
            "{'loss': 1.5459, 'grad_norm': 6.1639533042907715, 'learning_rate': 5.88550026752274e-06, 'epoch': 0.88}\n",
            "{'loss': 1.473, 'grad_norm': 7.288859844207764, 'learning_rate': 5.617977528089888e-06, 'epoch': 0.89}\n",
            "{'loss': 1.5222, 'grad_norm': 5.386969566345215, 'learning_rate': 5.3504547886570364e-06, 'epoch': 0.89}\n",
            "{'loss': 1.4131, 'grad_norm': 6.396021842956543, 'learning_rate': 5.082932049224185e-06, 'epoch': 0.9}\n",
            "{'loss': 1.4021, 'grad_norm': 5.5250091552734375, 'learning_rate': 4.815409309791332e-06, 'epoch': 0.9}\n",
            "{'loss': 1.4667, 'grad_norm': 7.591830253601074, 'learning_rate': 4.547886570358481e-06, 'epoch': 0.91}\n",
            "{'loss': 1.3651, 'grad_norm': 6.252076148986816, 'learning_rate': 4.280363830925629e-06, 'epoch': 0.92}\n",
            "{'loss': 1.4578, 'grad_norm': 6.795162200927734, 'learning_rate': 4.012841091492777e-06, 'epoch': 0.92}\n",
            "{'loss': 1.4959, 'grad_norm': 5.011119365692139, 'learning_rate': 3.7453183520599255e-06, 'epoch': 0.93}\n",
            "{'loss': 1.4531, 'grad_norm': 6.485016345977783, 'learning_rate': 3.477795612627074e-06, 'epoch': 0.93}\n",
            "{'loss': 1.4025, 'grad_norm': 6.368717193603516, 'learning_rate': 3.2102728731942214e-06, 'epoch': 0.94}\n",
            "{'loss': 1.4256, 'grad_norm': 4.734869956970215, 'learning_rate': 2.94275013376137e-06, 'epoch': 0.94}\n",
            "{'loss': 1.4195, 'grad_norm': 5.837057590484619, 'learning_rate': 2.6752273943285182e-06, 'epoch': 0.95}\n",
            "{'loss': 1.4998, 'grad_norm': 5.04879093170166, 'learning_rate': 2.407704654895666e-06, 'epoch': 0.95}\n",
            "{'loss': 1.4078, 'grad_norm': 5.502012729644775, 'learning_rate': 2.1401819154628146e-06, 'epoch': 0.96}\n",
            "{'loss': 1.3369, 'grad_norm': 6.556541919708252, 'learning_rate': 1.8726591760299627e-06, 'epoch': 0.96}\n",
            "{'loss': 1.4374, 'grad_norm': 6.509045124053955, 'learning_rate': 1.6051364365971107e-06, 'epoch': 0.97}\n",
            "{'loss': 1.3716, 'grad_norm': 4.716671466827393, 'learning_rate': 1.3376136971642591e-06, 'epoch': 0.97}\n",
            "{'loss': 1.4447, 'grad_norm': 5.724679470062256, 'learning_rate': 1.0700909577314073e-06, 'epoch': 0.98}\n",
            "{'loss': 1.5513, 'grad_norm': 7.412860870361328, 'learning_rate': 8.025682182985554e-07, 'epoch': 0.98}\n",
            "{'loss': 1.4271, 'grad_norm': 6.972139358520508, 'learning_rate': 5.350454788657036e-07, 'epoch': 0.99}\n",
            "{'loss': 1.4604, 'grad_norm': 6.8143157958984375, 'learning_rate': 2.675227394328518e-07, 'epoch': 1.0}\n",
            "{'train_runtime': 6196.0739, 'train_samples_per_second': 1.206, 'train_steps_per_second': 0.302, 'train_loss': 1.6416753602576422, 'epoch': 1.0}\n",
            "100% 1869/1869 [1:43:16<00:00,  3.32s/it]\n",
            "Fine-tuning completed for diffllama\n",
            "Fine-tuned model saved to ./models_finetuned/diffllama_sft\n",
            "‚úì diffllama fine-tuning completed: ./models_finetuned/diffllama_sft\n",
            "\n",
            "================================================================================\n",
            "SFT PIPELINE RESULTS\n",
            "================================================================================\n",
            "llama: ./models_finetuned/llama_sft\n",
            "diffllama: ./models_finetuned/diffllama_sft\n",
            "‚úì Supervised fine-tuning completed\n",
            "\n",
            "================================================================================\n",
            "STEP 4: POST-SFT EVALUATION\n",
            "================================================================================\n",
            "\n",
            "Evaluating fine-tuned llama...\n",
            "  - Clean dataset...\n",
            "Running evaluation on data/gsm8k_test.jsonl\n",
            "Model: ./models_finetuned/llama_sft\n",
            "Device: cuda\n",
            "Chain-of-thought: True\n",
            "Loading model from path: ./models_finetuned/llama_sft\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Model loaded from ./models_finetuned/llama_sft on cuda.\n",
            "Limited to 50 samples for testing\n",
            "Evaluating llama:   0% 0/50 [00:00<?, ?it/s]\n",
            "Example 1:\n",
            "Question: Janet‚Äôs ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for ...\n",
            "True answer: 18\n",
            "Predicted: 1080\n",
            "Correct: False\n",
            "Generated:  (There are 4 days a week.)\n",
            "\n",
            "Answer: She makes 3 x 16 = <<3*16=48>>48 eggs a day.\n",
            "She makes 48 x 6 = <<48*6=144>>144 eggs a week.\n",
            "She makes 144 x 4 = <<144*4=1920>>1920 eggs a week.\n",
            "She makes 1920/8 =...\n",
            "Evaluating llama:   2% 1/50 [00:02<02:05,  2.56s/it]\n",
            "Example 2:\n",
            "Question: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it ...\n",
            "True answer: 3\n",
            "Predicted: None\n",
            "Correct: False\n",
            "Generated: Answer: First find the total number of bolts in all the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the ...\n",
            "Evaluating llama:   4% 2/50 [00:13<05:57,  7.44s/it]\n",
            "Example 3:\n",
            "Question: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repai...\n",
            "True answer: 70000\n",
            "Predicted: 18000\n",
            "Correct: False\n",
            "Generated:   What was his total profit?\n",
            "\n",
            "Answer: The house cost 80,000*0.2=$<<80000*0.2=12000>>12,000\n",
            "So it cost 10000+12,000=$<<10000+12000=24000>>24,000\n",
            "That means he made 24000-20000=$<<24000-20000=18000>>18,...\n",
            "Evaluating llama: 100% 50/50 [03:54<00:00,  4.69s/it]\n",
            "\n",
            "Evaluation Results:\n",
            "Dataset: data/gsm8k_test.jsonl\n",
            "Model: ./models_finetuned/llama_sft\n",
            "Accuracy: 0.0200 (1/50)\n",
            "Time: 234.60 seconds\n",
            "  - INF dataset...\n",
            "Running evaluation on data/gsm8k_inf_test.jsonl\n",
            "Model: ./models_finetuned/llama_sft\n",
            "Device: cuda\n",
            "Chain-of-thought: True\n",
            "Loading model from path: ./models_finetuned/llama_sft\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Model loaded from ./models_finetuned/llama_sft on cuda.\n",
            "Limited to 50 samples for testing\n",
            "Evaluating llama:   0% 0/50 [00:00<?, ?it/s]\n",
            "Example 1:\n",
            "Question: Janet‚Äôs ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for ...\n",
            "True answer: 18\n",
            "Predicted: 8\n",
            "Correct: False\n",
            "Generated:  (There are 5 days in a month.)\n",
            "\n",
            "Answer: She makes 3 eggs per day because 1 + 4 = <<1+4=3>>3\n",
            "She makes 6 eggs a day because 3 x 7 = <<3*7=6>>6\n",
            "She makes 8 eggs a day because 6 / 4 = <<6/4=8>>8\n",
            "#### 8...\n",
            "Evaluating llama:   2% 1/50 [00:01<01:22,  1.69s/it]\n",
            "Example 2:\n",
            "Question: A robe takes 2 bolts of blue fiber and half that much white fiber. The local library has 10,000 book...\n",
            "True answer: 3\n",
            "Predicted: 750000\n",
            "Correct: False\n",
            "Generated: Answer: First find the number of bolts in total: 100,000 books on shelves + 10,000 books on shelves = <<100000+100000+100000=500000>>500000 books\n",
            "Then add the number of bolts in total to find the tota...\n",
            "Evaluating llama:   4% 2/50 [00:04<01:41,  2.12s/it]\n",
            "Example 3:\n",
            "Question: Josh decides to try flipping a house. The local library has 10,000 books on its shelves..  He buys a...\n",
            "True answer: 70000\n",
            "Predicted: 12000\n",
            "Correct: False\n",
            "Generated:  (This is 10000/10=<<10000/10=1000>>1,500 books\n",
            "He sold 3,000*2=<<30000*2=60000>>60,000 books\n",
            "So he made 60000-10000=$<<60000-10000=12000>>12,000\n",
            "#### 12000...\n",
            "Evaluating llama: 100% 50/50 [03:05<00:00,  3.71s/it]\n",
            "\n",
            "Evaluation Results:\n",
            "Dataset: data/gsm8k_inf_test.jsonl\n",
            "Model: ./models_finetuned/llama_sft\n",
            "Accuracy: 0.0200 (1/50)\n",
            "Time: 185.36 seconds\n",
            "  - RCS dataset...\n",
            "Running evaluation on data/gsm8k_rcs_test.jsonl\n",
            "Model: ./models_finetuned/llama_sft\n",
            "Device: cuda\n",
            "Chain-of-thought: True\n",
            "Loading model from path: ./models_finetuned/llama_sft\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Model loaded from ./models_finetuned/llama_sft on cuda.\n",
            "Limited to 50 samples for testing\n",
            "Evaluating llama:   0% 0/50 [00:00<?, ?it/s]\n",
            "Example 1:\n",
            "Question: Janet‚Äôs ducks lay 16 eggs per day. Consider that the sum of any number and zero is the number itself...\n",
            "True answer: 18\n",
            "Predicted: 1920\n",
            "Correct: False\n",
            "Generated:  (There are 4 days in a week.)\n",
            "\n",
            "Answer: The total number of eggs sold by the farmers is 16 x 3 = <<16*3=48>>48.\n",
            "The total number of eggs sold by the farmers is 48 + 6 = <<48+6=54>>54.\n",
            "The total number...\n",
            "Evaluating llama:   2% 1/50 [00:03<02:57,  3.62s/it]\n",
            "Example 2:\n",
            "Question: A robe takes 2 bolts of blue fiber and half that much white fiber. We should keep in mind that multi...\n",
            "True answer: 3\n",
            "Predicted: 2\n",
            "Correct: False\n",
            "Generated: Answer: First find the number of bolts in red fiber: 2 bolts of blue fiber and half that amount of the same amount of the same amount of the same amount of the same amount of the same amount of the sa...\n",
            "Evaluating llama:   4% 2/50 [00:13<05:56,  7.44s/it]\n",
            "Example 3:\n",
            "Question: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repai...\n",
            "True answer: 70000\n",
            "Predicted: 5000\n",
            "Correct: False\n",
            "Generated:   What was his total profit?\n",
            "\n",
            "Answer: The cost of the house was 80,000*0.2=$<<80000*0.2=20000>>20,000\n",
            "So he made 20000-20,000=$<<20000-20000=10000>>10,000\n",
            "He sold 1/2 of the remaining amount so he mad...\n",
            "Evaluating llama: 100% 50/50 [03:20<00:00,  4.00s/it]\n",
            "\n",
            "Evaluation Results:\n",
            "Dataset: data/gsm8k_rcs_test.jsonl\n",
            "Model: ./models_finetuned/llama_sft\n",
            "Accuracy: 0.0200 (1/50)\n",
            "Time: 200.15 seconds\n",
            "  - SD dataset...\n",
            "Running evaluation on data/gsm8k_sd_test.jsonl\n",
            "Model: ./models_finetuned/llama_sft\n",
            "Device: cuda\n",
            "Chain-of-thought: True\n",
            "Loading model from path: ./models_finetuned/llama_sft\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Model loaded from ./models_finetuned/llama_sft on cuda.\n",
            "Limited to 50 samples for testing\n",
            "Evaluating llama:   0% 0/50 [00:00<?, ?it/s]\n",
            "Example 1:\n",
            "Question: Janet‚Äôs ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for ...\n",
            "True answer: 18\n",
            "Predicted: 160\n",
            "Correct: False\n",
            "Generated:  (There are 4 days a week.)\n",
            "\n",
            "Answer: She makes 3 x 6 = <<3*6=18>>18 eggs a day.\n",
            "She makes 16 x 4 = <<16*4=32>>32 eggs a day.\n",
            "She makes 32 x 5 = <<32*5=160>>160 eggs a day.\n",
            "#### 160...\n",
            "Evaluating llama:   2% 1/50 [00:01<01:11,  1.47s/it]\n",
            "Example 2:\n",
            "Question: A robe takes 2 bolts of blue fiber and half that much white fiber. The weather was sunny and pleasan...\n",
            "True answer: 3\n",
            "Predicted: 2\n",
            "Correct: False\n",
            "Generated: Answer: First find the number of bolts in red fiber and half that amount in white fiber: 2 bolts of blue fiber and half that amount in white fiber: 2 bolts of blue fiber and half that amount in white ...\n",
            "Evaluating llama:   4% 2/50 [00:12<05:32,  6.92s/it]\n",
            "Example 3:\n",
            "Question: Josh decides to try flipping a house. The cashier was friendly and provided excellent customer servi...\n",
            "True answer: 70000\n",
            "Predicted: 4000\n",
            "Correct: False\n",
            "Generated:   What is his total profit?\n",
            "\n",
            "Answer: The cashier gave him $80,000 + $50,000 = $<<80000+5000=100000>>100,000\n",
            "He put $100,000 / $50,000 = $<<100000/50=2000>>2000 into repair\n",
            "So he made a profit of 2000 ...\n",
            "Evaluating llama: 100% 50/50 [03:08<00:00,  3.77s/it]\n",
            "\n",
            "Evaluation Results:\n",
            "Dataset: data/gsm8k_sd_test.jsonl\n",
            "Model: ./models_finetuned/llama_sft\n",
            "Accuracy: 0.0000 (0/50)\n",
            "Time: 188.38 seconds\n",
            "\n",
            "Evaluating fine-tuned diffllama...\n",
            "  - Clean dataset...\n",
            "Running evaluation on data/gsm8k_test.jsonl\n",
            "Model: ./models_finetuned/diffllama_sft\n",
            "Device: cuda\n",
            "Chain-of-thought: True\n",
            "Loading model from path: ./models_finetuned/diffllama_sft\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Model loaded from ./models_finetuned/diffllama_sft on cuda.\n",
            "Limited to 50 samples for testing\n",
            "Evaluating diffllama:   0% 0/50 [00:00<?, ?it/s]\n",
            "Example 1:\n",
            "Question: Janet‚Äôs ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for ...\n",
            "True answer: 18\n",
            "Predicted: 20\n",
            "Correct: False\n",
            "Generated:  (Answer: Janet's ducks lay 16 eggs each day, so they lay 16/4 = <<16/4=4>>4 eggs a day.\n",
            "She makes 3 eggs a day, so she makes 4*3 = <<4*3=12>>12 eggs a day.\n",
            "In total, she makes 12+4 = <<12+4=20>>20 eg...\n",
            "Evaluating diffllama:   2% 1/50 [00:02<01:43,  2.11s/it]\n",
            "Example 2:\n",
            "Question: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it ...\n",
            "True answer: 3\n",
            "Predicted: 11\n",
            "Correct: False\n",
            "Generated:   If the robe uses 3 red beads, 1 yellow bead, and 5 green beads, how many bolts are there in total?\n",
            "\n",
            "Answer: The robe uses 3 x 2 = <<3*2=6>>6 red beads.\n",
            "The robe uses 1/2 x 6 = <<1/2*6=3>>3 yellow be...\n",
            "Evaluating diffllama:   4% 2/50 [00:04<01:55,  2.40s/it]\n",
            "Example 3:\n",
            "Question: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repai...\n",
            "True answer: 70000\n",
            "Predicted: 15\n",
            "Correct: False\n",
            "Generated:   How much does he make?\n",
            "\n",
            "Answer: The house cost $80,000 - $50,000 = $<<80000-40000=2000>>20,000\n",
            "He put in $50,000 / $20,000 = $<<2000/2000=2>>2 per month.\n",
            "So he made $2 * 150% = $<<2*150*.01=15>>15 p...\n",
            "Evaluating diffllama: 100% 50/50 [02:10<00:00,  2.60s/it]\n",
            "\n",
            "Evaluation Results:\n",
            "Dataset: data/gsm8k_test.jsonl\n",
            "Model: ./models_finetuned/diffllama_sft\n",
            "Accuracy: 0.0600 (3/50)\n",
            "Time: 130.07 seconds\n",
            "  - INF dataset...\n",
            "Running evaluation on data/gsm8k_inf_test.jsonl\n",
            "Model: ./models_finetuned/diffllama_sft\n",
            "Device: cuda\n",
            "Chain-of-thought: True\n",
            "Loading model from path: ./models_finetuned/diffllama_sft\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Model loaded from ./models_finetuned/diffllama_sft on cuda.\n",
            "Limited to 50 samples for testing\n",
            "Evaluating diffllama:   0% 0/50 [00:00<?, ?it/s]\n",
            "Example 1:\n",
            "Question: Janet‚Äôs ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for ...\n",
            "True answer: 18\n",
            "Predicted: 8\n",
            "Correct: False\n",
            "Generated:  (Answer: Janet's ducks lay 16 eggs each day, so they lay 16/4 = <<16/4=4>>4 eggs per day.\n",
            "She makes 3 eggs per day, so she has 4*3 = <<4*3=12>>12 eggs per day.\n",
            "The chickens lay 12 eggs per day, so sh...\n",
            "Evaluating diffllama:   2% 1/50 [00:03<02:30,  3.06s/it]\n",
            "Example 2:\n",
            "Question: A robe takes 2 bolts of blue fiber and half that much white fiber. The local library has 10,000 book...\n",
            "True answer: 3\n",
            "Predicted: 60000\n",
            "Correct: False\n",
            "Generated:  (There are 1000 pencils in the closet.)\n",
            "\n",
            "Answer: First find how many bolts in the closet there are: 1000 pencils * 2 bolts = <<1000*2=2000>>2000 bolts\n",
            "Then multiply that number by the number of penci...\n",
            "Evaluating diffllama:   4% 2/50 [00:06<02:27,  3.08s/it]\n",
            "Example 3:\n",
            "Question: Josh decides to try flipping a house. The local library has 10,000 books on its shelves..  He buys a...\n",
            "True answer: 70000\n",
            "Predicted: 750\n",
            "Correct: False\n",
            "Generated:   What was his total savings?\n",
            "\n",
            "Answer: He bought 10,000*10,000=<<10000*2000=600000>>600,000 books\n",
            "He put in 50,000*.2=$<<500000*.2=250000>>250,000\n",
            "So he made 600,000+250,000=$<<600000+250000=750000>>7...\n",
            "Evaluating diffllama: 100% 50/50 [02:26<00:00,  2.93s/it]\n",
            "\n",
            "Evaluation Results:\n",
            "Dataset: data/gsm8k_inf_test.jsonl\n",
            "Model: ./models_finetuned/diffllama_sft\n",
            "Accuracy: 0.0200 (1/50)\n",
            "Time: 146.49 seconds\n",
            "  - RCS dataset...\n",
            "Running evaluation on data/gsm8k_rcs_test.jsonl\n",
            "Model: ./models_finetuned/diffllama_sft\n",
            "Device: cuda\n",
            "Chain-of-thought: True\n",
            "Loading model from path: ./models_finetuned/diffllama_sft\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Model loaded from ./models_finetuned/diffllama_sft on cuda.\n",
            "Limited to 50 samples for testing\n",
            "Evaluating diffllama:   0% 0/50 [00:00<?, ?it/s]\n",
            "Example 1:\n",
            "Question: Janet‚Äôs ducks lay 16 eggs per day. Consider that the sum of any number and zero is the number itself...\n",
            "True answer: 18\n",
            "Predicted: 1680\n",
            "Correct: False\n",
            "Generated:  (Answer: First find how many eggs Janet ate on each day by multiplying the number of eggs by the number of eggs per week: 16 eggs / 4 = <<16/4=4>>4 eggs/day\n",
            "Then multiply the number of eggs by the nu...\n",
            "Evaluating diffllama:   2% 1/50 [00:04<03:27,  4.23s/it]\n",
            "Example 2:\n",
            "Question: A robe takes 2 bolts of blue fiber and half that much white fiber. We should keep in mind that multi...\n",
            "True answer: 3\n",
            "Predicted: 1\n",
            "Correct: False\n",
            "Generated:  1 bolt = <<1=1>>1/4 of the total number of bolts.\n",
            "#### 1...\n",
            "Evaluating diffllama:   4% 2/50 [00:04<01:39,  2.07s/it]\n",
            "Example 3:\n",
            "Question: Josh decides to try flipping a house.  He buys a house for $80,000 and then puts in $50,000 in repai...\n",
            "True answer: 70000\n",
            "Predicted: 2500\n",
            "Correct: False\n",
            "Generated:   What was his original price?\n",
            "\n",
            "Answer: The total cost of the house was 20000*80 = $<<20000*80=1600000>>16,000\n",
            "He put in $50,000 so he made 10000-50,000=$<<10000-50,000=5000>>5,000\n",
            "The new price was 5...\n",
            "Evaluating diffllama: 100% 50/50 [02:08<00:00,  2.57s/it]\n",
            "\n",
            "Evaluation Results:\n",
            "Dataset: data/gsm8k_rcs_test.jsonl\n",
            "Model: ./models_finetuned/diffllama_sft\n",
            "Accuracy: 0.0000 (0/50)\n",
            "Time: 128.60 seconds\n",
            "  - SD dataset...\n",
            "Running evaluation on data/gsm8k_sd_test.jsonl\n",
            "Model: ./models_finetuned/diffllama_sft\n",
            "Device: cuda\n",
            "Chain-of-thought: True\n",
            "Loading model from path: ./models_finetuned/diffllama_sft\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Model loaded from ./models_finetuned/diffllama_sft on cuda.\n",
            "Limited to 50 samples for testing\n",
            "Evaluating diffllama:   0% 0/50 [00:00<?, ?it/s]\n",
            "Example 1:\n",
            "Question: Janet‚Äôs ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for ...\n",
            "True answer: 18\n",
            "Predicted: 6\n",
            "Correct: False\n",
            "Generated:  (Answer: Janet ate 3 * 4 = <<3*4=12>>12 eggs.\n",
            "She sold 1 + 2 = <<1+2=3>>3 eggs to her friends each day.\n",
            "So, she made 3 * 2 = <<3*2=6>>6 eggs a day.\n",
            "Therefore, she makes 12 - 6 = <<12-6=6>>6 eggs ever...\n",
            "Evaluating diffllama:   2% 1/50 [00:02<02:24,  2.95s/it]\n",
            "Example 2:\n",
            "Question: A robe takes 2 bolts of blue fiber and half that much white fiber. The weather was sunny and pleasan...\n",
            "True answer: 3\n",
            "Predicted: 2\n",
            "Correct: False\n",
            "Generated:  (There are 3 inches of blue fiber per square foot.)\n",
            "\n",
            "Answer: It took 1/2*2=<<1/2*2=1>>1 bolt to make the robe.\n",
            "So, it took 1+1=<<1+1=2>>2 bolts to make the robe.\n",
            "#### 2...\n",
            "Evaluating diffllama:   4% 2/50 [00:05<02:02,  2.56s/it]\n",
            "Example 3:\n",
            "Question: Josh decides to try flipping a house. The cashier was friendly and provided excellent customer servi...\n",
            "True answer: 70000\n",
            "Predicted: 110000\n",
            "Correct: False\n",
            "Generated:   What is his total savings?\n",
            "\n",
            "Answer: The total cost of the house was 16000+10000=$<<16000+10000=22000>>22000\n",
            "He put in $50,000 so that's 20000-2500=$<<20000-2500=5000>>5000\n",
            "The total amount he made f...\n",
            "Evaluating diffllama: 100% 50/50 [02:22<00:00,  2.84s/it]\n",
            "\n",
            "Evaluation Results:\n",
            "Dataset: data/gsm8k_sd_test.jsonl\n",
            "Model: ./models_finetuned/diffllama_sft\n",
            "Accuracy: 0.0000 (0/50)\n",
            "Time: 142.21 seconds\n",
            "\n",
            "Post-SFT Results:\n",
            "dataset        Clean   INF   RCS   SD\n",
            "model                                \n",
            "diffllama_sft   0.06  0.02  0.00  0.0\n",
            "llama_sft       0.02  0.02  0.02  0.0\n",
            "‚úì Post-SFT evaluation completed\n",
            "\n",
            "================================================================================\n",
            "STEP 5: ATTENTION VISUALIZATION & ANALYSIS\n",
            "================================================================================\n",
            "Running attention analysis (with fine-tuned models)...\n",
            "Creating attention visualizations...\n",
            "\n",
            "Visualizing question 1...\n",
            "  Using fine-tuned llama model: ./models_finetuned/llama_sft\n",
            "Visualizing attention for llama model from ./models_finetuned/llama_sft...\n",
            "Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How m...\n",
            "Loading model from path: ./models_finetuned/llama_sft\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Model loaded from ./models_finetuned/llama_sft on cuda.\n",
            "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Attention heatmap saved to results/attention_maps/clean_q1_sft/llama_sft_attn_layer-1_head0_sample.png\n",
            "  Using fine-tuned diffllama model: ./models_finetuned/diffllama_sft\n",
            "Visualizing attention for diffllama model from ./models_finetuned/diffllama_sft...\n",
            "Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How m...\n",
            "Loading model from path: ./models_finetuned/diffllama_sft\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Model loaded from ./models_finetuned/diffllama_sft on cuda.\n",
            "DiffLlamaModel is using DiffLlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "Attention heatmap saved to results/attention_maps/clean_q1_sft/diffllama_sft_attn_layer-1_head0_sample.png\n",
            "\n",
            "DiffLlama Analysis Summary (Layer -1, Head 0):\n",
            "  Lambda std dev (config): N/A\n",
            "  Captured hook components: 2\n",
            "  Hooked component keys: lambda_val, module_type_hooked\n",
            "Visualizing attention for llama model from ./models_finetuned/llama_sft...\n",
            "Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. The l...\n",
            "Loading model from path: ./models_finetuned/llama_sft\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Model loaded from ./models_finetuned/llama_sft on cuda.\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Attention heatmap saved to results/attention_maps/noisy_q1_sft/llama_sft_attn_layer-1_head0_sample.png\n",
            "Visualizing attention for diffllama model from ./models_finetuned/diffllama_sft...\n",
            "Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. The l...\n",
            "Loading model from path: ./models_finetuned/diffllama_sft\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Model loaded from ./models_finetuned/diffllama_sft on cuda.\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "Attention heatmap saved to results/attention_maps/noisy_q1_sft/diffllama_sft_attn_layer-1_head0_sample.png\n",
            "\n",
            "DiffLlama Analysis Summary (Layer -1, Head 0):\n",
            "  Lambda std dev (config): N/A\n",
            "  Captured hook components: 2\n",
            "  Hooked component keys: lambda_val, module_type_hooked\n",
            "\n",
            "Visualizing question 2...\n",
            "  Using fine-tuned llama model: ./models_finetuned/llama_sft\n",
            "Visualizing attention for llama model from ./models_finetuned/llama_sft...\n",
            "Question: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much ...\n",
            "Loading model from path: ./models_finetuned/llama_sft\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Model loaded from ./models_finetuned/llama_sft on cuda.\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Attention heatmap saved to results/attention_maps/clean_q2_sft/llama_sft_attn_layer-1_head0_sample.png\n",
            "  Using fine-tuned diffllama model: ./models_finetuned/diffllama_sft\n",
            "Visualizing attention for diffllama model from ./models_finetuned/diffllama_sft...\n",
            "Question: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much ...\n",
            "Loading model from path: ./models_finetuned/diffllama_sft\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Model loaded from ./models_finetuned/diffllama_sft on cuda.\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "Attention heatmap saved to results/attention_maps/clean_q2_sft/diffllama_sft_attn_layer-1_head0_sample.png\n",
            "\n",
            "DiffLlama Analysis Summary (Layer -1, Head 0):\n",
            "  Lambda std dev (config): N/A\n",
            "  Captured hook components: 2\n",
            "  Hooked component keys: lambda_val, module_type_hooked\n",
            "Visualizing attention for llama model from ./models_finetuned/llama_sft...\n",
            "Question: Weng earns $12 an hour for babysitting. There are 7 days in a week and 12 months in a year.. Yesterd...\n",
            "Loading model from path: ./models_finetuned/llama_sft\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Model loaded from ./models_finetuned/llama_sft on cuda.\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Attention heatmap saved to results/attention_maps/noisy_q2_sft/llama_sft_attn_layer-1_head0_sample.png\n",
            "Visualizing attention for diffllama model from ./models_finetuned/diffllama_sft...\n",
            "Question: Weng earns $12 an hour for babysitting. There are 7 days in a week and 12 months in a year.. Yesterd...\n",
            "Loading model from path: ./models_finetuned/diffllama_sft\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Model loaded from ./models_finetuned/diffllama_sft on cuda.\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "Attention heatmap saved to results/attention_maps/noisy_q2_sft/diffllama_sft_attn_layer-1_head0_sample.png\n",
            "\n",
            "DiffLlama Analysis Summary (Layer -1, Head 0):\n",
            "  Lambda std dev (config): N/A\n",
            "  Captured hook components: 2\n",
            "  Hooked component keys: lambda_val, module_type_hooked\n",
            "\n",
            "Visualizing question 3...\n",
            "  Using fine-tuned llama model: ./models_finetuned/llama_sft\n",
            "Visualizing attention for llama model from ./models_finetuned/llama_sft...\n",
            "Question: Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs....\n",
            "Loading model from path: ./models_finetuned/llama_sft\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Model loaded from ./models_finetuned/llama_sft on cuda.\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Attention heatmap saved to results/attention_maps/clean_q3_sft/llama_sft_attn_layer-1_head0_sample.png\n",
            "  Using fine-tuned diffllama model: ./models_finetuned/diffllama_sft\n",
            "Visualizing attention for diffllama model from ./models_finetuned/diffllama_sft...\n",
            "Question: Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs....\n",
            "Loading model from path: ./models_finetuned/diffllama_sft\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Model loaded from ./models_finetuned/diffllama_sft on cuda.\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "Attention heatmap saved to results/attention_maps/clean_q3_sft/diffllama_sft_attn_layer-1_head0_sample.png\n",
            "\n",
            "DiffLlama Analysis Summary (Layer -1, Head 0):\n",
            "  Lambda std dev (config): N/A\n",
            "  Captured hook components: 2\n",
            "  Hooked component keys: lambda_val, module_type_hooked\n",
            "Visualizing attention for llama model from ./models_finetuned/llama_sft...\n",
            "Question: Betty is saving money for a new wallet which costs $100. The temperature outside was 25 degrees Cels...\n",
            "Loading model from path: ./models_finetuned/llama_sft\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Model loaded from ./models_finetuned/llama_sft on cuda.\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Attention heatmap saved to results/attention_maps/noisy_q3_sft/llama_sft_attn_layer-1_head0_sample.png\n",
            "Visualizing attention for diffllama model from ./models_finetuned/diffllama_sft...\n",
            "Question: Betty is saving money for a new wallet which costs $100. The temperature outside was 25 degrees Cels...\n",
            "Loading model from path: ./models_finetuned/diffllama_sft\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Model loaded from ./models_finetuned/diffllama_sft on cuda.\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "Attention heatmap saved to results/attention_maps/noisy_q3_sft/diffllama_sft_attn_layer-1_head0_sample.png\n",
            "\n",
            "DiffLlama Analysis Summary (Layer -1, Head 0):\n",
            "  Lambda std dev (config): N/A\n",
            "  Captured hook components: 2\n",
            "  Hooked component keys: lambda_val, module_type_hooked\n",
            "\n",
            "Running quantitative attention analysis...\n",
            "Comparing attention patterns between models and datasets (SFT)...\n",
            "Using fine-tuned model for llama: ./models_finetuned/llama_sft\n",
            "\n",
            "Analyzing llama on clean data...\n",
            "Quantifying attention allocation for llama from ./models_finetuned/llama_sft...\n",
            "Loading model from path: ./models_finetuned/llama_sft\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Model loaded from ./models_finetuned/llama_sft on cuda.\n",
            "Analyzing llama:   0% 0/20 [00:00<?, ?it/s]Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Analyzing llama:  15% 3/20 [00:00<00:00, 22.77it/s]Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Analyzing llama:  30% 6/20 [00:00<00:00, 20.95it/s]Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Analyzing llama:  45% 9/20 [00:00<00:00, 20.23it/s]Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Analyzing llama:  60% 12/20 [00:00<00:00, 20.35it/s]Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Analyzing llama:  75% 15/20 [00:00<00:00, 21.61it/s]Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Analyzing llama:  90% 18/20 [00:00<00:00, 22.71it/s]Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Analyzing llama: 100% 20/20 [00:00<00:00, 22.15it/s]\n",
            "\n",
            "Attention Allocation Results for llama:\n",
            "KMI (Key Math Info): 0.094 ¬± 0.029\n",
            "NI (Noise Info): 0.000 ¬± 0.000\n",
            "OC (Other Context): 0.906 ¬± 0.029\n",
            "\n",
            "Analyzing llama on noisy data...\n",
            "Quantifying attention allocation for llama from ./models_finetuned/llama_sft...\n",
            "Loading model from path: ./models_finetuned/llama_sft\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Model loaded from ./models_finetuned/llama_sft on cuda.\n",
            "Analyzing llama:   0% 0/20 [00:00<?, ?it/s]Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Analyzing llama:  15% 3/20 [00:00<00:00, 21.53it/s]Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Analyzing llama:  30% 6/20 [00:00<00:00, 20.99it/s]Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Analyzing llama:  45% 9/20 [00:00<00:00, 23.98it/s]Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Analyzing llama:  60% 12/20 [00:00<00:00, 24.26it/s]Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Analyzing llama:  75% 15/20 [00:00<00:00, 24.87it/s]Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Analyzing llama:  90% 18/20 [00:00<00:00, 22.69it/s]Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Extracted attention matrix from model outputs.attentions for layer 15\n",
            "Analyzing llama: 100% 20/20 [00:00<00:00, 22.96it/s]\n",
            "\n",
            "Attention Allocation Results for llama:\n",
            "KMI (Key Math Info): 0.101 ¬± 0.024\n",
            "NI (Noise Info): 0.106 ¬± 0.047\n",
            "OC (Other Context): 0.792 ¬± 0.054\n",
            "Using fine-tuned model for diffllama: ./models_finetuned/diffllama_sft\n",
            "\n",
            "Analyzing diffllama on clean data...\n",
            "Quantifying attention allocation for diffllama from ./models_finetuned/diffllama_sft...\n",
            "Loading model from path: ./models_finetuned/diffllama_sft\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Model loaded from ./models_finetuned/diffllama_sft on cuda.\n",
            "Analyzing diffllama:   0% 0/20 [00:00<?, ?it/s]‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "Analyzing diffllama:  15% 3/20 [00:00<00:00, 25.58it/s]‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "Analyzing diffllama:  30% 6/20 [00:00<00:00, 25.28it/s]‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "Analyzing diffllama:  45% 9/20 [00:00<00:00, 25.77it/s]‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "Analyzing diffllama:  65% 13/20 [00:00<00:00, 27.62it/s]‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "Analyzing diffllama:  80% 16/20 [00:00<00:00, 26.83it/s]‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "Analyzing diffllama: 100% 20/20 [00:00<00:00, 27.19it/s]\n",
            "\n",
            "Attention Allocation Results for diffllama:\n",
            "KMI (Key Math Info): 0.094 ¬± 0.029\n",
            "NI (Noise Info): 0.000 ¬± 0.000\n",
            "OC (Other Context): 0.906 ¬± 0.029\n",
            "\n",
            "Analyzing diffllama on noisy data...\n",
            "Quantifying attention allocation for diffllama from ./models_finetuned/diffllama_sft...\n",
            "Loading model from path: ./models_finetuned/diffllama_sft\n",
            "Tokenizer loaded. EOS token: '<|eot_id|>', ID: 128009\n",
            "Tokenizer BOS token: '<|begin_of_text|>', ID: 128000\n",
            "Tokenizer UNK token: 'None', ID: None\n",
            "Tokenizer PAD token before setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Tokenizer PAD token after setting (from path): '<|finetune_right_pad_id|>', ID: 128004\n",
            "Model loaded from ./models_finetuned/diffllama_sft on cuda.\n",
            "Analyzing diffllama:   0% 0/20 [00:00<?, ?it/s]‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "Analyzing diffllama:  15% 3/20 [00:00<00:00, 21.58it/s]‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "Analyzing diffllama:  30% 6/20 [00:00<00:00, 24.90it/s]‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "Analyzing diffllama:  45% 9/20 [00:00<00:00, 26.47it/s]‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "Analyzing diffllama:  60% 12/20 [00:00<00:00, 27.31it/s]‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "Analyzing diffllama:  75% 15/20 [00:00<00:00, 27.72it/s]‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "Analyzing diffllama:  90% 18/20 [00:00<00:00, 27.94it/s]‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "‚úÖ Successfully extracted DiffLlama differential attention using specialized method\n",
            "Analyzing diffllama: 100% 20/20 [00:00<00:00, 26.81it/s]\n",
            "\n",
            "Attention Allocation Results for diffllama:\n",
            "KMI (Key Math Info): 0.101 ¬± 0.024\n",
            "NI (Noise Info): 0.106 ¬± 0.047\n",
            "OC (Other Context): 0.792 ¬± 0.054\n",
            "\n",
            "================================================================================\n",
            "ATTENTION ALLOCATION COMPARISON (SFT)\n",
            "================================================================================\n",
            "\n",
            "LLAMA (SFT) Model:\n",
            "  Clean Data - KMI: 0.094, NI: 0.000, OC: 0.906\n",
            "  Noisy Data - KMI: 0.101, NI: 0.106, OC: 0.792\n",
            "  Change (Noisy - Clean): KMI: +0.008, NI: +0.106\n",
            "\n",
            "DIFFLLAMA (SFT) Model:\n",
            "  Clean Data - KMI: 0.094, NI: 0.000, OC: 0.906\n",
            "  Noisy Data - KMI: 0.101, NI: 0.106, OC: 0.792\n",
            "  Change (Noisy - Clean): KMI: +0.008, NI: +0.106\n",
            "‚úì Attention analysis completed, results saved to results/attention_analysis_sft.json\n",
            "‚úì Attention visualization and analysis completed\n",
            "\n",
            "================================================================================\n",
            "GENERATING EXPERIMENT REPORT\n",
            "================================================================================\n",
            "‚úì Experiment report saved to results/experiment_report_20250603_030359.json\n",
            "‚úì Total experiment time: 3h 43m 29s\n",
            "\n",
            "================================================================================\n",
            "üéâ EXPERIMENT COMPLETED SUCCESSFULLY!\n",
            "================================================================================\n",
            "Generated files:\n",
            "  ‚úì results/sft_performance.csv\n",
            "  ‚úì results/attention_maps/\n",
            "\n",
            "üìä Check results/ directory for detailed outputs\n",
            "üìà Main results: results/zero_shot_performance.csv\n",
            "üß† Attention maps: results/attention_maps/\n",
            "\n",
            "‚è±Ô∏è  Total runtime: 3h 43m 29s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "custom_section"
      },
      "source": [
        "### üõ† Custom Experiment\n",
        "Adjust experiment parameters as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "custom_experiment"
      },
      "outputs": [],
      "source": [
        "# Custom experiment example\n",
        "# Only run evaluation, skip attention analysis to save time\n",
        "!python -m colab.experiment --mode medium --skip-attention --max-samples 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "results_section"
      },
      "source": [
        "## üìä Step 6: View Experiment Results\n",
        "\n",
        "After completing the experiment, review the generated result files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "list_results",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12b4ae11-b5bf-40d4-ba9d-af08edba4712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "drwx------ 5 root root 4096 Jun  2 23:17 attention_maps\n"
          ]
        }
      ],
      "source": [
        "# List generated result files\n",
        "!ls -la results/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "show_summary",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eae2bf2d-cd5d-4a30-9048-db9afab865c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No experiment summary found. Please run an experiment first.\n"
          ]
        }
      ],
      "source": [
        "# View the latest experiment summary\n",
        "import json\n",
        "import glob\n",
        "\n",
        "# Find the latest summary file\n",
        "summary_files = glob.glob('results/colab_summary_*.json')\n",
        "if summary_files:\n",
        "    latest_summary = max(summary_files)\n",
        "    print(f\"üìã Latest experiment summary: {latest_summary}\")\n",
        "\n",
        "    with open(latest_summary, 'r') as f:\n",
        "        summary = json.load(f)\n",
        "\n",
        "    print(\"\\nüìä Experiment Summary:\")\n",
        "    for key, value in summary.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "else:\n",
        "    print(\"No experiment summary found. Please run an experiment first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "show_results",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d616627-706e-4bbb-f9ee-130ce4fcf1f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No results found. Please run an experiment first.\n"
          ]
        }
      ],
      "source": [
        "# Display main results\n",
        "import pandas as pd\n",
        "\n",
        "# Find the latest results file\n",
        "result_files = glob.glob('results/colab_results_*.csv')\n",
        "if result_files:\n",
        "    latest_results = max(result_files)\n",
        "    print(f\"üìà Latest results: {latest_results}\")\n",
        "\n",
        "    df = pd.read_csv(latest_results)\n",
        "    print(\"\\nüìä Performance Comparison:\")\n",
        "    print(df.pivot(index='model', columns='dataset', values='accuracy'))\n",
        "\n",
        "    # Calculate performance differences\n",
        "    pivot_df = df.pivot(index='model', columns='dataset', values='accuracy')\n",
        "    if 'llama' in pivot_df.index and 'diffllama' in pivot_df.index:\n",
        "        print(\"\\nüîç Performance Difference (DiffLlama - Llama):\")\n",
        "        diff = pivot_df.loc['diffllama'] - pivot_df.loc['llama']\n",
        "        print(diff)\n",
        "else:\n",
        "    print(\"No results found. Please run an experiment first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualization_section"
      },
      "source": [
        "## üìà Step 7: Results Visualization\n",
        "\n",
        "If your experiment included attention analysis, you can view the generated attention heatmaps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "show_attention_maps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f251cbb9-e8aa-406a-c4dd-8ec4ea8623ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Attention Visualization Files:\n",
            "  üìä results/attention_maps/clean_q1/llama_attn_layer-1_head0_sample.png\n",
            "  üìä results/attention_maps/clean_q1/diffllama_attn_layer-1_head0_sample.png\n",
            "  üìä results/attention_maps/noisy_q1/llama_attn_layer-1_head0_sample.png\n",
            "  üìä results/attention_maps/noisy_q1/diffllama_attn_layer-1_head0_sample.png\n",
            "  üìä results/attention_maps/clean_q2/llama_attn_layer-1_head0_sample.png\n"
          ]
        }
      ],
      "source": [
        "# Display attention heatmaps\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "attention_dir = 'results/attention_maps'\n",
        "if os.path.exists(attention_dir):\n",
        "    print(\"üß† Attention Visualization Files:\")\n",
        "\n",
        "    # List all attention map files\n",
        "    for root, dirs, files in os.walk(attention_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.png'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                print(f\"  üìä {file_path}\")\n",
        "\n",
        "                # Display images (optional, uncomment to show)\n",
        "                # display(Image(file_path))\n",
        "else:\n",
        "    print(\"No attention maps found. Run experiment with attention analysis enabled.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "show_attention_analysis",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a57e6e06-1d75-4696-9b80-f4705ea3a16d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No attention analysis found. Run experiment with attention analysis enabled.\n"
          ]
        }
      ],
      "source": [
        "# Display attention analysis results\n",
        "attention_files = glob.glob('results/colab_attention_*.json')\n",
        "if attention_files:\n",
        "    latest_attention = max(attention_files)\n",
        "    print(f\"üß† Latest attention analysis: {latest_attention}\")\n",
        "\n",
        "    with open(latest_attention, 'r') as f:\n",
        "        attention_data = json.load(f)\n",
        "\n",
        "    print(\"\\nüìä Attention Allocation Analysis:\")\n",
        "    for model, data in attention_data.items():\n",
        "        print(f\"\\n{model.upper()} Model:\")\n",
        "        for condition, stats in data.items():\n",
        "            print(f\"  {condition.capitalize()}:\")\n",
        "            print(f\"    KMI (Key Math Info): {stats['kmi_mean']:.3f} ¬± {stats['kmi_std']:.3f}\")\n",
        "            print(f\"    NI (Noise Info): {stats['ni_mean']:.3f} ¬± {stats['ni_std']:.3f}\")\n",
        "            print(f\"    OC (Other Context): {stats['oc_mean']:.3f} ¬± {stats['oc_std']:.3f}\")\n",
        "else:\n",
        "    print(\"No attention analysis found. Run experiment with attention analysis enabled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download_section"
      },
      "source": [
        "## üíæ Step 8: Download Results\n",
        "\n",
        "Download experiment results locally or ensure they are saved in Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "download_results",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b55fe21-ca83-48d8-c8f9-4d11b6c7118a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Results packaged in: experiment_results_20250602_231718.zip\n",
            "You can download this file from Colab's Files panel.\n",
            "\n",
            "üíæ Results are also saved in Google Drive:\n",
            "  /content/drive/MyDrive/DiffLlama_Experiment/\n"
          ]
        }
      ],
      "source": [
        "# Compress result files for download\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "zip_filename = f'experiment_results_{timestamp}.zip'\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "    # Add all files from results directory\n",
        "    for root, dirs, files in os.walk('results'):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            zipf.write(file_path)\n",
        "\n",
        "print(f\"üì¶ Results packaged in: {zip_filename}\")\n",
        "print(\"You can download this file from Colab's Files panel.\")\n",
        "\n",
        "# Reminder if Google Drive was used\n",
        "if os.path.exists('/content/drive/MyDrive/DiffLlama_Experiment'):\n",
        "    print(\"\\nüíæ Results are also saved in Google Drive:\")\n",
        "    print(\"  /content/drive/MyDrive/DiffLlama_Experiment/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "troubleshooting_section"
      },
      "source": [
        "## üõ† Troubleshooting\n",
        "\n",
        "If you encounter issues, try the following solutions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clear_cache"
      },
      "outputs": [],
      "source": [
        "# Clear GPU memory cache\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"‚úÖ GPU cache cleared\")\n",
        "\n",
        "# Check available memory\n",
        "import psutil\n",
        "memory = psutil.virtual_memory()\n",
        "print(f\"üíæ RAM: {memory.available / 1e9:.1f}GB available / {memory.total / 1e9:.1f}GB total\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "restart_runtime"
      },
      "outputs": [],
      "source": [
        "# If memory is insufficient, you can restart the runtime (use with caution)\n",
        "# import os\n",
        "# os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion_section"
      },
      "source": [
        "## üéØ Experiment Conclusions\n",
        "\n",
        "Based on the experiment results, you can analyze the following key questions:\n",
        "\n",
        "1. **Noise Robustness**: Does DiffLlama perform better on noisy data?\n",
        "2. **Attention Mechanism**: Is differential attention more effective at focusing on key information?\n",
        "3. **Performance Degradation**: How do both models' performances change across different noise types?\n",
        "\n",
        "---\n",
        "\n",
        "**Thank you for using this experiment framework!** üéâ\n",
        "\n",
        "If you have issues, please check:\n",
        "- If GPU memory is sufficient\n",
        "- If all required files are uploaded\n",
        "- If network connection is stable\n",
        "\n",
        "**Tip**: It's recommended to run the quick test mode first to validate the environment before running the full experiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARSSQGg95F6N"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}