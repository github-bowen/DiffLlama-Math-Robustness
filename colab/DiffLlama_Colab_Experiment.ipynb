{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# 🔬 DiffLlama vs Llama: Google Colab Experiment\n",
        "\n",
        "This Notebook is designed to run a comparative experiment on Google Colab environment to evaluate the noise robustness of DiffLlama and Llama on mathematical reasoning tasks.\n",
        "\n",
        "## 📋 Experiment Overview\n",
        "- **Objective**: Compare DiffLlama-375M and Llama-375M performance on noisy math problems\n",
        "- **Dataset**: GSM8K math reasoning dataset and its noisy variants\n",
        "- **Evaluation**: Zero-shot performance + attention mechanism analysis\n",
        "- **Environment**: Google Colab (GPU recommended)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_section"
      },
      "source": [
        "## 🚀 Step 1: Environment Setup\n",
        "\n",
        "First, check the runtime environment and configure necessary settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "check_environment",
        "outputId": "fc3636ae-b502-48c7-80f3-8c06a087c830"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🖥️  CUDA available: False\n",
            "⚠️  No GPU detected. Experiment will be slow on CPU.\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"🖥️  CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"🔧 GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"💾 GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"⚠️  No GPU detected. Experiment will be slow on CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clone_repo",
        "outputId": "3f8dc053-13bf-4be3-832e-6e74ed7ef47a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Project files found\n"
          ]
        }
      ],
      "source": [
        "# Clone from Git repository if project files are not in current directory\n",
        "# Replace with your actual repository URL\n",
        "import os\n",
        "if not os.path.exists('colab/experiment.py'):\n",
        "    print(\"📥 Cloning repository...\")\n",
        "    !git clone https://github.com/github-bowen/DiffLlama-Math-Robustness.git\n",
        "    print(\"📥 Copying files...\")\n",
        "    !cp -r DiffLlama-Math-Robustness/* .\n",
        "    print(\"📥 Removing repository...\")\n",
        "    !rm -rf DiffLlama-Math-Robustness\n",
        "    print(\"📥 Done\")\n",
        "else:\n",
        "    print(\"✅ Project files found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upload_section"
      },
      "source": [
        "## 📁 Step 2: Upload Project Files\n",
        "\n",
        "If you didn't clone using Git, manually upload the following files to Colab:\n",
        "\n",
        "**Required Files**:\n",
        "- `colab_experiment.py` (main Colab script)\n",
        "- `pre_download_models.py` (model download script)\n",
        "- All Python files in the `src/` directory\n",
        "- `requirements.txt`\n",
        "\n",
        "Use Colab's file upload feature or copy files from Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructions_section"
      },
      "source": [
        "## 📖 Step 3: View Usage Instructions\n",
        "\n",
        "Run the command below to view detailed usage instructions and options."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "show_instructions",
        "outputId": "acd9be1d-4cd1-4027-8214-1584d3a43e9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🎯 GOOGLE COLAB USAGE INSTRUCTIONS\n",
            "\n",
            "1. 📱 Basic Setup (Run once):\n",
            "   !python colab/experiment.py --setup\n",
            "\n",
            "2. 🚀 Quick Test (Recommended first run):\n",
            "   !python colab/experiment.py --mode quick\n",
            "\n",
            "3. 📊 Medium Experiment:\n",
            "   !python colab/experiment.py --mode medium\n",
            "\n",
            "4. 🔬 Full Experiment:\n",
            "   !python colab/experiment.py --mode full --max-samples 500\n",
            "\n",
            "🔧 Options:\n",
            "   --mode: quick/medium/full\n",
            "  : Save models and results to Google Drive\n",
            "   --max-samples: Limit number of evaluation samples\n",
            "   --skip-attention: Skip attention analysis to save time\n",
            "   --help: Show all options\n",
            "\n",
            "💡 Tips:\n",
            "   - Use to persist models across sessions\n",
            "   - Start with quick mode to verify everything works\n",
            "   - Monitor GPU memory usage in Colab\n",
            "\n",
            "📁 Results will be saved to:\n",
            "   - Local: /content/results/\n",
            "   - Drive: /content/drive/MyDrive/DiffLlama_Experiment/results/\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display usage instructions\n",
        "!python colab/experiment.py --instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_run_section"
      },
      "source": [
        "## 🔧 Step 4: Initial Setup\n",
        "\n",
        "Run initial setup to install dependencies and configure the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq8duQXXpnBa",
        "outputId": "5824dab8-d1af-4c77-c2db-ebbcf514407c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "initial_setup",
        "outputId": "142da218-3b51-417a-e4f5-1ead00f358ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "🔬 DIFFLAMA VS LLAMA - GOOGLE COLAB EXPERIMENT\n",
            "================================================================================\n",
            "🕐 Start time: 2025-06-01 23:13:08\n",
            "📦 Installing dependencies...\n",
            "✅ Dependencies installed\n",
            "🔧 Setting up Colab environment...\n",
            "🔗 Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ Google Drive mounted successfully\n",
            "📁 Using Google Drive storage: /content/drive/MyDrive/DiffLlama_Experiment\n",
            "  ✓ cache -> /content/drive/MyDrive/DiffLlama_Experiment/models\n",
            "  ✓ data -> /content/drive/MyDrive/DiffLlama_Experiment/data\n",
            "  ✓ results -> /content/drive/MyDrive/DiffLlama_Experiment/results\n",
            "✅ Setup completed\n"
          ]
        }
      ],
      "source": [
        "# Run initial setup (includes Google Drive mounting)\n",
        "!python colab/experiment.py --setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gziSZrcrxTug",
        "outputId": "c4e143b4-756e-4a91-f5d7-9d4594e8a525"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 100\n",
            "drwxr-xr-x 1 root root  4096 Jun  1 23:14 .\n",
            "drwxr-xr-x 1 root root  4096 Jun  1 23:06 ..\n",
            "lrwxrwxrwx 1 root root    50 Jun  1 23:14 cache -> /content/drive/MyDrive/DiffLlama_Experiment/models\n",
            "drwxr-xr-x 2 root root  4096 Jun  1 23:14 colab\n",
            "drwxr-xr-x 4 root root  4096 May 29 14:01 .config\n",
            "lrwxrwxrwx 1 root root    48 Jun  1 23:14 data -> /content/drive/MyDrive/DiffLlama_Experiment/data\n",
            "-rw-r--r-- 1 root root 16921 Jun  1 23:07 DiffLlama_Colab_Experiment.ipynb\n",
            "drwxr-xr-x 2 root root  4096 Jun  1 23:07 docs\n",
            "drwx------ 6 root root  4096 Jun  1 23:10 drive\n",
            "-rw-r--r-- 1 root root  2663 Jun  1 23:07 interactive_inference.py\n",
            "-rw-r--r-- 1 root root  1074 Jun  1 23:07 LICENSE\n",
            "-rw-r--r-- 1 root root 16092 Jun  1 23:07 main.py\n",
            "drwxr-xr-x 2 root root  4096 Jun  1 23:11 models_finetuned\n",
            "-rw-r--r-- 1 root root  8948 Jun  1 23:07 README.md\n",
            "-rw-r--r-- 1 root root   219 Jun  1 23:13 requirements.txt\n",
            "lrwxrwxrwx 1 root root    51 Jun  1 23:14 results -> /content/drive/MyDrive/DiffLlama_Experiment/results\n",
            "drwxr-xr-x 1 root root  4096 May 29 14:01 sample_data\n",
            "drwxr-xr-x 2 root root  4096 Jun  1 23:07 scripts\n",
            "drwxr-xr-x 2 root root  4096 Jun  1 23:07 src\n"
          ]
        }
      ],
      "source": [
        "!ls -al"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "experiment_section"
      },
      "source": [
        "## 🚀 Step 5: Run Experiments\n",
        "\n",
        "Choose an appropriate experiment mode based on your needs:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quick_test_section"
      },
      "source": [
        "### 🏃 Quick Test (Recommended for First Run)\n",
        "Validate the experiment workflow using a small number of samples, takes about 30-60 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "quick_test"
      },
      "outputs": [],
      "source": [
        "# Quick test mode\n",
        "!python colab/experiment.py --mode quick"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "medium_test_section"
      },
      "source": [
        "### 📊 Medium-Scale Experiment\n",
        "Use a moderate number of samples, balancing time and result quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "medium_test"
      },
      "outputs": [],
      "source": [
        "# Medium-scale experiment (make sure quick test runs successfully first)\n",
        "!python colab/experiment.py --mode medium"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "full_test_section"
      },
      "source": [
        "### 🔬 Full Experiment\n",
        "Use the complete dataset for the experiment, may take several hours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "full_test"
      },
      "outputs": [],
      "source": [
        "# Full experiment (run only when you have enough time)\n",
        "!python colab/experiment.py --mode full --max-samples 500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "custom_section"
      },
      "source": [
        "### 🛠 Custom Experiment\n",
        "Adjust experiment parameters as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "custom_experiment"
      },
      "outputs": [],
      "source": [
        "# Custom experiment example\n",
        "# Only run evaluation, skip attention analysis to save time\n",
        "!python colab/experiment.pyy --mode medium --skip-attention --max-samples 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "results_section"
      },
      "source": [
        "## 📊 Step 6: View Experiment Results\n",
        "\n",
        "After completing the experiment, review the generated result files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "list_results"
      },
      "outputs": [],
      "source": [
        "# List generated result files\n",
        "!ls -la results/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "show_summary"
      },
      "outputs": [],
      "source": [
        "# View the latest experiment summary\n",
        "import json\n",
        "import glob\n",
        "\n",
        "# Find the latest summary file\n",
        "summary_files = glob.glob('results/colab_summary_*.json')\n",
        "if summary_files:\n",
        "    latest_summary = max(summary_files)\n",
        "    print(f\"📋 Latest experiment summary: {latest_summary}\")\n",
        "\n",
        "    with open(latest_summary, 'r') as f:\n",
        "        summary = json.load(f)\n",
        "\n",
        "    print(\"\\n📊 Experiment Summary:\")\n",
        "    for key, value in summary.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "else:\n",
        "    print(\"No experiment summary found. Please run an experiment first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "show_results"
      },
      "outputs": [],
      "source": [
        "# Display main results\n",
        "import pandas as pd\n",
        "\n",
        "# Find the latest results file\n",
        "result_files = glob.glob('results/colab_results_*.csv')\n",
        "if result_files:\n",
        "    latest_results = max(result_files)\n",
        "    print(f\"📈 Latest results: {latest_results}\")\n",
        "\n",
        "    df = pd.read_csv(latest_results)\n",
        "    print(\"\\n📊 Performance Comparison:\")\n",
        "    print(df.pivot(index='model', columns='dataset', values='accuracy'))\n",
        "\n",
        "    # Calculate performance differences\n",
        "    pivot_df = df.pivot(index='model', columns='dataset', values='accuracy')\n",
        "    if 'llama' in pivot_df.index and 'diffllama' in pivot_df.index:\n",
        "        print(\"\\n🔍 Performance Difference (DiffLlama - Llama):\")\n",
        "        diff = pivot_df.loc['diffllama'] - pivot_df.loc['llama']\n",
        "        print(diff)\n",
        "else:\n",
        "    print(\"No results found. Please run an experiment first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualization_section"
      },
      "source": [
        "## 📈 Step 7: Results Visualization\n",
        "\n",
        "If your experiment included attention analysis, you can view the generated attention heatmaps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "show_attention_maps"
      },
      "outputs": [],
      "source": [
        "# Display attention heatmaps\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "attention_dir = 'results/attention_maps'\n",
        "if os.path.exists(attention_dir):\n",
        "    print(\"🧠 Attention Visualization Files:\")\n",
        "\n",
        "    # List all attention map files\n",
        "    for root, dirs, files in os.walk(attention_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.png'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                print(f\"  📊 {file_path}\")\n",
        "\n",
        "                # Display images (optional, uncomment to show)\n",
        "                # display(Image(file_path))\n",
        "else:\n",
        "    print(\"No attention maps found. Run experiment with attention analysis enabled.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "show_attention_analysis"
      },
      "outputs": [],
      "source": [
        "# Display attention analysis results\n",
        "attention_files = glob.glob('results/colab_attention_*.json')\n",
        "if attention_files:\n",
        "    latest_attention = max(attention_files)\n",
        "    print(f\"🧠 Latest attention analysis: {latest_attention}\")\n",
        "\n",
        "    with open(latest_attention, 'r') as f:\n",
        "        attention_data = json.load(f)\n",
        "\n",
        "    print(\"\\n📊 Attention Allocation Analysis:\")\n",
        "    for model, data in attention_data.items():\n",
        "        print(f\"\\n{model.upper()} Model:\")\n",
        "        for condition, stats in data.items():\n",
        "            print(f\"  {condition.capitalize()}:\")\n",
        "            print(f\"    KMI (Key Math Info): {stats['kmi_mean']:.3f} ± {stats['kmi_std']:.3f}\")\n",
        "            print(f\"    NI (Noise Info): {stats['ni_mean']:.3f} ± {stats['ni_std']:.3f}\")\n",
        "            print(f\"    OC (Other Context): {stats['oc_mean']:.3f} ± {stats['oc_std']:.3f}\")\n",
        "else:\n",
        "    print(\"No attention analysis found. Run experiment with attention analysis enabled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download_section"
      },
      "source": [
        "## 💾 Step 8: Download Results\n",
        "\n",
        "Download experiment results locally or ensure they are saved in Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_results"
      },
      "outputs": [],
      "source": [
        "# Compress result files for download\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "zip_filename = f'experiment_results_{timestamp}.zip'\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "    # Add all files from results directory\n",
        "    for root, dirs, files in os.walk('results'):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            zipf.write(file_path)\n",
        "\n",
        "print(f\"📦 Results packaged in: {zip_filename}\")\n",
        "print(\"You can download this file from Colab's Files panel.\")\n",
        "\n",
        "# Reminder if Google Drive was used\n",
        "if os.path.exists('/content/drive/MyDrive/DiffLlama_Experiment'):\n",
        "    print(\"\\n💾 Results are also saved in Google Drive:\")\n",
        "    print(\"  /content/drive/MyDrive/DiffLlama_Experiment/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "troubleshooting_section"
      },
      "source": [
        "## 🛠 Troubleshooting\n",
        "\n",
        "If you encounter issues, try the following solutions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clear_cache"
      },
      "outputs": [],
      "source": [
        "# Clear GPU memory cache\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"✅ GPU cache cleared\")\n",
        "\n",
        "# Check available memory\n",
        "import psutil\n",
        "memory = psutil.virtual_memory()\n",
        "print(f\"💾 RAM: {memory.available / 1e9:.1f}GB available / {memory.total / 1e9:.1f}GB total\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "restart_runtime"
      },
      "outputs": [],
      "source": [
        "# If memory is insufficient, you can restart the runtime (use with caution)\n",
        "# import os\n",
        "# os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion_section"
      },
      "source": [
        "## 🎯 Experiment Conclusions\n",
        "\n",
        "Based on the experiment results, you can analyze the following key questions:\n",
        "\n",
        "1. **Noise Robustness**: Does DiffLlama perform better on noisy data?\n",
        "2. **Attention Mechanism**: Is differential attention more effective at focusing on key information?\n",
        "3. **Performance Degradation**: How do both models' performances change across different noise types?\n",
        "\n",
        "---\n",
        "\n",
        "**Thank you for using this experiment framework!** 🎉\n",
        "\n",
        "If you have issues, please check:\n",
        "- If GPU memory is sufficient\n",
        "- If all required files are uploaded\n",
        "- If network connection is stable\n",
        "\n",
        "**Tip**: It's recommended to run the quick test mode first to validate the environment before running the full experiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
