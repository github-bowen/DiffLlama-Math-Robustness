#!/usr/bin/env python3
"""
Setup Test Script - æµ‹è¯•å®éªŒç¯å¢ƒé…ç½®

æ­¤è„šæœ¬éªŒè¯æ‰€æœ‰å¿…éœ€çš„ä¾èµ–ã€æ¨¡å‹å’Œæ•°æ®æ–‡ä»¶æ˜¯å¦æ­£ç¡®é…ç½®ã€‚
åœ¨è¿è¡Œä¸»å®éªŒä¹‹å‰å»ºè®®å…ˆè¿è¡Œæ­¤è„šæœ¬è¿›è¡ŒéªŒè¯ã€‚
"""

import os
import sys
import json
import traceback
from pathlib import Path

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def test_python_environment():
    """æµ‹è¯• Python ç¯å¢ƒå’ŒåŸºç¡€ä¾èµ–"""
    print("ğŸ” æµ‹è¯• Python ç¯å¢ƒ...")
    
    try:
        import torch
        print(f"âœ… PyTorch {torch.__version__}")
        
        if torch.cuda.is_available():
            print(f"âœ… CUDA å¯ç”¨: {torch.cuda.get_device_name(0)}")
            print(f"   GPU å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        else:
            print("âš ï¸  CUDA ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ CPUï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
    except ImportError:
        print("âŒ PyTorch æœªå®‰è£…")
        return False
    
    try:
        import transformers
        print(f"âœ… Transformers {transformers.__version__}")
    except ImportError:
        print("âŒ Transformers æœªå®‰è£…")
        return False
    
    try:
        import datasets
        print(f"âœ… Datasets {datasets.__version__}")
    except ImportError:
        print("âŒ Datasets æœªå®‰è£…")
        return False
    
    try:
        import numpy as np
        import pandas as pd
        import matplotlib.pyplot as plt
        import seaborn as sns
        print("âœ… æ•°æ®åˆ†æåº“ (numpy, pandas, matplotlib, seaborn)")
    except ImportError as e:
        print(f"âŒ æ•°æ®åˆ†æåº“ç¼ºå¤±: {e}")
        return False
    
    return True

def test_source_files():
    """æµ‹è¯•æºä»£ç æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    print("\nğŸ“ æµ‹è¯•æºä»£ç æ–‡ä»¶...")
    
    required_files = [
        "src/utils.py",
        "src/model_loader.py",
        "src/noise_injection.py", 
        "src/evaluation.py",
        "src/fine_tuning.py",
        "src/attention_visualizer.py"
    ]
    
    missing_files = []
    for file_path in required_files:
        if os.path.exists(file_path):
            print(f"âœ… {file_path}")
        else:
            print(f"âŒ {file_path}")
            missing_files.append(file_path)
    
    if missing_files:
        print(f"\nâš ï¸  ç¼ºå°‘ {len(missing_files)} ä¸ªæºæ–‡ä»¶")
        return False
    
    return True

def test_imports():
    """æµ‹è¯•æ ¸å¿ƒæ¨¡å—å¯¼å…¥"""
    print("\nğŸ”§ æµ‹è¯•æ¨¡å—å¯¼å…¥...")
    
    test_modules = [
        ("src.utils", "ä¸‹è½½å’Œå¤„ç†å·¥å…·"),
        ("src.model_loader", "æ¨¡å‹åŠ è½½å™¨"),
        ("src.noise_injection", "å™ªå£°æ³¨å…¥"),
        ("src.evaluation", "è¯„ä¼°æ¨¡å—"),
        ("src.fine_tuning", "å¾®è°ƒæ¨¡å—"),
        ("src.attention_visualizer", "æ³¨æ„åŠ›å¯è§†åŒ–")
    ]
    
    failed_imports = []
    for module_name, description in test_modules:
        try:
            __import__(module_name)
            print(f"âœ… {module_name} ({description})")
        except ImportError as e:
            print(f"âŒ {module_name} ({description}): {e}")
            failed_imports.append(module_name)
        except Exception as e:
            print(f"âš ï¸  {module_name} ({description}): {e}")
    
    if failed_imports:
        print(f"\nâš ï¸  {len(failed_imports)} ä¸ªæ¨¡å—å¯¼å…¥å¤±è´¥")
        return False
    
    return True

def test_model_availability():
    """æµ‹è¯•æ¨¡å‹æ–‡ä»¶æ˜¯å¦å¯ç”¨"""
    print("\nğŸ¤– æµ‹è¯•æ¨¡å‹å¯ç”¨æ€§...")
    
    model_paths = {
        "DiffLlama": "cache/models--reyllama--DiffLlama-375M",
        "Llama": "cache/models--reyllama--Llama_375M"
    }
    
    missing_models = []
    for model_name, model_path in model_paths.items():
        if os.path.exists(model_path):
            print(f"âœ… {model_name}: {model_path}")
            
            # æ£€æŸ¥æ¨¡å‹å†…å®¹
            if os.path.isdir(model_path):
                contents = os.listdir(model_path)
                if contents:
                    print(f"   ğŸ“ åŒ…å« {len(contents)} ä¸ªæ–‡ä»¶/ç›®å½•")
                else:
                    print(f"   âš ï¸  ç›®å½•ä¸ºç©º")
        else:
            print(f"âŒ {model_name}: {model_path}")
            missing_models.append(model_name)
    
    if missing_models:
        print(f"\nâš ï¸  ç¼ºå°‘ {len(missing_models)} ä¸ªæ¨¡å‹")
        print("è¿è¡Œä»¥ä¸‹å‘½ä»¤ä¸‹è½½æ¨¡å‹:")
        print("python scripts/download_models.py")
        return False
    
    return True

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½åŠŸèƒ½"""
    print("\nğŸš€ æµ‹è¯•æ¨¡å‹åŠ è½½...")
    
    try:
        from src.model_loader import load_model_and_tokenizer
        
        # æµ‹è¯•åŠ è½½ DiffLlamaï¼ˆä½¿ç”¨è¾ƒå°çš„è®¾ç½®ä»¥èŠ‚çœå†…å­˜ï¼‰
        print("  åŠ è½½ DiffLlama...")
        model, tokenizer = load_model_and_tokenizer("diffllama")
        print("  âœ… DiffLlama åŠ è½½æˆåŠŸ")
        
        # æ¸…ç†å†…å­˜
        del model, tokenizer
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # æµ‹è¯•åŠ è½½ Llama
        print("  åŠ è½½ Llama...")
        model, tokenizer = load_model_and_tokenizer("llama")
        print("  âœ… Llama åŠ è½½æˆåŠŸ")
        
        # æ¸…ç†å†…å­˜
        del model, tokenizer
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_data_generation():
    """æµ‹è¯•æ•°æ®ç”ŸæˆåŠŸèƒ½"""
    print("\nğŸ“Š æµ‹è¯•æ•°æ®ç”Ÿæˆ...")
    
    try:
        from src.utils import download_gsm8k
        from src.noise_injection import inject_inf_noise
        
        # æµ‹è¯• GSM8K ä¸‹è½½ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if not os.path.exists("data/gsm8k_test.jsonl"):
            print("  ä¸‹è½½ GSM8K æ•°æ®é›†...")
            download_gsm8k()
        
        if os.path.exists("data/gsm8k_test.jsonl"):
            print("  âœ… GSM8K æ•°æ®é›†å¯ç”¨")
        else:
            print("  âŒ GSM8K æ•°æ®é›†ä¸‹è½½å¤±è´¥")
            return False
        
        # æµ‹è¯•å™ªå£°æ³¨å…¥ï¼ˆå°æ ·æœ¬ï¼‰
        print("  æµ‹è¯•å™ªå£°æ³¨å…¥...")
        test_question = "Lisa has 10 apples. She gives 3 to her friend. How many apples does she have left?"
        noisy_question = inject_inf_noise(test_question)
        
        if len(noisy_question) > len(test_question):
            print("  âœ… å™ªå£°æ³¨å…¥åŠŸèƒ½æ­£å¸¸")
        else:
            print("  âš ï¸  å™ªå£°æ³¨å…¥å¯èƒ½æœªç”Ÿæ•ˆ")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®ç”Ÿæˆæµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_evaluation_pipeline():
    """æµ‹è¯•è¯„ä¼°æµç¨‹"""
    print("\nğŸ” æµ‹è¯•è¯„ä¼°æµç¨‹...")
    
    try:
        from src.evaluation import extract_answer, evaluate_answer
        
        # æµ‹è¯•ç­”æ¡ˆæå–
        test_response = "Let me think step by step. Lisa has 10 apples and gives away 3, so 10 - 3 = 7. The answer is 7."
        extracted = extract_answer(test_response)
        print(f"  ç­”æ¡ˆæå–æµ‹è¯•: '{extracted}'")
        
        # æµ‹è¯•ç­”æ¡ˆè¯„ä¼°
        correct = evaluate_answer(extracted, "7")
        print(f"  ç­”æ¡ˆè¯„ä¼°æµ‹è¯•: {correct}")
        
        if correct:
            print("  âœ… è¯„ä¼°æµç¨‹åŠŸèƒ½æ­£å¸¸")
        else:
            print("  âš ï¸  è¯„ä¼°æµç¨‹å¯èƒ½æœ‰é—®é¢˜")
        
        return True
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°æµç¨‹æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_directory_structure():
    """æµ‹è¯•ç›®å½•ç»“æ„"""
    print("\nğŸ“ æµ‹è¯•ç›®å½•ç»“æ„...")
    
    required_dirs = ["src", "data", "results", "cache"]
    
    for dir_name in required_dirs:
        if os.path.exists(dir_name):
            print(f"âœ… {dir_name}/")
        else:
            print(f"âš ï¸  {dir_name}/ (å°†è‡ªåŠ¨åˆ›å»º)")
            os.makedirs(dir_name, exist_ok=True)
    
    return True

def run_comprehensive_test():
    """è¿è¡Œå…¨é¢çš„ç¯å¢ƒæµ‹è¯•"""
    print("="*80)
    print("ğŸ§ª DiffLlama å®éªŒç¯å¢ƒæµ‹è¯•")
    print("="*80)
    
    tests = [
        ("Python ç¯å¢ƒ", test_python_environment),
        ("ç›®å½•ç»“æ„", test_directory_structure),
        ("æºä»£ç æ–‡ä»¶", test_source_files),
        ("æ¨¡å—å¯¼å…¥", test_imports),
        ("æ¨¡å‹å¯ç”¨æ€§", test_model_availability),
        ("æ•°æ®ç”Ÿæˆ", test_data_generation),
        ("è¯„ä¼°æµç¨‹", test_evaluation_pipeline)
    ]
    
    results = {}
    for test_name, test_func in tests:
        try:
            results[test_name] = test_func()
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å‡ºç°å¼‚å¸¸: {e}")
            results[test_name] = False
    
    # æ€»ç»“
    print("\n" + "="*80)
    print("ğŸ“‹ æµ‹è¯•ç»“æœæ€»ç»“")
    print("="*80)
    
    passed = sum(results.values())
    total = len(results)
    
    for test_name, passed_test in results.items():
        status = "âœ… é€šè¿‡" if passed_test else "âŒ å¤±è´¥"
        print(f"{test_name:20} {status}")
    
    print(f"\nğŸ¯ æ€»ä½“ç»“æœ: {passed}/{total} é¡¹æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç¯å¢ƒé…ç½®æ­£ç¡®ï¼Œå¯ä»¥è¿è¡Œå®éªŒã€‚")
        return True
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯å¹¶è§£å†³é—®é¢˜ã€‚")
        return False

def quick_test():
    """å¿«é€Ÿæµ‹è¯•æ ¸å¿ƒåŠŸèƒ½"""
    print("ğŸš€ å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
    print("-" * 40)
    
    # åªæµ‹è¯•æœ€å…³é”®çš„åŠŸèƒ½
    critical_tests = [
        test_python_environment,
        test_source_files,
        test_imports,
        test_directory_structure
    ]
    
    all_passed = True
    for test_func in critical_tests:
        try:
            if not test_func():
                all_passed = False
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
            all_passed = False
    
    if all_passed:
        print("\nâœ… æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•é€šè¿‡")
    else:
        print("\nâŒ æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•å¤±è´¥")
    
    return all_passed

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="DiffLlama å®éªŒç¯å¢ƒæµ‹è¯•")
    parser.add_argument("--quick", action="store_true", help="å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
    parser.add_argument("--models", action="store_true", help="ä»…æµ‹è¯•æ¨¡å‹")
    
    args = parser.parse_args()
    
    if args.quick:
        success = quick_test()
    elif args.models:
        success = test_model_availability() and test_model_loading()
    else:
        success = run_comprehensive_test()
    
    sys.exit(0 if success else 1) 