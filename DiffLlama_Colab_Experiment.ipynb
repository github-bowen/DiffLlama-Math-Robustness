{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# ğŸ”¬ DiffLlama vs Llama: Google Colab å®éªŒ\n",
        "\n",
        "è¿™ä¸ª Notebook ç”¨äºåœ¨ Google Colab ç¯å¢ƒä¸­è¿è¡Œ DiffLlama ä¸ Llama åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„å™ªå£°é²æ£’æ€§å¯¹æ¯”å®éªŒã€‚\n",
        "\n",
        "## ğŸ“‹ å®éªŒæ¦‚è¿°\n",
        "- **ç›®æ ‡**: å¯¹æ¯” DiffLlama-375M ä¸ Llama-375M åœ¨å«å™ªæ•°å­¦é—®é¢˜ä¸Šçš„è¡¨ç°\n",
        "- **æ•°æ®é›†**: GSM8K æ•°å­¦æ¨ç†æ•°æ®é›†åŠå…¶å™ªå£°å˜ä½“\n",
        "- **è¯„ä¼°**: é›¶æ ·æœ¬æ€§èƒ½ + æ³¨æ„åŠ›æœºåˆ¶åˆ†æ\n",
        "- **ç¯å¢ƒ**: Google Colab (GPU æ¨è)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_section"
      },
      "source": [
        "## ğŸš€ ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒè®¾ç½®\n",
        "\n",
        "é¦–å…ˆæ£€æŸ¥è¿è¡Œç¯å¢ƒå¹¶è®¾ç½®å¿…è¦çš„é…ç½®ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "check_environment",
        "outputId": "fc3636ae-b502-48c7-80f3-8c06a087c830"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ–¥ï¸  CUDA available: False\n",
            "âš ï¸  No GPU detected. Experiment will be slow on CPU.\n"
          ]
        }
      ],
      "source": [
        "# æ£€æŸ¥ GPU å¯ç”¨æ€§\n",
        "import torch\n",
        "print(f\"ğŸ–¥ï¸  CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"ğŸ”§ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"âš ï¸  No GPU detected. Experiment will be slow on CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clone_repo",
        "outputId": "3f8dc053-13bf-4be3-832e-6e74ed7ef47a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Project files found\n"
          ]
        }
      ],
      "source": [
        "# å¦‚æœé¡¹ç›®æ–‡ä»¶ä¸åœ¨å½“å‰ç›®å½•ï¼Œä» Git ä»“åº“å…‹éš†\n",
        "# æ›¿æ¢ä¸ºæ‚¨çš„å®é™…ä»“åº“åœ°å€\n",
        "import os\n",
        "if not os.path.exists('colab/experiment.py'):\n",
        "    print(\"ğŸ“¥ Cloning repository...\")\n",
        "    !git clone https://github.com/github-bowen/DiffLlama-Math-Robustness.git\n",
        "    print(\"ğŸ“¥ Copying files...\")\n",
        "    !cp -r DiffLlama-Math-Robustness/* .\n",
        "    print(\"ğŸ“¥ Removing repository...\")\n",
        "    !rm -rf DiffLlama-Math-Robustness\n",
        "    print(\"ğŸ“¥ Done\")\n",
        "else:\n",
        "    print(\"âœ… Project files found\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upload_section"
      },
      "source": [
        "## ğŸ“ ç¬¬äºŒæ­¥ï¼šä¸Šä¼ é¡¹ç›®æ–‡ä»¶\n",
        "\n",
        "å¦‚æœæ‚¨æ²¡æœ‰ä½¿ç”¨ Git å…‹éš†ï¼Œè¯·æ‰‹åŠ¨ä¸Šä¼ ä»¥ä¸‹æ–‡ä»¶åˆ° Colabï¼š\n",
        "\n",
        "**å¿…éœ€æ–‡ä»¶**:\n",
        "- `colab_experiment.py` (ä¸»è¦çš„ Colab è„šæœ¬)\n",
        "- `pre_download_models.py` (æ¨¡å‹ä¸‹è½½è„šæœ¬)\n",
        "- `src/` ç›®å½•ä¸‹çš„æ‰€æœ‰ Python æ–‡ä»¶\n",
        "- `requirements.txt`\n",
        "\n",
        "ä½¿ç”¨ Colab çš„æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½æˆ–ä» Google Drive å¤åˆ¶æ–‡ä»¶ã€‚"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "instructions_section"
      },
      "source": [
        "## ğŸ“– ç¬¬ä¸‰æ­¥ï¼šæŸ¥çœ‹ä½¿ç”¨è¯´æ˜\n",
        "\n",
        "è¿è¡Œä¸‹é¢çš„å‘½ä»¤æŸ¥çœ‹è¯¦ç»†çš„ä½¿ç”¨è¯´æ˜å’Œé€‰é¡¹ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "show_instructions",
        "outputId": "acd9be1d-4cd1-4027-8214-1584d3a43e9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ¯ GOOGLE COLAB USAGE INSTRUCTIONS\n",
            "\n",
            "1. ğŸ“± Basic Setup (Run once):\n",
            "   !python colab/experiment.py --setup\n",
            "\n",
            "2. ğŸš€ Quick Test (Recommended first run):\n",
            "   !python colab/experiment.py --mode quick\n",
            "\n",
            "3. ğŸ“Š Medium Experiment:\n",
            "   !python colab/experiment.py --mode medium --use-drive\n",
            "\n",
            "4. ğŸ”¬ Full Experiment:\n",
            "   !python colab/experiment.py --mode full --use-drive --max-samples 500\n",
            "\n",
            "ğŸ”§ Options:\n",
            "   --mode: quick/medium/full\n",
            "   --use-drive: Save models and results to Google Drive\n",
            "   --max-samples: Limit number of evaluation samples\n",
            "   --skip-attention: Skip attention analysis to save time\n",
            "   --help: Show all options\n",
            "\n",
            "ğŸ’¡ Tips:\n",
            "   - Use --use-drive to persist models across sessions\n",
            "   - Start with quick mode to verify everything works\n",
            "   - Monitor GPU memory usage in Colab\n",
            "\n",
            "ğŸ“ Results will be saved to:\n",
            "   - Local: /content/results/\n",
            "   - Drive: /content/drive/MyDrive/DiffLlama_Experiment/results/\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# æ˜¾ç¤ºä½¿ç”¨è¯´æ˜\n",
        "!python colab/experiment.py --instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_run_section"
      },
      "source": [
        "## ğŸ”§ ç¬¬å››æ­¥ï¼šåˆå§‹è®¾ç½®\n",
        "\n",
        "è¿è¡Œåˆå§‹è®¾ç½®ï¼Œå®‰è£…ä¾èµ–åŒ…å¹¶é…ç½®ç¯å¢ƒã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq8duQXXpnBa",
        "outputId": "5824dab8-d1af-4c77-c2db-ebbcf514407c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "initial_setup",
        "outputId": "142da218-3b51-417a-e4f5-1ead00f358ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ğŸ”¬ DIFFLAMA VS LLAMA - GOOGLE COLAB EXPERIMENT\n",
            "================================================================================\n",
            "ğŸ• Start time: 2025-06-01 23:13:08\n",
            "ğŸ“¦ Installing dependencies...\n",
            "âœ… Dependencies installed\n",
            "ğŸ”§ Setting up Colab environment...\n",
            "ğŸ”— Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ… Google Drive mounted successfully\n",
            "ğŸ“ Using Google Drive storage: /content/drive/MyDrive/DiffLlama_Experiment\n",
            "  âœ“ cache -> /content/drive/MyDrive/DiffLlama_Experiment/models\n",
            "  âœ“ data -> /content/drive/MyDrive/DiffLlama_Experiment/data\n",
            "  âœ“ results -> /content/drive/MyDrive/DiffLlama_Experiment/results\n",
            "âœ… Setup completed\n"
          ]
        }
      ],
      "source": [
        "# è¿è¡Œåˆå§‹è®¾ç½® (åŒ…å« Google Drive æŒ‚è½½)\n",
        "!python colab/experiment.py --setup --use-drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gziSZrcrxTug",
        "outputId": "c4e143b4-756e-4a91-f5d7-9d4594e8a525"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 100\n",
            "drwxr-xr-x 1 root root  4096 Jun  1 23:14 .\n",
            "drwxr-xr-x 1 root root  4096 Jun  1 23:06 ..\n",
            "lrwxrwxrwx 1 root root    50 Jun  1 23:14 cache -> /content/drive/MyDrive/DiffLlama_Experiment/models\n",
            "drwxr-xr-x 2 root root  4096 Jun  1 23:14 colab\n",
            "drwxr-xr-x 4 root root  4096 May 29 14:01 .config\n",
            "lrwxrwxrwx 1 root root    48 Jun  1 23:14 data -> /content/drive/MyDrive/DiffLlama_Experiment/data\n",
            "-rw-r--r-- 1 root root 16921 Jun  1 23:07 DiffLlama_Colab_Experiment.ipynb\n",
            "drwxr-xr-x 2 root root  4096 Jun  1 23:07 docs\n",
            "drwx------ 6 root root  4096 Jun  1 23:10 drive\n",
            "-rw-r--r-- 1 root root  2663 Jun  1 23:07 interactive_inference.py\n",
            "-rw-r--r-- 1 root root  1074 Jun  1 23:07 LICENSE\n",
            "-rw-r--r-- 1 root root 16092 Jun  1 23:07 main.py\n",
            "drwxr-xr-x 2 root root  4096 Jun  1 23:11 models_finetuned\n",
            "-rw-r--r-- 1 root root  8948 Jun  1 23:07 README.md\n",
            "-rw-r--r-- 1 root root   219 Jun  1 23:13 requirements.txt\n",
            "lrwxrwxrwx 1 root root    51 Jun  1 23:14 results -> /content/drive/MyDrive/DiffLlama_Experiment/results\n",
            "drwxr-xr-x 1 root root  4096 May 29 14:01 sample_data\n",
            "drwxr-xr-x 2 root root  4096 Jun  1 23:07 scripts\n",
            "drwxr-xr-x 2 root root  4096 Jun  1 23:07 src\n"
          ]
        }
      ],
      "source": [
        "!ls -al"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "experiment_section"
      },
      "source": [
        "## ğŸš€ ç¬¬äº”æ­¥ï¼šè¿è¡Œå®éªŒ\n",
        "\n",
        "æ ¹æ®æ‚¨çš„éœ€æ±‚é€‰æ‹©åˆé€‚çš„å®éªŒæ¨¡å¼ï¼š"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quick_test_section"
      },
      "source": [
        "### ğŸƒ å¿«é€Ÿæµ‹è¯• (æ¨èé¦–æ¬¡è¿è¡Œ)\n",
        "ä½¿ç”¨å°‘é‡æ ·æœ¬éªŒè¯å®éªŒæµç¨‹ï¼Œå¤§çº¦éœ€è¦ 30-60 åˆ†é’Ÿã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "quick_test"
      },
      "outputs": [],
      "source": [
        "# å¿«é€Ÿæµ‹è¯•æ¨¡å¼\n",
        "!python colab/experiment.py --mode quick --use-drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "medium_test_section"
      },
      "source": [
        "### ğŸ“Š ä¸­ç­‰è§„æ¨¡å®éªŒ\n",
        "ä½¿ç”¨ä¸­ç­‰æ•°é‡æ ·æœ¬ï¼Œå¹³è¡¡æ—¶é—´å’Œç»“æœè´¨é‡ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "medium_test"
      },
      "outputs": [],
      "source": [
        "# ä¸­ç­‰è§„æ¨¡å®éªŒ (è¿è¡Œå‰è¯·ç¡®ä¿å¿«é€Ÿæµ‹è¯•æˆåŠŸ)\n",
        "!python colab/experiment.py --mode medium --use-drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "full_test_section"
      },
      "source": [
        "### ğŸ”¬ å®Œæ•´å®éªŒ\n",
        "ä½¿ç”¨å®Œæ•´æ•°æ®é›†è¿›è¡Œå®éªŒï¼Œå¯èƒ½éœ€è¦æ•°å°æ—¶ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "full_test"
      },
      "outputs": [],
      "source": [
        "# å®Œæ•´å®éªŒ (ä»…åœ¨æœ‰è¶³å¤Ÿæ—¶é—´æ—¶è¿è¡Œ)\n",
        "!python colab/experiment.py --mode full --use-drive --max-samples 500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "custom_section"
      },
      "source": [
        "### ğŸ›  è‡ªå®šä¹‰å®éªŒ\n",
        "æ ¹æ®éœ€è¦è°ƒæ•´å®éªŒå‚æ•°ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "custom_experiment"
      },
      "outputs": [],
      "source": [
        "# è‡ªå®šä¹‰å®éªŒç¤ºä¾‹\n",
        "# åªè¿è¡Œè¯„ä¼°ï¼Œè·³è¿‡æ³¨æ„åŠ›åˆ†æä»¥èŠ‚çœæ—¶é—´\n",
        "!python colab/experiment.pyy --mode medium --use-drive --skip-attention --max-samples 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "results_section"
      },
      "source": [
        "## ğŸ“Š ç¬¬å…­æ­¥ï¼šæŸ¥çœ‹å®éªŒç»“æœ\n",
        "\n",
        "å®éªŒå®Œæˆåï¼ŒæŸ¥çœ‹ç”Ÿæˆçš„ç»“æœæ–‡ä»¶ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "list_results"
      },
      "outputs": [],
      "source": [
        "# åˆ—å‡ºç”Ÿæˆçš„ç»“æœæ–‡ä»¶\n",
        "!ls -la results/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "show_summary"
      },
      "outputs": [],
      "source": [
        "# æŸ¥çœ‹æœ€æ–°çš„å®éªŒæ‘˜è¦\n",
        "import json\n",
        "import glob\n",
        "\n",
        "# æ‰¾åˆ°æœ€æ–°çš„æ‘˜è¦æ–‡ä»¶\n",
        "summary_files = glob.glob('results/colab_summary_*.json')\n",
        "if summary_files:\n",
        "    latest_summary = max(summary_files)\n",
        "    print(f\"ğŸ“‹ Latest experiment summary: {latest_summary}\")\n",
        "\n",
        "    with open(latest_summary, 'r') as f:\n",
        "        summary = json.load(f)\n",
        "\n",
        "    print(\"\\nğŸ“Š Experiment Summary:\")\n",
        "    for key, value in summary.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "else:\n",
        "    print(\"No experiment summary found. Please run an experiment first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "show_results"
      },
      "outputs": [],
      "source": [
        "# æ˜¾ç¤ºä¸»è¦ç»“æœ\n",
        "import pandas as pd\n",
        "\n",
        "# æ‰¾åˆ°æœ€æ–°çš„ç»“æœæ–‡ä»¶\n",
        "result_files = glob.glob('results/colab_results_*.csv')\n",
        "if result_files:\n",
        "    latest_results = max(result_files)\n",
        "    print(f\"ğŸ“ˆ Latest results: {latest_results}\")\n",
        "\n",
        "    df = pd.read_csv(latest_results)\n",
        "    print(\"\\nğŸ“Š Performance Comparison:\")\n",
        "    print(df.pivot(index='model', columns='dataset', values='accuracy'))\n",
        "\n",
        "    # è®¡ç®—æ€§èƒ½å·®å¼‚\n",
        "    pivot_df = df.pivot(index='model', columns='dataset', values='accuracy')\n",
        "    if 'llama' in pivot_df.index and 'diffllama' in pivot_df.index:\n",
        "        print(\"\\nğŸ” Performance Difference (DiffLlama - Llama):\")\n",
        "        diff = pivot_df.loc['diffllama'] - pivot_df.loc['llama']\n",
        "        print(diff)\n",
        "else:\n",
        "    print(\"No results found. Please run an experiment first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualization_section"
      },
      "source": [
        "## ğŸ“ˆ ç¬¬ä¸ƒæ­¥ï¼šç»“æœå¯è§†åŒ–\n",
        "\n",
        "å¦‚æœå®éªŒåŒ…å«äº†æ³¨æ„åŠ›åˆ†æï¼Œå¯ä»¥æŸ¥çœ‹ç”Ÿæˆçš„æ³¨æ„åŠ›çƒ­åŠ›å›¾ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "show_attention_maps"
      },
      "outputs": [],
      "source": [
        "# æ˜¾ç¤ºæ³¨æ„åŠ›çƒ­åŠ›å›¾\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image, display\n",
        "import os\n",
        "\n",
        "attention_dir = 'results/attention_maps'\n",
        "if os.path.exists(attention_dir):\n",
        "    print(\"ğŸ§  Attention Visualization Files:\")\n",
        "\n",
        "    # åˆ—å‡ºæ‰€æœ‰æ³¨æ„åŠ›å›¾æ–‡ä»¶\n",
        "    for root, dirs, files in os.walk(attention_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.png'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                print(f\"  ğŸ“Š {file_path}\")\n",
        "\n",
        "                # æ˜¾ç¤ºå›¾ç‰‡ (å¯é€‰ï¼Œå–æ¶ˆæ³¨é‡Šä»¥æ˜¾ç¤º)\n",
        "                # display(Image(file_path))\n",
        "else:\n",
        "    print(\"No attention maps found. Run experiment with attention analysis enabled.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "show_attention_analysis"
      },
      "outputs": [],
      "source": [
        "# æ˜¾ç¤ºæ³¨æ„åŠ›åˆ†æç»“æœ\n",
        "attention_files = glob.glob('results/colab_attention_*.json')\n",
        "if attention_files:\n",
        "    latest_attention = max(attention_files)\n",
        "    print(f\"ğŸ§  Latest attention analysis: {latest_attention}\")\n",
        "\n",
        "    with open(latest_attention, 'r') as f:\n",
        "        attention_data = json.load(f)\n",
        "\n",
        "    print(\"\\nğŸ“Š Attention Allocation Analysis:\")\n",
        "    for model, data in attention_data.items():\n",
        "        print(f\"\\n{model.upper()} Model:\")\n",
        "        for condition, stats in data.items():\n",
        "            print(f\"  {condition.capitalize()}:\")\n",
        "            print(f\"    KMI (Key Math Info): {stats['kmi_mean']:.3f} Â± {stats['kmi_std']:.3f}\")\n",
        "            print(f\"    NI (Noise Info): {stats['ni_mean']:.3f} Â± {stats['ni_std']:.3f}\")\n",
        "            print(f\"    OC (Other Context): {stats['oc_mean']:.3f} Â± {stats['oc_std']:.3f}\")\n",
        "else:\n",
        "    print(\"No attention analysis found. Run experiment with attention analysis enabled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download_section"
      },
      "source": [
        "## ğŸ’¾ ç¬¬å…«æ­¥ï¼šä¸‹è½½ç»“æœ\n",
        "\n",
        "å°†å®éªŒç»“æœä¸‹è½½åˆ°æœ¬åœ°æˆ–ç¡®ä¿ä¿å­˜åœ¨ Google Drive ä¸­ã€‚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_results"
      },
      "outputs": [],
      "source": [
        "# å‹ç¼©ç»“æœæ–‡ä»¶ä»¥ä¾¿ä¸‹è½½\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "zip_filename = f'experiment_results_{timestamp}.zip'\n",
        "\n",
        "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
        "    # æ·»åŠ ç»“æœç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶\n",
        "    for root, dirs, files in os.walk('results'):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            zipf.write(file_path)\n",
        "\n",
        "print(f\"ğŸ“¦ Results packaged in: {zip_filename}\")\n",
        "print(\"You can download this file from Colab's Files panel.\")\n",
        "\n",
        "# å¦‚æœä½¿ç”¨äº† Google Driveï¼Œæé†’ç”¨æˆ·\n",
        "if os.path.exists('/content/drive/MyDrive/DiffLlama_Experiment'):\n",
        "    print(\"\\nğŸ’¾ Results are also saved in Google Drive:\")\n",
        "    print(\"  /content/drive/MyDrive/DiffLlama_Experiment/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "troubleshooting_section"
      },
      "source": [
        "## ğŸ›  æ•…éšœæ’é™¤\n",
        "\n",
        "å¦‚æœé‡åˆ°é—®é¢˜ï¼Œå°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆï¼š"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clear_cache"
      },
      "outputs": [],
      "source": [
        "# æ¸…ç† GPU å†…å­˜ç¼“å­˜\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"âœ… GPU cache cleared\")\n",
        "\n",
        "# æ£€æŸ¥å¯ç”¨å†…å­˜\n",
        "import psutil\n",
        "memory = psutil.virtual_memory()\n",
        "print(f\"ğŸ’¾ RAM: {memory.available / 1e9:.1f}GB available / {memory.total / 1e9:.1f}GB total\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "restart_runtime"
      },
      "outputs": [],
      "source": [
        "# å¦‚æœå†…å­˜ä¸è¶³ï¼Œå¯ä»¥é‡å¯è¿è¡Œæ—¶ (è°¨æ…ä½¿ç”¨)\n",
        "# import os\n",
        "# os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion_section"
      },
      "source": [
        "## ğŸ¯ å®éªŒç»“è®º\n",
        "\n",
        "æ ¹æ®å®éªŒç»“æœï¼Œæ‚¨å¯ä»¥åˆ†æä»¥ä¸‹å…³é”®é—®é¢˜ï¼š\n",
        "\n",
        "1. **å™ªå£°é²æ£’æ€§**: DiffLlama æ˜¯å¦åœ¨å«å™ªæ•°æ®ä¸Šè¡¨ç°æ›´å¥½ï¼Ÿ\n",
        "2. **æ³¨æ„åŠ›æœºåˆ¶**: å·®åˆ†æ³¨æ„åŠ›æ˜¯å¦æ›´æœ‰æ•ˆåœ°èšç„¦å…³é”®ä¿¡æ¯ï¼Ÿ\n",
        "3. **æ€§èƒ½ä¸‹é™**: ä¸¤ä¸ªæ¨¡å‹åœ¨ä¸åŒå™ªå£°ç±»å‹ä¸‹çš„æ€§èƒ½å˜åŒ–å¦‚ä½•ï¼Ÿ\n",
        "\n",
        "---\n",
        "\n",
        "**æ„Ÿè°¢ä½¿ç”¨æ­¤å®éªŒæ¡†æ¶ï¼** ğŸ‰\n",
        "\n",
        "å¦‚æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š\n",
        "- GPU å†…å­˜æ˜¯å¦å……è¶³\n",
        "- æ‰€æœ‰å¿…éœ€æ–‡ä»¶æ˜¯å¦ä¸Šä¼ \n",
        "- ç½‘ç»œè¿æ¥æ˜¯å¦ç¨³å®š\n",
        "\n",
        "**æç¤º**: å»ºè®®å…ˆè¿è¡Œå¿«é€Ÿæµ‹è¯•æ¨¡å¼éªŒè¯ç¯å¢ƒï¼Œå†è¿›è¡Œå®Œæ•´å®éªŒã€‚"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}