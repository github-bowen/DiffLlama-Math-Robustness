{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "header"
      },
      "source": [
       "# ğŸ”¬ DiffLlama vs Llama: Google Colab å®éªŒ\n",
       "\n",
       "è¿™ä¸ª Notebook ç”¨äºåœ¨ Google Colab ç¯å¢ƒä¸­è¿è¡Œ DiffLlama ä¸ Llama åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„å™ªå£°é²æ£’æ€§å¯¹æ¯”å®éªŒã€‚\n",
       "\n",
       "## ğŸ“‹ å®éªŒæ¦‚è¿°\n",
       "- **ç›®æ ‡**: å¯¹æ¯” DiffLlama-375M ä¸ Llama-375M åœ¨å«å™ªæ•°å­¦é—®é¢˜ä¸Šçš„è¡¨ç°\n",
       "- **æ•°æ®é›†**: GSM8K æ•°å­¦æ¨ç†æ•°æ®é›†åŠå…¶å™ªå£°å˜ä½“\n",
       "- **è¯„ä¼°**: é›¶æ ·æœ¬æ€§èƒ½ + æ³¨æ„åŠ›æœºåˆ¶åˆ†æ\n",
       "- **ç¯å¢ƒ**: Google Colab (GPU æ¨è)\n",
       "\n",
       "---"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "setup_section"
      },
      "source": [
       "## ğŸš€ ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒè®¾ç½®\n",
       "\n",
       "é¦–å…ˆæ£€æŸ¥è¿è¡Œç¯å¢ƒå¹¶è®¾ç½®å¿…è¦çš„é…ç½®ã€‚"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "check_environment"
      },
      "outputs": [],
      "source": [
       "# æ£€æŸ¥ GPU å¯ç”¨æ€§\n",
       "import torch\n",
       "print(f\"ğŸ–¥ï¸  CUDA available: {torch.cuda.is_available()}\")\n",
       "if torch.cuda.is_available():\n",
       "    print(f\"ğŸ”§ GPU: {torch.cuda.get_device_name(0)}\")\n",
       "    print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
       "else:\n",
       "    print(\"âš ï¸  No GPU detected. Experiment will be slow on CPU.\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "clone_repo"
      },
      "outputs": [],
      "source": [
       "# å¦‚æœé¡¹ç›®æ–‡ä»¶ä¸åœ¨å½“å‰ç›®å½•ï¼Œä» Git ä»“åº“å…‹éš†\n",
       "# æ›¿æ¢ä¸ºæ‚¨çš„å®é™…ä»“åº“åœ°å€\n",
       "import os\n",
       "if not os.path.exists('colab_experiment.py'):\n",
       "    print(\"ğŸ“¥ Cloning repository...\")\n",
       "    # !git clone https://github.com/your-username/your-repo.git\n",
       "    # !cp -r your-repo/* .\n",
       "    print(\"âš ï¸  Please upload your project files to Colab or clone from your repository\")\n",
       "else:\n",
       "    print(\"âœ… Project files found\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "upload_section"
      },
      "source": [
       "## ğŸ“ ç¬¬äºŒæ­¥ï¼šä¸Šä¼ é¡¹ç›®æ–‡ä»¶\n",
       "\n",
       "å¦‚æœæ‚¨æ²¡æœ‰ä½¿ç”¨ Git å…‹éš†ï¼Œè¯·æ‰‹åŠ¨ä¸Šä¼ ä»¥ä¸‹æ–‡ä»¶åˆ° Colabï¼š\n",
       "\n",
       "**å¿…éœ€æ–‡ä»¶**:\n",
       "- `colab_experiment.py` (ä¸»è¦çš„ Colab è„šæœ¬)\n",
       "- `pre_download_models.py` (æ¨¡å‹ä¸‹è½½è„šæœ¬)\n",
       "- `src/` ç›®å½•ä¸‹çš„æ‰€æœ‰ Python æ–‡ä»¶\n",
       "- `requirements.txt`\n",
       "\n",
       "ä½¿ç”¨ Colab çš„æ–‡ä»¶ä¸Šä¼ åŠŸèƒ½æˆ–ä» Google Drive å¤åˆ¶æ–‡ä»¶ã€‚"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "verify_files"
      },
      "outputs": [],
      "source": [
       "# éªŒè¯å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
       "required_files = [\n",
       "    'colab_experiment.py',\n",
       "    'pre_download_models.py',\n",
       "    'src/utils.py',\n",
       "    'src/model_loader.py',\n",
       "    'src/noise_injection.py',\n",
       "    'src/evaluation.py',\n",
       "    'src/attention_visualizer.py'\n",
       "]\n",
       "\n",
       "missing_files = []\n",
       "for file in required_files:\n",
       "    if os.path.exists(file):\n",
       "        print(f\"âœ… {file}\")\n",
       "    else:\n",
       "        print(f\"âŒ {file}\")\n",
       "        missing_files.append(file)\n",
       "\n",
       "if missing_files:\n",
       "    print(f\"\\nâš ï¸  Missing {len(missing_files)} files. Please upload them before proceeding.\")\n",
       "else:\n",
       "    print(\"\\nğŸ‰ All required files found!\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "instructions_section"
      },
      "source": [
       "## ğŸ“– ç¬¬ä¸‰æ­¥ï¼šæŸ¥çœ‹ä½¿ç”¨è¯´æ˜\n",
       "\n",
       "è¿è¡Œä¸‹é¢çš„å‘½ä»¤æŸ¥çœ‹è¯¦ç»†çš„ä½¿ç”¨è¯´æ˜å’Œé€‰é¡¹ã€‚"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "show_instructions"
      },
      "outputs": [],
      "source": [
       "# æ˜¾ç¤ºä½¿ç”¨è¯´æ˜\n",
       "!python colab_experiment.py --instructions"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "setup_run_section"
      },
      "source": [
       "## ğŸ”§ ç¬¬å››æ­¥ï¼šåˆå§‹è®¾ç½®\n",
       "\n",
       "è¿è¡Œåˆå§‹è®¾ç½®ï¼Œå®‰è£…ä¾èµ–åŒ…å¹¶é…ç½®ç¯å¢ƒã€‚"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "initial_setup"
      },
      "outputs": [],
      "source": [
       "# è¿è¡Œåˆå§‹è®¾ç½® (åŒ…å« Google Drive æŒ‚è½½)\n",
       "!python colab_experiment.py --setup --use-drive"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "experiment_section"
      },
      "source": [
       "## ğŸš€ ç¬¬äº”æ­¥ï¼šè¿è¡Œå®éªŒ\n",
       "\n",
       "æ ¹æ®æ‚¨çš„éœ€æ±‚é€‰æ‹©åˆé€‚çš„å®éªŒæ¨¡å¼ï¼š"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "quick_test_section"
      },
      "source": [
       "### ğŸƒ å¿«é€Ÿæµ‹è¯• (æ¨èé¦–æ¬¡è¿è¡Œ)\n",
       "ä½¿ç”¨å°‘é‡æ ·æœ¬éªŒè¯å®éªŒæµç¨‹ï¼Œå¤§çº¦éœ€è¦ 30-60 åˆ†é’Ÿã€‚"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "quick_test"
      },
      "outputs": [],
      "source": [
       "# å¿«é€Ÿæµ‹è¯•æ¨¡å¼\n",
       "!python colab_experiment.py --mode quick --use-drive"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "medium_test_section"
      },
      "source": [
       "### ğŸ“Š ä¸­ç­‰è§„æ¨¡å®éªŒ\n",
       "ä½¿ç”¨ä¸­ç­‰æ•°é‡æ ·æœ¬ï¼Œå¹³è¡¡æ—¶é—´å’Œç»“æœè´¨é‡ã€‚"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "medium_test"
      },
      "outputs": [],
      "source": [
       "# ä¸­ç­‰è§„æ¨¡å®éªŒ (è¿è¡Œå‰è¯·ç¡®ä¿å¿«é€Ÿæµ‹è¯•æˆåŠŸ)\n",
       "# !python colab_experiment.py --mode medium --use-drive"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "full_test_section"
      },
      "source": [
       "### ğŸ”¬ å®Œæ•´å®éªŒ\n",
       "ä½¿ç”¨å®Œæ•´æ•°æ®é›†è¿›è¡Œå®éªŒï¼Œå¯èƒ½éœ€è¦æ•°å°æ—¶ã€‚"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "full_test"
      },
      "outputs": [],
      "source": [
       "# å®Œæ•´å®éªŒ (ä»…åœ¨æœ‰è¶³å¤Ÿæ—¶é—´æ—¶è¿è¡Œ)\n",
       "# !python colab_experiment.py --mode full --use-drive --max-samples 500"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "custom_section"
      },
      "source": [
       "### ğŸ›  è‡ªå®šä¹‰å®éªŒ\n",
       "æ ¹æ®éœ€è¦è°ƒæ•´å®éªŒå‚æ•°ã€‚"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "custom_experiment"
      },
      "outputs": [],
      "source": [
       "# è‡ªå®šä¹‰å®éªŒç¤ºä¾‹\n",
       "# åªè¿è¡Œè¯„ä¼°ï¼Œè·³è¿‡æ³¨æ„åŠ›åˆ†æä»¥èŠ‚çœæ—¶é—´\n",
       "# !python colab_experiment.py --mode medium --use-drive --skip-attention --max-samples 100"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "results_section"
      },
      "source": [
       "## ğŸ“Š ç¬¬å…­æ­¥ï¼šæŸ¥çœ‹å®éªŒç»“æœ\n",
       "\n",
       "å®éªŒå®Œæˆåï¼ŒæŸ¥çœ‹ç”Ÿæˆçš„ç»“æœæ–‡ä»¶ã€‚"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "list_results"
      },
      "outputs": [],
      "source": [
       "# åˆ—å‡ºç”Ÿæˆçš„ç»“æœæ–‡ä»¶\n",
       "!ls -la results/"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "show_summary"
      },
      "outputs": [],
      "source": [
       "# æŸ¥çœ‹æœ€æ–°çš„å®éªŒæ‘˜è¦\n",
       "import json\n",
       "import glob\n",
       "\n",
       "# æ‰¾åˆ°æœ€æ–°çš„æ‘˜è¦æ–‡ä»¶\n",
       "summary_files = glob.glob('results/colab_summary_*.json')\n",
       "if summary_files:\n",
       "    latest_summary = max(summary_files)\n",
       "    print(f\"ğŸ“‹ Latest experiment summary: {latest_summary}\")\n",
       "    \n",
       "    with open(latest_summary, 'r') as f:\n",
       "        summary = json.load(f)\n",
       "    \n",
       "    print(\"\\nğŸ“Š Experiment Summary:\")\n",
       "    for key, value in summary.items():\n",
       "        print(f\"  {key}: {value}\")\n",
       "else:\n",
       "    print(\"No experiment summary found. Please run an experiment first.\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "show_results"
      },
      "outputs": [],
      "source": [
       "# æ˜¾ç¤ºä¸»è¦ç»“æœ\n",
       "import pandas as pd\n",
       "\n",
       "# æ‰¾åˆ°æœ€æ–°çš„ç»“æœæ–‡ä»¶\n",
       "result_files = glob.glob('results/colab_results_*.csv')\n",
       "if result_files:\n",
       "    latest_results = max(result_files)\n",
       "    print(f\"ğŸ“ˆ Latest results: {latest_results}\")\n",
       "    \n",
       "    df = pd.read_csv(latest_results)\n",
       "    print(\"\\nğŸ“Š Performance Comparison:\")\n",
       "    print(df.pivot(index='model', columns='dataset', values='accuracy'))\n",
       "    \n",
       "    # è®¡ç®—æ€§èƒ½å·®å¼‚\n",
       "    pivot_df = df.pivot(index='model', columns='dataset', values='accuracy')\n",
       "    if 'llama' in pivot_df.index and 'diffllama' in pivot_df.index:\n",
       "        print(\"\\nğŸ” Performance Difference (DiffLlama - Llama):\")\n",
       "        diff = pivot_df.loc['diffllama'] - pivot_df.loc['llama']\n",
       "        print(diff)\n",
       "else:\n",
       "    print(\"No results found. Please run an experiment first.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "visualization_section"
      },
      "source": [
       "## ğŸ“ˆ ç¬¬ä¸ƒæ­¥ï¼šç»“æœå¯è§†åŒ–\n",
       "\n",
       "å¦‚æœå®éªŒåŒ…å«äº†æ³¨æ„åŠ›åˆ†æï¼Œå¯ä»¥æŸ¥çœ‹ç”Ÿæˆçš„æ³¨æ„åŠ›çƒ­åŠ›å›¾ã€‚"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "show_attention_maps"
      },
      "outputs": [],
      "source": [
       "# æ˜¾ç¤ºæ³¨æ„åŠ›çƒ­åŠ›å›¾\n",
       "import matplotlib.pyplot as plt\n",
       "from IPython.display import Image, display\n",
       "import os\n",
       "\n",
       "attention_dir = 'results/attention_maps'\n",
       "if os.path.exists(attention_dir):\n",
       "    print(\"ğŸ§  Attention Visualization Files:\")\n",
       "    \n",
       "    # åˆ—å‡ºæ‰€æœ‰æ³¨æ„åŠ›å›¾æ–‡ä»¶\n",
       "    for root, dirs, files in os.walk(attention_dir):\n",
       "        for file in files:\n",
       "            if file.endswith('.png'):\n",
       "                file_path = os.path.join(root, file)\n",
       "                print(f\"  ğŸ“Š {file_path}\")\n",
       "                \n",
       "                # æ˜¾ç¤ºå›¾ç‰‡ (å¯é€‰ï¼Œå–æ¶ˆæ³¨é‡Šä»¥æ˜¾ç¤º)\n",
       "                # display(Image(file_path))\n",
       "else:\n",
       "    print(\"No attention maps found. Run experiment with attention analysis enabled.\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "show_attention_analysis"
      },
      "outputs": [],
      "source": [
       "# æ˜¾ç¤ºæ³¨æ„åŠ›åˆ†æç»“æœ\n",
       "attention_files = glob.glob('results/colab_attention_*.json')\n",
       "if attention_files:\n",
       "    latest_attention = max(attention_files)\n",
       "    print(f\"ğŸ§  Latest attention analysis: {latest_attention}\")\n",
       "    \n",
       "    with open(latest_attention, 'r') as f:\n",
       "        attention_data = json.load(f)\n",
       "    \n",
       "    print(\"\\nğŸ“Š Attention Allocation Analysis:\")\n",
       "    for model, data in attention_data.items():\n",
       "        print(f\"\\n{model.upper()} Model:\")\n",
       "        for condition, stats in data.items():\n",
       "            print(f\"  {condition.capitalize()}:\")\n",
       "            print(f\"    KMI (Key Math Info): {stats['kmi_mean']:.3f} Â± {stats['kmi_std']:.3f}\")\n",
       "            print(f\"    NI (Noise Info): {stats['ni_mean']:.3f} Â± {stats['ni_std']:.3f}\")\n",
       "            print(f\"    OC (Other Context): {stats['oc_mean']:.3f} Â± {stats['oc_std']:.3f}\")\nelse:\n    print(\"No attention analysis found. Run experiment with attention analysis enabled.\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "download_section"
      },
      "source": [
       "## ğŸ’¾ ç¬¬å…«æ­¥ï¼šä¸‹è½½ç»“æœ\n",
       "\n",
       "å°†å®éªŒç»“æœä¸‹è½½åˆ°æœ¬åœ°æˆ–ç¡®ä¿ä¿å­˜åœ¨ Google Drive ä¸­ã€‚"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "download_results"
      },
      "outputs": [],
      "source": [
       "# å‹ç¼©ç»“æœæ–‡ä»¶ä»¥ä¾¿ä¸‹è½½\n",
       "import zipfile\n",
       "from datetime import datetime\n",
       "\n",
       "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
       "zip_filename = f'experiment_results_{timestamp}.zip'\n",
       "\n",
       "with zipfile.ZipFile(zip_filename, 'w') as zipf:\n",
       "    # æ·»åŠ ç»“æœç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶\n",
       "    for root, dirs, files in os.walk('results'):\n",
       "        for file in files:\n",
       "            file_path = os.path.join(root, file)\n",
       "            zipf.write(file_path)\n",
       "\n",
       "print(f\"ğŸ“¦ Results packaged in: {zip_filename}\")\n",
       "print(\"You can download this file from Colab's Files panel.\")\n",
       "\n",
       "# å¦‚æœä½¿ç”¨äº† Google Driveï¼Œæé†’ç”¨æˆ·\n",
       "if os.path.exists('/content/drive/MyDrive/DiffLlama_Experiment'):\n",
       "    print(\"\\nğŸ’¾ Results are also saved in Google Drive:\")\n",
       "    print(\"  /content/drive/MyDrive/DiffLlama_Experiment/\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "troubleshooting_section"
      },
      "source": [
       "## ğŸ›  æ•…éšœæ’é™¤\n",
       "\n",
       "å¦‚æœé‡åˆ°é—®é¢˜ï¼Œå°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆï¼š"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "clear_cache"
      },
      "outputs": [],
      "source": [
       "# æ¸…ç† GPU å†…å­˜ç¼“å­˜\n",
       "import torch\n",
       "if torch.cuda.is_available():\n",
       "    torch.cuda.empty_cache()\n",
       "    print(\"âœ… GPU cache cleared\")\n",
       "\n",
       "# æ£€æŸ¥å¯ç”¨å†…å­˜\n",
       "import psutil\n",
       "memory = psutil.virtual_memory()\n",
       "print(f\"ğŸ’¾ RAM: {memory.available / 1e9:.1f}GB available / {memory.total / 1e9:.1f}GB total\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
       "id": "restart_runtime"
      },
      "outputs": [],
      "source": [
       "# å¦‚æœå†…å­˜ä¸è¶³ï¼Œå¯ä»¥é‡å¯è¿è¡Œæ—¶ (è°¨æ…ä½¿ç”¨)\n",
       "# import os\n",
       "# os.kill(os.getpid(), 9)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {
       "id": "conclusion_section"
      },
      "source": [
       "## ğŸ¯ å®éªŒç»“è®º\n",
       "\n",
       "æ ¹æ®å®éªŒç»“æœï¼Œæ‚¨å¯ä»¥åˆ†æä»¥ä¸‹å…³é”®é—®é¢˜ï¼š\n",
       "\n",
       "1. **å™ªå£°é²æ£’æ€§**: DiffLlama æ˜¯å¦åœ¨å«å™ªæ•°æ®ä¸Šè¡¨ç°æ›´å¥½ï¼Ÿ\n",
       "2. **æ³¨æ„åŠ›æœºåˆ¶**: å·®åˆ†æ³¨æ„åŠ›æ˜¯å¦æ›´æœ‰æ•ˆåœ°èšç„¦å…³é”®ä¿¡æ¯ï¼Ÿ\n",
       "3. **æ€§èƒ½ä¸‹é™**: ä¸¤ä¸ªæ¨¡å‹åœ¨ä¸åŒå™ªå£°ç±»å‹ä¸‹çš„æ€§èƒ½å˜åŒ–å¦‚ä½•ï¼Ÿ\n",
       "\n",
       "---\n",
       "\n",
       "**æ„Ÿè°¢ä½¿ç”¨æ­¤å®éªŒæ¡†æ¶ï¼** ğŸ‰\n",
       "\n",
       "å¦‚æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š\n",
       "- GPU å†…å­˜æ˜¯å¦å……è¶³\n",
       "- æ‰€æœ‰å¿…éœ€æ–‡ä»¶æ˜¯å¦ä¸Šä¼ \n",
       "- ç½‘ç»œè¿æ¥æ˜¯å¦ç¨³å®š\n",
       "\n",
       "**æç¤º**: å»ºè®®å…ˆè¿è¡Œå¿«é€Ÿæµ‹è¯•æ¨¡å¼éªŒè¯ç¯å¢ƒï¼Œå†è¿›è¡Œå®Œæ•´å®éªŒã€‚"
      ]
     }
    ],
    "metadata": {
     "accelerator": "GPU",
     "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
     },
     "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
     },
     "language_info": {
      "name": "python"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 0
   }